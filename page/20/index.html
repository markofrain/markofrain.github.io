<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     雪里
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/main.css">

  
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
  
  

  

  <script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
</head>

</html>

<body>
  <div id="app">
    <main class="content">
      
<section class="cover">
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover4.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">雪里</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>

<div id="main">
  <section class="outer">
  <article class="articles">
    
    
    
    
    <article id="post-Java/JVM虚拟机原理" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/24/Java/JVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%8E%9F%E7%90%86/"
    >JVM虚拟机原理</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/24/Java/JVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%8E%9F%E7%90%86/" class="article-date">
  <time datetime="2020-03-24T00:46:12.000Z" itemprop="datePublished">2020-03-24</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Java/">Java</a>
  </div>

      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      
      

      
      <h1 id="JVM-Java虚拟机原理"><a href="#JVM-Java虚拟机原理" class="headerlink" title="JVM Java虚拟机原理"></a>JVM Java虚拟机原理</h1><h2 id="类加载器"><a href="#类加载器" class="headerlink" title="类加载器"></a>类加载器</h2><p>类加载器子系统负责从文件系统或网络中加载Class文件，class文件在文件开头有特定的文件标识。</p>
<p>ClassLoader只负责加载，是否可以运行由Execution Engine决定。</p>
<p>加载类信息存放在方法区的内存空间。除了类信息，还有运行时常量信息，字符串字面量和数字常量(这部分常量是Class文件中的常量池信息部分的内存映射)。</p>
<p><strong>环节</strong></p>
<p>Loading加载—&gt;链接(Verification验证—&gt;Preparation准备—&gt;Resolution解析)—&gt;Initialzation初始化</p>
<p><strong>加载.class文件的方式。</strong></p>
<ul>
<li>本地直接加载</li>
<li>通过网络获取:Web Applet</li>
<li>从zip，jar，war中获取</li>
<li>动态代理</li>
<li>其他文件生成,JSP</li>
<li>加密文件中获取，防Class文件被反编译措施</li>
</ul>
<p><strong>验证</strong></p>
<p>确保Class文件字节流中包含信息符合当前虚拟机要求，保证被加载类的正确性。包括验证，文件格式验证、元数据、字节码、符号引用。</p>
<p><strong>准备</strong></p>
<p>为变量分配内存并设置该类变量的默认初始值，即零值。</p>
<ul>
<li>不包含final修饰的static变量，final在编译的时候就分配了，准备阶段会显示初始化。</li>
<li><u>不会为实例变量分配初始值</u>，类变量会分配在方法区中，而实例变量会随着对象一起分配到Java堆中。</li>
</ul>
<p><strong>解析</strong></p>
<p>将常量池内的符号引用转换为直接引用的过程。</p>
<p>符号引用就是一组符号来描述所引用的目标。</p>
<p>直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。</p>
<p>解析动作主要针对类或接口、字段、方法、类方法、接口方法、方法类型等。</p>
<p><strong>初始化</strong></p>
<p>初始化阶段就是执行类构造器方法&lt;clinit&gt;()的过程。</p>
<ul>
<li><p>此方法不需要定义，是javac编译器自动收集类中的所有类变量的赋值动作和静态代码块中的语句合并而来。</p>
</li>
<li><p>构造器方法中指令按语句在源文件中出现的顺序执行。</p>
</li>
<li><p>&lt;clinit&gt;()不同于类的构造器(&lt;init&gt;–&gt;bytecode viewer工具可以看到)</p>
</li>
<li><p>若该类具有父类,JVM会保证子类的clinit执行前执行父类的clinit</p>
</li>
<li><p>虚拟机必须保证一个类的&lt;clinit&gt;()方法在多线程下被同步加锁。</p>
</li>
</ul>
<h3 id="类加载器分类"><a href="#类加载器分类" class="headerlink" title="类加载器分类"></a>类加载器分类</h3><p>分为引导类加载器(BootStrap ClassLoader)和自定义加载器(所有派生于ClassLoader的加载器都是自定义加载器)。</p>
<p>拓展类加载器(ExtClassLoader)、系统类加载器间接的继承了ClassLoader。</p>
<h3 id="引导类加载器"><a href="#引导类加载器" class="headerlink" title="引导类加载器"></a>引导类加载器</h3><ul>
<li>这个类加载使用C/C++语言实现，嵌套在JVM内部。</li>
<li>用来加载核心库rt.jar，resources.jar，sun.boot.class.path路径下的内容，用于提供JVM自身需要的类。</li>
<li>并不继承自java.lang.ClassLoader，没有父加载器</li>
<li>加载拓展类和应用程序类加载器，并指定为他们的父类加载器</li>
<li>出于安全考虑，BootStrap启动类加载器只加载包含包名为java、javax、sun开头的类</li>
</ul>
<h3 id="应用程序类加载器"><a href="#应用程序类加载器" class="headerlink" title="应用程序类加载器"></a>应用程序类加载器</h3><ul>
<li><p>有sum.misc.launcher$AppClassLoader实现，派生于ClassLoader类，父加载器为拓展类加载器。</p>
</li>
<li><p>负责加载环境变量classpath或系统属性java.class.path指定路径下的类库。</p>
</li>
<li><p>是程序的默认类加载器，一般来说，Java应用的类都是由它完成的。</p>
</li>
<li><p>通过ClassLoader#getSystemClassLoader方法获取到该类加载器。</p>
</li>
</ul>
<h3 id="拓展类加载器"><a href="#拓展类加载器" class="headerlink" title="拓展类加载器"></a>拓展类加载器</h3><p>Java语言编写，有sum.misc.Launcher$ExtClassLoader实现，派生于ClassLoader类，父加载器为启动类加载器。</p>
<p>从java.ext.dirs系统属性所指定的目录中加载类库，或从jdk目录的rt/lib/ext/子目录下加载类库。如果用户创建的jar放在此目录，也由它加载。</p>
<h3 id="用户自定义加载类"><a href="#用户自定义加载类" class="headerlink" title="用户自定义加载类"></a>用户自定义加载类</h3><p>为什么要自定义加载类？</p>
<ul>
<li>隔离加载类。当引用中间件时，确保框架jar使用的加载和中间件jar加载的分开，自定义加载类隔离</li>
<li>修改类的加载方式</li>
<li>拓展加载源</li>
<li>防止源码泄露</li>
</ul>
<p><strong>步骤</strong></p>
<ul>
<li><p>用户通过集成java.lang.ClassLoader类的方式，实现自己的类加载器，以满足特殊需求。</p>
</li>
<li><p>建议把自定义逻辑卸载findClass方法中</p>
</li>
</ul>
<h3 id="双亲委派机制"><a href="#双亲委派机制" class="headerlink" title="双亲委派机制"></a>双亲委派机制</h3><p>Java虚拟机对class文件采用的时按需加载的方式，也就是说当需要使用该类时才会将他的class文件加载到内存生成class对象。而且加载某个类的class文件时，java虚拟机采用的时双亲委派机制，<strong>即把请求交由父类处理，它是一种任务委派模式</strong>。</p>
<p><strong>工作原理</strong></p>
<ol>
<li>如果一个类加载器收到了类加载请求,它并不会自己先去加载,而是把这个请求委托给父类的加载器去执行;</li>
<li>如果父类加载器还存在其父类加载器,则进一步向上委托,依次递归,请求最终将到达顶层的启动类加载器;</li>
<li>如果父类加载器可以完成类加载任务,就成功返回,倘若父类加载器无法完成此加载任务,子加载器才会尝试自己去加载,这就是双亲委派模式。</li>
</ol>
<p><img src="https://chen-sys.oss-cn-beijing.aliyuncs.com/picture/TIM%E6%88%AA%E5%9B%BE20200315214250.png" alt=""></p>
<p>例子：假如有一个自定义完全相同的完全限定名的String类，那么，该类会从appClassLoader向上委托，然后引导类会加载java.lang.String,也就是他自己能扫描到类库下的jdk 的String类。加载的是核心类库的String类。</p>
<p>而如果自定义的String类执行main方法，则报错，在类java.lang.String中找不到main方法。此时类加载器一直委托到引导类加载器，会加载核心API的String，但它没有main方法，因此会报错。</p>
<blockquote>
<p> <strong>沙箱安全机制</strong></p>
</blockquote>
<p><strong>优势</strong></p>
<ul>
<li><p>防止新API被篡改</p>
</li>
<li><p>避免类重复加载</p>
</li>
</ul>
<h3 id="类的主动使用和被动使用"><a href="#类的主动使用和被动使用" class="headerlink" title="类的主动使用和被动使用"></a>类的主动使用和被动使用</h3><p>在JVM中表示两个class对象是否为同一对象存在的另两个必要条件:</p>
<ul>
<li>类的完整类名完全相同，包括包名</li>
<li>加载这个类的ClassLoader(指ClassLoader实例对象)必须相同</li>
</ul>
<p>只要加载他们的类加载器不一样，两个类的对象也不一样。</p>
<p>如果一个类型由用户类加载器加载，那么JVM会将这个类加载器的一个引用作为类型信息的一部分保存在方法区中。</p>
<p>当解析一个类型到另一个类型的引用的时候，JVM需要保证这两个类型的类加载器是相同的。</p>
<p><strong>Java程序对类的使用方式分为:主动使用和被动使用</strong><br>主动使用,又分为七种情况:<br>创建类的实例</p>
<ul>
<li>访问某个类或接口的静态变量,或者对该静态变量赋值</li>
<li>调用类的静态方法</li>
<li>反射(比如: Class. forName(“com. atguigu.test”))</li>
<li>初始化一个类的子类</li>
<li>Java虚拟机启动时被标明为启动类的类</li>
<li>JDK7开始提供的动态语言支持:<br>java.lang. invoke. MethodHandle实例的解析结果<br> REF getStatic、 REF putStatic REF invokeStatic句柄对<br>应的类没有初始化,则初始化</li>
</ul>
<p>除了以上七种情况,其他使用Java类的方式都被看作是对类的被动使用,<br>都不会导致类的初始化。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      

    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Java/Java8新特性" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/24/Java/Java8%E6%96%B0%E7%89%B9%E6%80%A7/"
    >Java8新特性</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/24/Java/Java8%E6%96%B0%E7%89%B9%E6%80%A7/" class="article-date">
  <time datetime="2020-03-24T00:45:12.000Z" itemprop="datePublished">2020-03-24</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Java/">Java</a>
  </div>

      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      
      

      
      <h1 id="Java8新特性"><a href="#Java8新特性" class="headerlink" title="Java8新特性"></a>Java8新特性</h1><ul>
<li>速度更快</li>
<li>代码更少(<strong>新增Lambda语法</strong>)</li>
<li><strong>强大的Stream API</strong></li>
<li>便于并行</li>
<li>最大化减少空指针异常 Optional</li>
</ul>
<h2 id="Lambda表达式"><a href="#Lambda表达式" class="headerlink" title="Lambda表达式"></a>Lambda表达式</h2><p>Lambda表达式即函数式编程，需要<code>函数式接口</code>的支持。<br>函数式接口即:</p>
<ul>
<li>接口中只有1个方法</li>
<li>接口支持泛型</li>
<li>使用@FunctionalInterface注解修饰接口</li>
</ul>
<p>函数式编程只为简化代码量，简化写法，只是将new Xxxx(){public int xx(){}}这些需要实现接口方法的代码进行简化，而并非是执行这个方法。<strong>其目的依旧是实现接口方法</strong>。而<strong>变量是接口类</strong>并非方法返回。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@FunctionalInterface</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">MyNumbercompare</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">compare</span><span class="params">(Integer n1,Integer n2)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>实现函数式接口的方式</p>
<ul>
<li>() -&gt; System.out.println();//无参数，无返回值</li>
<li>(x) -&gt; System.out.println();//只有一个参数，且无返回值</li>
<li>x -&gt; System.out.println();//只有一个参数，小括号可以不写</li>
<li>Comparator<Integer> com = (x,y) -&gt; Integer.compare(x,y);//主体只有一条语句，return和大括号可以不写</li>
</ul>
<h3 id="default修饰符"><a href="#default修饰符" class="headerlink" title="default修饰符"></a>default修饰符</h3><p>可以再接口类中修饰方法，该方法是一个具体实现。<br>在接口中添加新功能特性，不影响接口的实现类。<br>不影响函数式接口。<br>关键字default，那么就无须提供该方法的实现。<br>函数式接口中，可以使用多个default修饰方法</p>
<h3 id="四大内置核心函数式接口"><a href="#四大内置核心函数式接口" class="headerlink" title="四大内置核心函数式接口"></a>四大内置核心函数式接口</h3><ul>
<li>Consumer<T>:消费型接口,void accept(T t);</li>
<li>Supplier<T>:供给型接口,T get();</li>
<li>Function<T>:函数型接口,R apply(T t);</li>
<li>Predicate<T>: 段言型接口,boolean test(T t);<br>其他相关所有函数式接口，都在java.util.function包里</li>
<li>BiFunction&lt;T,U,R&gt;: 参数类型为T,U,返回值为R</li>
<li>UnaryOperator<T>: 参数类型T,返回值T；对类型为T的对象进行一元运算</li>
<li>BinaryOperator<T>:参数T,T,返回值T;对类型为T的对象进行二元运算</li>
<li>ToIntFunction<T>:参数T,返回int类型</li>
<li>ToILongFunction<T>:参数T,返回long类型</li>
<li>ToDoubleFunction<T>:参数T,返回double类型</li>
<li>IntFunction<R>:参数int，返回类型为R</li>
</ul>
<h3 id="方法引用与构造器引用"><a href="#方法引用与构造器引用" class="headerlink" title="方法引用与构造器引用"></a>方法引用与构造器引用</h3><p>若Lambda体中内容有方法已经实现了，我们可以使用方法引用。<br>语法格式:</p>
<ul>
<li>对象::实例方法名</li>
<li>类:静态方法名</li>
<li>类::实例方法名</li>
</ul>
<p>需要保证的是，++函数式接口的方法++的参数列表与返回值类型要与++方法引用的实现方法++的参数列表与返回值类型一致。<br>例子如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Consumer&lt;String&gt; com = (x)-&gt; System.out.println(x);</span><br><span class="line"></span><br><span class="line">    Consumer&lt;String&gt; consumer = System.out::println;</span><br><span class="line"></span><br><span class="line">    consumer.accept(<span class="string">&quot;sss&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于上面两种，可以理解为两者都是右侧表达式，且表达式相同，只不过是显示方式不同<br>另外对方法引用还有一个理解是，上面的一句话，只需要保证其右侧表达式和函数式接口的方法是同参数列表同返回类型的，不就代表引用吗。</p>
<p><strong>类名::实例方法名的使用说明:</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">BiPredicate&lt;String,String&gt; bp = (x,y) -&gt; x.equals(y);</span><br><span class="line"></span><br><span class="line">BiPredicate&lt;String,String&gt; bp = String::equals;</span><br></pre></td></tr></table></figure>
<p>++即方法参数的第一个参数x为方法的调用对象，第二个参数及后面的为对象方法的参数列表.++</p>
<p><strong>构造器引用:</strong><br>例如:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Function&lt;Integer,MyClass&gt; fun = (n) -&gt; <span class="keyword">new</span> MyClass(n);</span><br><span class="line">Function&lt;Integer,MyClass&gt; fun = MyClass::<span class="keyword">new</span>;</span><br></pre></td></tr></table></figure>
<p>以上案例与方法引用类似，同样构造器参数列表要与接口提供的方法的参数列表一致。<br><strong>数组引用:</strong><br>例如:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Function&lt;Integer,Integer[]&gt; fun = (n) -&gt; <span class="keyword">new</span> Integer[n];</span><br><span class="line">Function&lt;Integer,Integer[]&gt; fun = Integer[]::<span class="keyword">new</span>;</span><br></pre></td></tr></table></figure>

<h2 id="Stream-API"><a href="#Stream-API" class="headerlink" title="Stream API"></a>Stream API</h2><p>流Stream是数据渠道，用于操作数据源(集合，数组)所生成的元素序列。流的来源还可以是I/O channel等s</p>
<ul>
<li>Stream自己不会存储元素</li>
<li>Stream不会改变源对象。相反，它们会持有结果的新Stream</li>
<li>Stream操作是延迟执行的。意味着它们会等到需要结果的时候才执行</li>
</ul>
<p>流的操作分为三个步骤:</p>
<ul>
<li>创建Stream，从集合或数组等数据源获取流</li>
<li>中间操作，对数据源进行处理</li>
<li>终止操作，执行中间操作链，产生结果</li>
</ul>
<h3 id="创建流"><a href="#创建流" class="headerlink" title="创建流"></a>创建流</h3><ol>
<li>Collection接口获得流</li>
</ol>
<ul>
<li>default Stream<E> stream():返回一个顺序流</li>
<li>default Stream<E> paralleStream():返回一个并行流</li>
</ul>
<ol start="2">
<li>数组创建流<br>使用Arrays的静态Stream()重载方法可以获得数据流，重载包括int,long,double,T泛型。</li>
</ol>
<ul>
<li>public static IntStream stream(int[] array)</li>
<li>public static LongStream stream(long[] array)</li>
<li>public static DoubleStream stream(double[] array)</li>
<li>public static <T> Stream<T> stream(T[] array)</li>
</ul>
<ol start="3">
<li><p>通过Stream类创建<br>静态方法of,创建一个流，接收任意数量的参数<br>public static<T> Stream<T> of(T… values)</p>
</li>
<li><p>函数创建流<br>使用静态方法Stream.iterator()和Stream.generate()创建流</p>
</li>
</ol>
<ul>
<li>迭代:public static<T> Stream<T> iterate(final T seed, final UnaryOperator<T> f) </li>
<li>生成:public static<T> Stream<T> generate(Supplier<T> s)</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Stream&lt;Integer&gt; stream = Stream.iterate(<span class="number">0</span>,(x)-&gt;x+<span class="number">2</span>);</span><br><span class="line">stream.limit(<span class="number">10</span>).forEach(System.out::println);</span><br><span class="line"></span><br><span class="line">Stream.generate(()-&gt; Math.random()).limit(<span class="number">10</span>).forEach(System.out::println);</span><br></pre></td></tr></table></figure>
<h3 id="中间操作"><a href="#中间操作" class="headerlink" title="中间操作"></a>中间操作</h3><p>多个中间操作可以连接起来形成一个流水线，除非终止操作，即类似ForEach,println等，否则<strong>中间操作不会执行任何的处理</strong>，而在终止操作时一次性处理全部处理，称为惰性求值。</p>
<ol>
<li>筛选与切片</li>
</ol>
<ul>
<li>filter(Predicate p):接收lambda表达式，段言型，即过滤去除。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Stream&lt;Double&gt; stream1 = Stream.generate(()-&gt; Math.random()).limit(<span class="number">20</span>);</span><br><span class="line"></span><br><span class="line">Stream&lt;Double&gt; stream1x =  stream1.filter((x)-&gt;&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;Stream API&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> x&gt;<span class="number">0.4</span>;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">stream1.forEach(System.out::println);</span><br></pre></td></tr></table></figure>
<p>如上，若没有第三段的forEach，则运行到第二段后，是不会进行打印Stream API的，知道有打印的才会打印，而被过滤的数据不被计入到limit20里，所以在ForEach只有会有20个double随机数被打印出来，和不定的Stream API打印。</p>
<ul>
<li>limit(int number):在Stream找到所需元素固定次数后，就不在迭代过滤。</li>
<li>skip(int n):跳过n个元素，如果流中元素不足n个，则返回空流</li>
<li>distinct():去除重复记录,通过hashCode()和euqals()来去重</li>
</ul>
<ol start="2">
<li>映射</li>
</ol>
<ul>
<li>map:接受一个函数作为参数，该函数会应用到所有的元素上，并将其映射成一个新的元素。</li>
<li>flatMap:接收一个函数作为参数，将flatMap的的函数中返回的流连接成一个流，即若flatMap的参数函数返回一个流，那么它会将返回的流连接成一个流，即从Stream&lt;Stream<String>&gt; 变成 Stream<String></li>
</ul>
<ol start="3">
<li>排序</li>
</ol>
<ul>
<li>sorted:自然排序</li>
<li>stored(Comparator com):参数接受一个Comparator接口函数，用于自定义排序</li>
</ul>
<ol start="4">
<li>查找与匹配</li>
</ol>
<ul>
<li>allMatch(Predicate p):检查是否匹配所有元素</li>
<li>anyMatch(Predicate p):检查是否至少匹配一个元素</li>
<li>noneMatch(predicate p):检查是否没有匹配所有元素</li>
<li>findFirst():返回第一个元素</li>
<li>findAny():返回当前流中任意元素</li>
<li>count():返回流中的元素数量</li>
<li>max(Comparator c):返回流中最大值</li>
<li>min(Comparator c):返回流中最小值</li>
<li>forEach(Consumer c):内部迭代</li>
</ul>
<p><img src="https://chen-sys.oss-cn-beijing.aliyuncs.com/other/TIM%E6%88%AA%E5%9B%BE20190604115348.png" alt=""><br><img src="https://chen-sys.oss-cn-beijing.aliyuncs.com/other/TIM%E6%88%AA%E5%9B%BE20190604115508.png" alt=""><br><img src="https://chen-sys.oss-cn-beijing.aliyuncs.com/other/TIM%E6%88%AA%E5%9B%BE20190604115552.png" alt=""></p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Java/" rel="tag">Java</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Hadoop/集群搭建相关问题" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/24/Hadoop/%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"
    >集群搭建相关问题</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/24/Hadoop/%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/" class="article-date">
  <time datetime="2020-03-24T00:43:12.000Z" itemprop="datePublished">2020-03-24</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>

      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      
      

      
      <h2 id="免密登录失败Permission-denied-publickey-gssapi-keyex-gssapi-with-mic-password"><a href="#免密登录失败Permission-denied-publickey-gssapi-keyex-gssapi-with-mic-password" class="headerlink" title="免密登录失败Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password)?"></a>免密登录失败Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password)?</h2><p>通常该问题，网上说需要sshd_config配置，一个是SELinux被打开了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">第一种是需要这样做</span><br><span class="line">PubkeyAuthentication yes </span><br><span class="line">第二种需要使用root关闭(暂时)</span><br><span class="line">setenforce 0 </span><br></pre></td></tr></table></figure>

<p>注意在ssh进行免密登录的时候，一定注意几台机器的用户名是否一样。比如各服务器用户名不一样，则你直接<code>ssh 主机名</code>那这样是有问题的，必须<code>ssh 用户名@主机名</code>;如果各机器用户名一样，那么直接可以<code>ssh 主机名</code>，这样登录后才行。<strong>否则就是以当前用户登录其他主机，肯定是登不上的</strong>,这也是为什么我第一次配置时没有任何问题的就好了，之后都不行，原来是用户名设的不一样。</p>
<p>如果你hadoop集群出现了异常，那么实在不行就尝试将<strong>几台机器的用户名都修改成一样的</strong>。我觉得应该必须一样</p>
<h2 id="修改用户名"><a href="#修改用户名" class="headerlink" title="修改用户名"></a><a target="_blank" rel="noopener" href="https://www.cnblogs.com/guojuboke/p/10680213.html">修改用户名</a></h2><p>修改<br>/etc/passwd<br>/etc/shadow<br>/etc/group<br>/etc/gshadow<br>这四个文件里面原来的用户名名称都改为现在需要改变的名称<br>修改home文件下原来的文件名称<br>然后可以重启或不重启。但是密码没有改变。</p>
<h2 id="互通"><a href="#互通" class="headerlink" title="互通"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/crazyxinma/article/details/83029351">互通</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa </span><br><span class="line">ssh-copy-id IP ；（自身IP）</span><br><span class="line">ssh-copy-id IP ；（其他节点IP）</span><br></pre></td></tr></table></figure>

<h2 id="Inconsistent-checkpoint-fields-LV-64-namespaceID-1824398440-cTime-1562671135968-clusterId-CID-8366be1d-a420-44ef-9c91-0b2b6c779fd4-blockpoolId-BP-1686275258-192-168-181-140-1562671135968-Expecting-respectively-64-1349177589-1562664989"><a href="#Inconsistent-checkpoint-fields-LV-64-namespaceID-1824398440-cTime-1562671135968-clusterId-CID-8366be1d-a420-44ef-9c91-0b2b6c779fd4-blockpoolId-BP-1686275258-192-168-181-140-1562671135968-Expecting-respectively-64-1349177589-1562664989" class="headerlink" title="Inconsistent checkpoint fields. LV = -64 namespaceID = 1824398440 cTime = 1562671135968 ; clusterId = CID-8366be1d-a420-44ef-9c91-0b2b6c779fd4 ; blockpoolId = BP-1686275258-192.168.181.140-1562671135968. Expecting respectively: -64; 1349177589; 1562664989"></a>Inconsistent checkpoint fields. LV = -64 namespaceID = 1824398440 cTime = 1562671135968 ; clusterId = CID-8366be1d-a420-44ef-9c91-0b2b6c779fd4 ; blockpoolId = BP-1686275258-192.168.181.140-1562671135968. Expecting respectively: -64; 1349177589; 1562664989</h2><p>注意,core-site.xml中的hadoop.tmp.dir是否写对了</p>
<h2 id="出现Retrying-connect-to-server-centos2-192-168-181-141-8031-Already-tried-8-time-s-retry-policy-is-RetryUpToMaximumCountWithFixedSleep-maxRetries-10-sleepTime-1000-MILLISECONDS"><a href="#出现Retrying-connect-to-server-centos2-192-168-181-141-8031-Already-tried-8-time-s-retry-policy-is-RetryUpToMaximumCountWithFixedSleep-maxRetries-10-sleepTime-1000-MILLISECONDS" class="headerlink" title="出现Retrying connect to server: centos2/192.168.181.141:8031. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"></a>出现Retrying connect to server: centos2/192.168.181.141:8031. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</h2><p>关闭所有机器的防火墙，如果你不关闭，只开放对应端口，可能不太行，因为除了配置上的一些端口，还有许多隐藏的端口需要。但允许某些服务器访问没有试过</p>
<h2 id="启动start-yarn-sh后，当前节点没有NodeManager"><a href="#启动start-yarn-sh后，当前节点没有NodeManager" class="headerlink" title="启动start-yarn.sh后，当前节点没有NodeManager"></a>启动start-yarn.sh后，当前节点没有NodeManager</h2><p>检查配置项，是否配置全。另外对于hadoop3和hadoop2的数据节点由slaves文件变成的workers文件了。这个要注意，否则总是无法启动全datanode。</p>
<h2 id="另外访问50090或50070的，有的视频教程只配置的50090，但是可以访问50070，当你访问50090时，会出现和50070一样的界面，但之后Home和Overview，且http路径为xxx-xxx-xxx-xxx-50090-status-html，但是你访问50070时访问不到的。"><a href="#另外访问50090或50070的，有的视频教程只配置的50090，但是可以访问50070，当你访问50090时，会出现和50070一样的界面，但之后Home和Overview，且http路径为xxx-xxx-xxx-xxx-50090-status-html，但是你访问50070时访问不到的。" class="headerlink" title="另外访问50090或50070的，有的视频教程只配置的50090，但是可以访问50070，当你访问50090时，会出现和50070一样的界面，但之后Home和Overview，且http路径为xxx.xxx.xxx.xxx:50090/status.html，但是你访问50070时访问不到的。"></a>另外访问50090或50070的，有的视频教程只配置的50090，但是可以访问50070，当你访问50090时，会出现和50070一样的界面，但之后Home和Overview，且http路径为<code>xxx.xxx.xxx.xxx:50090/status.html</code>，但是你访问50070时访问不到的。</h2><p>所以你不仅需要配置secondary的httpaddress，还要直接配置namenode的httpaddress,在hdfs-site.xml文件中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;centos3:50090&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;description&gt;secondarynamenode运行节点的信息,和namenode不同节点&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;centos1:50070&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>另外每次关闭，下次重新启动 时都先stop-dfs.sh和stop-yarn,然后在删除所有节点的data logs，这样hdfs namenode -format之后对的操作才可能不出问题。平常关闭启动的话就不需要hdfs namenode -format,仅对没搭建起来集群的，需要重新部署的，要删干净。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Hadoop/Hadoop完全分布式集群" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/24/Hadoop/Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4/"
    >Hadoop完全分布式集群</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/24/Hadoop/Hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4/" class="article-date">
  <time datetime="2020-03-24T00:40:20.000Z" itemprop="datePublished">2020-03-24</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>

      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      
      

      
      <h1 id="第一章-虚拟机配置"><a href="#第一章-虚拟机配置" class="headerlink" title="第一章 虚拟机配置"></a>第一章 虚拟机配置</h1><p>创建centos7.x虚拟机不再描述,网卡连接方式使用桥接方式</p>
<p><strong>网络问题,创建NAT虚拟网络</strong><br>可在VMware的编辑，虚拟网络编辑器中，添加网络，点击右下角的管理员操作权限，点击添加网络按钮，随意选一个VMnet，选择VMnet为NAT模式。<br>勾选将主机虚拟适配器连接到此网络和勾选使用本地DHCP服务将IP地址分配给虚拟机。然后可以修改HDCP设置，默认可以不更改。</p>
<h2 id="创建完虚拟机后，无法使用sudo问题"><a href="#创建完虚拟机后，无法使用sudo问题" class="headerlink" title="创建完虚拟机后，无法使用sudo问题"></a>创建完虚拟机后，无法使用sudo问题</h2><p>使用命令<code>su root</code>进入root用户,执行<code>visudo</code>命令打开sudoer文件,找到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root    ALL&#x3D;(ALL)       ALL</span><br></pre></td></tr></table></figure>
<p>在其下面添加与其一样的一行，并把root改为自己的用户名,然后保存退出,如:</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">centos    ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure>

<h2 id="安装jdk和hadoop"><a href="#安装jdk和hadoop" class="headerlink" title="安装jdk和hadoop"></a>安装jdk和hadoop</h2><p>jdk选JDK8的,hadoop建议选择3.x版本的，这里，我使用JDK8和hadoop3.1.3。目前JDK11与hadoop3.x版本的匹配问题还没有找到解决办法，使用jdk会导致在启动时出现java.lang.ClassNotFoundException: javax.activation.DataSource的异常。</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[centos@centos3 ~]$ java -version</span><br><span class="line">java version &quot;1.8.0_231&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_231-b11)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode)</span><br><span class="line">[centos@localhost ~]$ hadoop version</span><br><span class="line">Hadoop 3.1.3</span><br><span class="line">Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579</span><br><span class="line">Compiled by ztang on 2019-09-12T02:47Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum ec785077c385118ac91aadde5ec9799</span><br><span class="line">This command was run using /home/centos/hadoop/share/hadoop/common/hadoop-common-3.1.3.jar</span><br></pre></td></tr></table></figure>
<p>将jdk8和hadoop解压，放置在~/下,在/etc/profile中配置环境变量,<code>sudo vim /etc/profile</code>命令，打开后在结尾追加</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/centos/jdk8</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">export HADOOP_HOME=/home/centos/hadoop</span><br><span class="line">export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</span><br></pre></td></tr></table></figure>
<p>然后使用<code>source /etc/profile</code>编译文件。然后可以输入上面的版本命令，看看信息。</p>
<h2 id="暂时关闭防火墙"><a href="#暂时关闭防火墙" class="headerlink" title="暂时关闭防火墙"></a>暂时关闭防火墙</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld.service</span><br></pre></td></tr></table></figure>

<h2 id="规定三台服务器的IP-这个ip地址是随意的。"><a href="#规定三台服务器的IP-这个ip地址是随意的。" class="headerlink" title="规定三台服务器的IP,这个ip地址是随意的。"></a>规定三台服务器的IP,这个ip地址是随意的。</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.181.129 centos1</span><br><span class="line">192.168.181.130 centos2</span><br><span class="line">192.168.181.131 centos3</span><br></pre></td></tr></table></figure>
<p>修改/etc/host文件，配置映射文件,将上面的配置放在该文件中。</p>
<h2 id="克隆虚拟机"><a href="#克隆虚拟机" class="headerlink" title="克隆虚拟机"></a>克隆虚拟机</h2><p>关闭虚拟机，点击菜单栏的虚拟机，选择管理，选择克隆，选择完全克隆，选择存储位置，然后进行克隆。如果你的当前磁盘是固态，那么是非常非常快的，不超过1分钟。</p>
<h2 id="配置网络"><a href="#配置网络" class="headerlink" title="配置网络"></a>配置网络</h2><p>打开全部虚拟机，其实完全克隆后，主机名已经变了，从centos1到centos3，hostname是直接改过了的。你可以到设置-详细信息-用户里改一下用户名称为主机名名称。<br>然后配置一下另外两个的网络,这里配ip，上面好像没说。可以直接到设置-网络，点有线连接下的那个网卡点IPV4，设为手动，分别填写，你在虚拟网络配置里配置的网络ip，或者可以不用这用，因为是hdcp的，所以直接ifconfig查看ip结构，把之前预先设计好的ip填进去，并把地址和子网掩码放进去，网关为.1，DNS设为114.114.114.114。然后ipv6这点击禁用。<br>然后尝试互相ping并ping一下<a target="_blank" rel="noopener" href="http://www.baidu.com看是否外网可以通。">www.baidu.com看是否外网可以通。</a><br>如果找不到地方，看不到英文的话，可以设置成中文，点设置-region-language，看上部分，选择语言，汉语中文，然后根据他的提示重启一下就好了，让修改home下的文件名称的话，<strong>选不修改且不再提示</strong>。</p>
<h2 id="虚拟机免密登录"><a href="#虚拟机免密登录" class="headerlink" title="虚拟机免密登录"></a>虚拟机免密登录</h2><p>在三台机器都执行以下指令</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa ；</span><br><span class="line">ssh-copy-id IP (自身)</span><br><span class="line">ssh-copy-id IP (其他节点)</span><br></pre></td></tr></table></figure>
<p>执行第一条指令时，回车三次，第二条和第三条是一样的，输入yes，然后输入ip的密码。<br>这三条指令，每台机器都执行一遍。<br>执行完毕后，从129开始，ssh centos1,ssh centos2,ssh centos3,三台机器都执行一遍这三个命令，表示要ssh连接一次，记住ip。</p>
<h1 id="第二章-集群配置"><a href="#第二章-集群配置" class="headerlink" title="第二章 集群配置"></a>第二章 集群配置</h1><h2 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h2><table>
<thead>
<tr>
<th></th>
<th>centos1</th>
<th>centos2</th>
<th>centos3</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td>NameNode<br/>DataNode</td>
<td>DataNode</td>
<td>SecondaryNameNode<br/>DataNode</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td>NodeManager<br/>ResourceManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<p>每个DataNode一定是有个NodeManager的。NameNode是比较重要的，管理所有的节点，所以一般是单独占一个服务器，但是只有三台机器又要做三个机器的数据存储。</p>
<h2 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h2><p>进入hadoop的etc/hadoop</p>
<ol>
<li>配置hadoop-env.sh，找到被注释的JAVA_HOME与HADOOP_HOME，解开注释，修改为对应位置。</li>
<li>修改core-site.xml,添加如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 指定HDFS中的NameNode地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">   &lt;value&gt;hdfs:&#x2F;&#x2F;centos1:9000&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 指定hadoop运行时产生的临时文件 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">   &lt;value&gt;&#x2F;home&#x2F;centos&#x2F;hadoop&#x2F;data&#x2F;hadoopdata&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure></li>
<li>修改hdfs-site.xml，添加如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;centos3:50090&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;description&gt;secondarynamenode运行节点的信息,和namenode不同节点&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;description&gt;HDFS的数据块副本存储个数&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;dfs.namenode.http-address&lt;&#x2F;name&gt;</span><br><span class="line">	&lt;value&gt;centos1:50070&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;home&#x2F;centos&#x2F;hadoop&#x2F;data&#x2F;name&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;description&gt;namenode的数据存储目录&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;home&#x2F;centos&#x2F;hadoop&#x2F;data&#x2F;data&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;description&gt;datanode的数据存储目录&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure></li>
<li>修改yarn-site.xml，添加如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;centos2&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;description&gt;yarn集群为mapreduce程序提供的shuffle服务&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
有时候，有的yarn还是什么日志报错，具体问题忘了，是需要配置<code>yarn.application.classpath</code>的属性，它的值通过<code>hadoop classpath</code>命令打出来<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.application.classpath&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;&#x2F;home&#x2F;hadoop&#x2F;install&#x2F;hadoop&#x2F;etc&#x2F;hadoop:&#x2F;home&#x2F;hadoop&#x2F;install&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;*:&#x2F;home&#x2F;hadoop&#x2F;install&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;*:&#x2F;home&#x2F;hadoop&#x2F;install&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;hdfs:&#x2F;home&#x2F;hadoop&#x2F;install&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;hdfs&#x2F;lib&#x2F;*:&#x2F;home&#x2F;hadoop&#x2F;install&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;hdfs&#x2F;*:&#x2F;home&#x2F;hadoop&#x2F;install&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;lib&#x2F;*:&#x2F;home&#x2F;hadoop&#x2F;install&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;*:&#x2F;home&#x2F;hadoop&#x2F;install&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;yarn:&#x2F;home&#x2F;hadoop&#x2F;install&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;yarn&#x2F;lib&#x2F;*:&#x2F;home&#x2F;hadoop&#x2F;install&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;yarn&#x2F;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure></li>
<li>修改mapred-site.xml，添加如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;centos1:10020&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;centos1:19888&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="hadoop拷贝到centos2和centos3"><a href="#hadoop拷贝到centos2和centos3" class="headerlink" title="hadoop拷贝到centos2和centos3"></a>hadoop拷贝到centos2和centos3</h2><p>之前的克隆的时候的从centos1中克隆来的，但没有配置，在这里，先把centos2和centos3的hadoop目录删了，通过命令吧刚配置的centos1的hadoop拷贝过去。当前，如果读到这，你还没有克隆的话，先把这里的配置配了再克隆吧</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r .&#x2F;hadoop centos@centos2:~&#x2F;</span><br><span class="line">scp -r .&#x2F;hadoop centos@centos3:~&#x2F;</span><br></pre></td></tr></table></figure>
<h2 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h2><ol>
<li>首先执行一次格式化hdf,centos1中(namenode)<br>这是必须的，但下此再要重启服务的话就不需要了。<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[centos@centos1 hadoop]$ hdfs namenode -format</span><br><span class="line">2019-12-06 08:00:49,798 INFO namenode.NameNode: STARTUP_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host = centos1/192.168.181.129</span><br><span class="line">STARTUP_MSG:   args = [-format]</span><br><span class="line">STARTUP_MSG:   version = 3.1.3</span><br><span class="line">STARTUP_MSG:   classpath = /home/centos/hadoop/etc/hadoop:/home/ce</span><br><span class="line">....省略</span><br></pre></td></tr></table></figure></li>
<li>启动start-dfs,start-yarn</li>
</ol>
<p>在centos1中，也就是namenode中启动<code>start-dfs.sh</code>,必须在namenode中启动<br>然后去centos2,也就是resourceManager，<code>start-yarn.sh</code>,必须在resourceManager上启动<br>以上的启动位置，是强制的，否则出现服务启动缺失等其他异常情况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[centos@centos1 hadoop]$ start-dfs.sh </span><br><span class="line">Starting namenodes on [centos1]</span><br><span class="line">Starting datanodes</span><br><span class="line">Starting secondary namenodes [centos3]</span><br><span class="line">[centos@centos2 hadoop]$ start-yarn.sh </span><br><span class="line">Starting resourcemanager</span><br><span class="line">Starting nodemanagers</span><br><span class="line">[centos@centos1 hadoop]$ jps</span><br><span class="line">15426 NameNode</span><br><span class="line">15938 Jps</span><br><span class="line">15561 DataNode</span><br><span class="line">15868 NodeManager</span><br><span class="line">[centos@centos2 hadoop]$ jps</span><br><span class="line">15028 ResourceManager</span><br><span class="line">15428 Jps</span><br><span class="line">14872 DataNode</span><br><span class="line">15161 NodeManager</span><br><span class="line">[centos@centos3 hadoop]$ jps</span><br><span class="line">14214 SecondaryNameNode</span><br><span class="line">14104 DataNode</span><br><span class="line">14317 NodeManager</span><br><span class="line">14414 Jps</span><br></pre></td></tr></table></figure>
<p>这里，因为测试的时候忘记执行<code>hdfs namenode -format</code>命令了，所以出现了错误，所以日志目录没有删除，这里如果第一次启动的话会提示WARN没有logs文件，会自动创建。不必在意。</p>
<p>以上服务，一个都不能少。如果少了哪个，则查看nameNode机器的日志或其他节点的日志。<br>如果集群启动失败，则必须把服务先关停，然后删除每个节点的data 和logs目录，然后再hdfs namenode -format。</p>
<p>记得关闭防火墙，避免无法访问web服务，如果不想关闭，那么去开放端口吧。每次重启需要执行关闭命令，也可以永久关闭。</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 查看状态</span><br><span class="line">systemctl status firewalld.service</span><br><span class="line"># 临时关闭</span><br><span class="line">systemctl stop firewalld.service</span><br></pre></td></tr></table></figure>
<p>然后可以再宿主机上，通过访问centos1的ip地址:50070端口即可访问web页面。这里的地址是192.168.181.129:50070.<br>进入后选择最后一项Utilities，选择呢Brower the file system即可看到HDFS文件系统。</p>
<h2 id="HDFS文件测试"><a href="#HDFS文件测试" class="headerlink" title="HDFS文件测试"></a>HDFS文件测试</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p &#x2F;user&#x2F;input</span><br><span class="line">hdfs dfs -put abc.txt &#x2F;user&#x2F;input</span><br><span class="line"></span><br><span class="line">hdfs dfs -mkdir -p &#x2F;user&#x2F;package</span><br><span class="line">hdfs dfs -put ~&#x2F;jdk-8u231-linux-x64.tar.gzip &#x2F;user&#x2F;package</span><br></pre></td></tr></table></figure>
<p>上传了一个小文件，一个大文件。查看centos1:50070,发现是有的。以128M为一个block，abc.txt占用一个块，jdk为俩个块。</p>
<p>如何在本地查找,进入到data目录中找<br><code>/home/centos/hadoop/data/data/current/BP-2112740988-192.168.181.129-1562738006972/current/finalized/subdir0/subdir0</code></p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Hadoop/Docker部署完全分布式Hadoop" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/03/22/Hadoop/Docker%E9%83%A8%E7%BD%B2%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8FHadoop/"
    >Docker部署完全分布式Hadoop</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/22/Hadoop/Docker%E9%83%A8%E7%BD%B2%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8FHadoop/" class="article-date">
  <time datetime="2020-03-22T07:42:52.000Z" itemprop="datePublished">2020-03-22</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>

      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      
      

      
      <h1 id="Docker部署完全分布式Hadoop"><a href="#Docker部署完全分布式Hadoop" class="headerlink" title="Docker部署完全分布式Hadoop"></a>Docker部署完全分布式Hadoop</h1><h2 id="编写镜像文件构建镜像"><a href="#编写镜像文件构建镜像" class="headerlink" title="编写镜像文件构建镜像"></a>编写镜像文件构建镜像</h2><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> centos</span><br><span class="line"><span class="keyword">MAINTAINER</span> cgq_rain@<span class="number">163</span>.com</span><br><span class="line"><span class="comment"># 切换工作目录</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /usr</span></span><br><span class="line"><span class="comment"># 执行命令创建目录</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mkdir jdk</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mkdir hadoop</span></span><br><span class="line"><span class="comment"># 添加本地文件到镜像中</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> jdk1.8 /usr/jdk</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> hadoop /usr/hadoop</span></span><br><span class="line"><span class="comment"># 配置环境变量</span></span><br><span class="line"><span class="keyword">ENV</span> JAVA_HOME=/usr/jdk</span><br><span class="line"><span class="keyword">ENV</span> HADOOP_HOME=/usr/hadoop</span><br><span class="line"><span class="keyword">ENV</span> PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</span><br><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum -y install net-tools</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum -y install openssh-server</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum -y install openssh-clients</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum -y install passwd</span></span><br></pre></td></tr></table></figure>

<p>要添加文件，jdk1.8和hadoop都是目录，而<code>ADD jdk1.8 /usr/jdk</code>表示将当前dockerfile同目录下的jdk1.8目录下所有数据添加到容器的/usr/jdk目录中。</p>
<p>使用命令构建镜像，执行命令完成后，镜像会出现在docker images中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -f Dockerfile -t chen/base-hadoop:0.1 .</span><br></pre></td></tr></table></figure>

<ul>
<li>-f参数表示dockerfile文件</li>
<li>-t表示镜像名和版本</li>
<li>最后的 . 表示当前目录</li>
</ul>
<p>确保dockerfile所属目录尽量是干净的目录。</p>
<h2 id="配置自定义网络"><a href="#配置自定义网络" class="headerlink" title="配置自定义网络"></a>配置自定义网络</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network create --subnet=192.168.0.0/24 hadoop</span><br></pre></td></tr></table></figure>

<p>自定义网络，192.168.0.0网段，网络名为hadoop，默认为桥接网络。</p>
<p>在创建完成后在<code>ifconfig</code>中会看到定义的网络</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">br-3c0945ebc199: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.0.1  netmask 255.255.255.0  broadcast 192.168.0.255</span><br><span class="line">        inet6 fe80::42:67ff:fee6:b900  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 02:42:67:e6:b9:00  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 19586  bytes 4595681 (4.3 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 14136  bytes 3194964 (3.0 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure>

<p>此时，如果容器已经配置好ip，则虚拟机和容器是可以互相ping通的。</p>
<h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name hadoop1  --network=hadoop --ip=192.168.0.11 -dit --privileged chen/base-hadoop:0.1 /usr/sbin/init</span><br></pre></td></tr></table></figure>

<p>执行此命令，需要注意顺序问题，否则启动会出错。另外因为如果要是用centos的systemctl命令则需要 –privileged 和/usr/sbin/init 参数。</p>
<p>分别启动三个容器命名为hadoop1，hadoop2，hadoop3，指定ip为11,12,13</p>
<h2 id="修改root密码"><a href="#修改root密码" class="headerlink" title="修改root密码"></a>修改root密码</h2><p>这个在配置免密登录是需要用到密码</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">passwd root</span><br></pre></td></tr></table></figure>

<p>passwd命令默认没有，在构建镜像是已经安装了passwd依赖。</p>
<h2 id="配置hosts文件"><a href="#配置hosts文件" class="headerlink" title="配置hosts文件"></a>配置hosts文件</h2><p>三个容器都配置,修改/etc/hosts文件</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.0.11	centos1</span><br><span class="line">192.168.0.12	centos2</span><br><span class="line">192.168.0.13	centos3</span><br></pre></td></tr></table></figure>

<h2 id="免密登录配置"><a href="#免密登录配置" class="headerlink" title="免密登录配置"></a>免密登录配置</h2><p>三个容器都执行这三条命令。执行<code>ssh-copy-id</code>命令时输入root密码就是修改的root密码。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id 192.168.0.11</span><br><span class="line">ssh-copy-id 192.168.0.12</span><br><span class="line">ssh-copy-id 192.168.0.13</span><br></pre></td></tr></table></figure>

<h2 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h2><table>
<thead>
<tr>
<th></th>
<th>centos1</th>
<th>centos2</th>
<th>centos3</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td>NameNode<br/>DataNode</td>
<td>DataNode</td>
<td>SecondaryNameNode<br/>DataNode</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td>NodeManager ResourceManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<p>以上是集群规划。</p>
<p>每个DataNode一定是有个NodeManager的。NameNode是比较重要的，管理所有的节点，所以一般是单独占一个服务器，但是只有三台机器又要做三个机器的数据存储。</p>
<p><strong>修改配置文件</strong></p>
<ol>
<li>配置hadoop-env.sh，找到被注释的JAVA_HOME与HADOOP_HOME，解开注释，修改为对应位置。</li>
<li>修改core-site.xml,添加如下</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 指定HDFS中的NameNode地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">   &lt;value&gt;hdfs:&#x2F;&#x2F;centos1:9000&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 指定hadoop运行时产生的临时文件 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">   &lt;value&gt;&#x2F;usr&#x2F;hadoop&#x2F;data&#x2F;hadoopdata&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>修改hdfs-site.xml，添加如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;centos3:50090&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;description&gt;secondarynamenode运行节点的信息,和namenode不同节点&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;description&gt;HDFS的数据块副本存储个数&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;dfs.namenode.http-address&lt;&#x2F;name&gt;</span><br><span class="line">	&lt;value&gt;centos1:50070&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;usr&#x2F;hadoop&#x2F;data&#x2F;name&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;description&gt;namenode的数据存储目录&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;usr&#x2F;hadoop&#x2F;data&#x2F;data&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;description&gt;datanode的数据存储目录&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure></li>
<li>修改yarn-site.xml，添加如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;centos2&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;description&gt;yarn集群为mapreduce程序提供的shuffle服务&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
有时候，有的yarn还是什么日志报错，具体问题忘了，是需要配置<code>yarn.application.classpath</code>的属性，它的值通过<code>hadoop classpath</code>命令打出来。hadoop是直接添加到镜像里的，所以要想不改必须此处配置的和环境变量一致。建议进入容器更改这个值。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.application.classpath&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;&#x2F;usr&#x2F;hadoop&#x2F;etc&#x2F;hadoop:&#x2F;usr&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;*:&#x2F;usr&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;*:&#x2F;usr&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;hdfs:&#x2F;usr&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;hdfs&#x2F;lib&#x2F;*:&#x2F;usr&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;hdfs&#x2F;*:&#x2F;usr&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;lib&#x2F;*:&#x2F;usr&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;*:&#x2F;usr&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;yarn:&#x2F;usr&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;yarn&#x2F;lib&#x2F;*:&#x2F;usr&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;yarn&#x2F;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure></li>
<li>修改mapred-site.xml，添加如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;centos1:10020&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;centos1:19888&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<blockquote>
<p><strong>建议在构建镜像前把hadoop配置好，确定好环境变量，把hadoop配置文件配置的和环境变量确定一致。</strong></p>
</blockquote>
<h2 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h2><h3 id="修改启动脚本配置"><a href="#修改启动脚本配置" class="headerlink" title="修改启动脚本配置"></a>修改启动脚本配置</h3><p>启动集群前需要配置一下start-dfs.sh文件。因为容器默认用户是root，而启动start-dfs.sh会报错</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ERROR: Attempting to operate on hdfs namenode as root</span><br><span class="line">ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.</span><br><span class="line">Stopping datanodes</span><br><span class="line">ERROR: Attempting to operate on hdfs datanode as root</span><br><span class="line">ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.</span><br><span class="line">Stopping secondary namenodes [centos3]</span><br><span class="line">ERROR: Attempting to operate on hdfs secondarynamenode as root</span><br><span class="line">ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.</span><br></pre></td></tr></table></figure>

<p>这个问题在三台虚拟机上，使用普通用户处理是没有问题的。</p>
<p>错误说没有配置HDFS_NAMENODE_USER和HDFS_SECONDARYNAMENODE_USER，因此需要进行配置。</p>
<p>因此，在启动集群之前配置start-dfs.sh和stop-dfs.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HDFS_DATANODE_USER=root</span><br><span class="line">HDFS_NAMENODE_USER=root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER=root</span><br></pre></td></tr></table></figure>

<p>以及start-yarn.sh和stop-yarn.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>

<p>以上配置每台机器都需要。且需要把配置放在第二行，bin bash声明后面</p>
<h3 id="格式化并启动集群"><a href="#格式化并启动集群" class="headerlink" title="格式化并启动集群"></a>格式化并启动集群</h3><p>格式化,在namenode中,就是centos1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>



<p>在hadoop1的机器执行命令，也就是centos1,必须在namenode中启动hdfs</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>在hadoop2的机器上执行命令，也就是centos2,必须在resourceManager中启动yarn,这也是强制的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>

<h3 id="启动结果"><a href="#启动结果" class="headerlink" title="启动结果"></a>启动结果</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@901ae10904fb usr]# jps</span><br><span class="line">7012 Jps</span><br><span class="line">6393 DataNode</span><br><span class="line">6732 NodeManager</span><br><span class="line">6253 NameNode</span><br><span class="line"></span><br><span class="line">[root@9550b16ec82d hadoop]# jps</span><br><span class="line">3952 Jps</span><br><span class="line">3602 NodeManager</span><br><span class="line">3465 ResourceManager</span><br><span class="line">3278 DataNode</span><br><span class="line"></span><br><span class="line">[root@59d708cc0206 hadoop]# jps</span><br><span class="line">2384 SecondaryNameNode</span><br><span class="line">2466 NodeManager</span><br><span class="line">2275 DataNode</span><br><span class="line">2631 Jps</span><br></pre></td></tr></table></figure>

<p>从上至下分别为centos1到centos3的输出进程。必须满足此种情况才能正常使用。如果nameNode无法正常启动，首先检查配置文件，其次重新格式化一次，在执行<code>hdfs namenode -format</code>之前，先把三台机器的data目录和logs目录全部删除，再执行格式化命令，确保centos1先启动dfs，如果namenode没有问题，再centos2执行yarn。否则需要在centos1中查找namenode的日志记录,查找namenode没有正常启动的原因。</p>
<h3 id="关闭集群"><a href="#关闭集群" class="headerlink" title="关闭集群"></a>关闭集群</h3><p>首先在centos2上关闭stop-yarn.sh，再在centos1上stop-dfs.sh</p>
<h2 id="测试hdfs并访问50070"><a href="#测试hdfs并访问50070" class="headerlink" title="测试hdfs并访问50070"></a>测试hdfs并访问50070</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p &#x2F;user&#x2F;input</span><br><span class="line">hdfs dfs -put a.txt &#x2F;user&#x2F;input</span><br></pre></td></tr></table></figure>

<p>在虚拟机上访问，192.168.0.11:50070就可以看到了。可以查看文件系统目录。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/19/">上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="page-number" href="/page/19/">19</a><span class="page-number current">20</span><a class="page-number" href="/page/21/">21</a><a class="page-number" href="/page/22/">22</a><span class="space">&hellip;</span><a class="page-number" href="/page/47/">47</a><a class="extend next" rel="next" href="/page/21/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2020-2021
        陈光奇
      </li>
      <li>
        
        Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
      </li>
      
      <li>
        <a href="http://www.beian.miit.gov.cn/" target="_black">京ICP备17065668号-3</a>
      </li>
      
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
      <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    <aside class="sidebar">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer.jpg" alt="雪里"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/photos">相册</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2020/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.png">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/share.js"></script>


<script src="/js/lazyload.min.js"></script>


<script>
  try {
    var typed = new Typed("#subtitle", {
      strings: ['面朝大海，春暖花开', '愿你一生努力，一生被爱', '想要的都拥有，得不到的都释怀'],
      startDelay: 0,
      typeSpeed: 200,
      loop: true,
      backSpeed: 100,
      showCursor: true
    });
  } catch (err) {
  }

</script>





<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/js/ayer.js"></script>



<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>



<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>

    
  </div>
</body>

</html>