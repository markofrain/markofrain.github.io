<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> 雪里</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover4.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">雪里</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.staticfile.org/typed.js/2.0.12/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['面朝大海，春暖花开', '愿你一生努力，一生被爱', '想要的都拥有，得不到的都释怀'],
        startDelay: 0,
        typeSpeed: 200,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  
  <article class="articles">
    
    
    
    
    <article
  id="post-Scala/Scala数据类型"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/05/04/Scala/Scala%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"
    >Scala数据类型</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/05/04/Scala/Scala%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/" class="article-date">
  <time datetime="2022-05-04T04:00:00.000Z" itemprop="datePublished">2022-05-04</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Scala/">Scala</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>单个元素直接就转成序列了 (Seq操作符)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(fields: <span class="type">Seq</span>[<span class="type">StructField</span>]): <span class="type">StructType</span> = <span class="type">StructType</span>(fields.toArray)</span><br><span class="line"></span><br><span class="line"><span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>,<span class="type">LongType</span>)::<span class="type">Nil</span>)</span><br></pre></td></tr></table></figure>

 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Spark/Spark-SQL"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/05/04/Spark/Spark-SQL/"
    >Spark SQL</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/05/04/Spark/Spark-SQL/" class="article-date">
  <time datetime="2022-05-03T16:00:00.000Z" itemprop="datePublished">2022-05-04</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><p>​        Spark SQL是一个用于结构化数据处理的Spark模块。与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了更多关于数据结构和正在执行的计算的信息。Spark SQL在内部使用这些额外的信息来执行额外的优化。与Spark SQL交互的方式有多种，包括SQL和Dataset API。当计算结果时，将使用相同的执行引擎，而与您使用的API/语言无关。这种统一意味着开发人员可以轻松地在不同的api之间来回切换，这些api提供了表达给定转换的最自然的方式。</p>
<p>​        SparkSQL 可以简化 RDD 的开发，提高开发效率，且执行效率非常快，所以实际工作中，基本上采用的就是 SparkSQL。Spark SQL 为了简化 RDD 的开发，提高开发效率，提供了 2 个编程抽象，类似 Spark Core 中的 RDD</p>
<ul>
<li>DataFrame</li>
<li>DataSet</li>
</ul>
<h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul>
<li><p>易整合：无缝的整合了 SQL 查询和 Spark 编程</p>
</li>
<li><p>统一的数据访问：使用相同的方式连接不同的数据源</p>
</li>
<li><p>兼容 Hive：在已有的仓库上直接运行 SQL 或者 HiveQL</p>
</li>
<li><p>标准数据连接：通过 JDBC 或者 ODBC 来连接</p>
</li>
</ul>
<h2 id="DataFrame和DataSet"><a href="#DataFrame和DataSet" class="headerlink" title="DataFrame和DataSet"></a>DataFrame和DataSet</h2><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>​        在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame所表示的二维表数据集的每一列都带有名称和类型.</p>
<p>​        同时，与 Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）。从 API </p>
<p>易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要</p>
<p>更加友好，门槛更低。</p>
<p><img src="https://fsats-blog.oss-cn-beijing.aliyuncs.com/blog/image-20220504224019368.png" alt="image-20220504224019368"></p>
<p>​        左侧的 RDD[Person]虽然以 Person 为类型参数，但 Spark 框架本身不了解 Person 类的内部结构。而右侧的 DataFrame 却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。</p>
<p>​        <strong>DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待</strong>。</p>
<p>​        DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计</p>
<p>划通过 Spark catalyst optimiser 进行优化。</p>
<h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>​        DataSet 是分布式数据集合。DataSet 是 Spark 1.6 中添加的一个新抽象，是 DataFrame的一个扩展。它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter等等）。</p>
<p><strong>DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象</strong></p>
<ul>
<li><p><strong>用户友好的 API 风格，既具有类型安全检查也具有 DataFrame 的查询优化特性</strong>；</p>
</li>
<li><p>用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet 中的字段名称；</p>
</li>
<li><p>DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。 </p>
</li>
<li><p>DataFrame 是 DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为 DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。获取数据时需要指定顺序</p>
</li>
</ul>
<p>​        简而言之的说，RDD只是一堆数据，他可以是任意列表或Bean类型的数据，但是它只处理数据，并不操心数据是什么类型。具体如果处理数据由开发者来操作。</p>
<p>​        而DataFrame可以认为是一个含数据类型的RDD，它存储了数据的元数据信息，有其元数据属性和其数据类型，可以简要地认为它类似一张表。可以使用SQL语句的方式操作，以及部分API。</p>
<p>​        DataSet是DataFrame的一个拓展，有更好的API处理风格，可以直接操作API(类似Mysql Plus)来做分析。</p>
<h3 id="核心编程操作"><a href="#核心编程操作" class="headerlink" title="核心编程操作"></a>核心编程操作</h3><p>​        Spark Core 中，如果想要执行应用程序，需要首先构建上下文环境对象 SparkContext，Spark SQL 其实可以理解为对 Spark Core 的一种封装，不仅仅在模型上进行了封装，上下文环境对象也进行了封装。</p>
<p>​        SparkSession 是 Spark 最新的 SQL 查询起始点，实质上是 SQLContext 和 HiveContext的组合，所以在 SQLContex 和 HiveContext 上可用的 API 在 SparkSession 上同样是可以使用的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br></pre></td></tr></table></figure>



<h3 id="RDD、DataFrame、DataSet-操作及互相转换"><a href="#RDD、DataFrame、DataSet-操作及互相转换" class="headerlink" title="RDD、DataFrame、DataSet 操作及互相转换"></a>RDD、DataFrame、DataSet 操作及互相转换</h3><h4 id="DataFrame-1"><a href="#DataFrame-1" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>特定泛型的DataSet <code>type DataFrame = Dataset[Row]</code></p>
<p>使用spark的read方法获取数据，以json为例，此处需要的json是一行经过压缩的数据才行，或者是多行json格式的压缩数据，否则会加载失败。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;<span class="attr">&quot;username&quot;</span>:<span class="string">&quot;zhangsan&quot;</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">12</span>&#125;,&#123;<span class="attr">&quot;username&quot;</span>:<span class="string">&quot;xiaohong&quot;</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">16</span>&#125;,&#123;<span class="attr">&quot;username&quot;</span>:<span class="string">&quot;xiaoming&quot;</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">20</span>&#125;]</span><br></pre></td></tr></table></figure>

<p>使用show方法，可以在控制打印数据。字段名是按照字典类型的顺序</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;json/user.json&quot;</span>)</span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="number">12</span>|zhangsan|</span><br><span class="line">| <span class="number">16</span>|xiaohong|</span><br><span class="line">| <span class="number">20</span>|xiaoming|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure>

<h5 id="SQL风格语法"><a href="#SQL风格语法" class="headerlink" title="SQL风格语法"></a>SQL风格语法</h5><p>使用的是SparkSession的方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个临时视图，仅在当前SparkSession会话有效</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;select * from user&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;select username,age from user&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;select avg(age) as avgAge from user&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h5 id="DSL风格"><a href="#DSL风格" class="headerlink" title="DSL风格"></a>DSL风格</h5><p>使用DSL风格，需要从SparkSession中导入包，直接写在该行代码下即可</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> row = df.filter(<span class="symbol">&#x27;age</span> &gt; <span class="number">15</span>).select($<span class="string">&quot;username&quot;</span>,$<span class="string">&quot;age&quot;</span>+<span class="number">1</span>).orderBy(<span class="symbol">&#x27;age</span>.desc).first()</span><br><span class="line">println(row)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">[xiaoming,<span class="number">21</span>]</span><br></pre></td></tr></table></figure>

<p>以上代码中，必须要引入<code>import spark.implicits._</code>，否则在编辑器中就会出现语法错误。</p>
<p>可以直接使用字段名作为参数，如果为参数进行更改或附加使用，则需要使用<code>$&quot;字段名&quot;</code>或<code>&#39;字段名</code>的方式引用字段，这样才能做操作。另外在这些多参方法中，如果其中一个字段使用了DSL语法，则所有字段都需要设置成DSL语法格式，否则依然会出现语法错误。</p>
<h4 id="DataSet-1"><a href="#DataSet-1" class="headerlink" title="DataSet"></a>DataSet</h4><p>可以通过序列创建DataSet，序列中可以是基本数据类集合类型或Bean类型。但必须要导入implicits转换器</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> seq = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">Int</span>] = seq.toDS()</span><br><span class="line">ds.show()</span><br></pre></td></tr></table></figure>

<p>其相关api方法与DataFrame基本一致</p>
<h4 id="互相转换"><a href="#互相转换" class="headerlink" title="互相转换"></a>互相转换</h4><h5 id="RDD-lt-gt-DataFrame"><a href="#RDD-lt-gt-DataFrame" class="headerlink" title="RDD &lt;=&gt; DataFrame"></a>RDD &lt;=&gt; DataFrame</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;小红&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;小明&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;小刚&quot;</span>, <span class="number">14</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = value.toDF(<span class="string">&quot;username&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Row</span>] = df.rdd</span><br></pre></td></tr></table></figure>

<h5 id="DataFrame-lt-gt-DataSet"><a href="#DataFrame-lt-gt-DataSet" class="headerlink" title="DataFrame &lt;=&gt; DataSet"></a>DataFrame &lt;=&gt; DataSet</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ds:<span class="type">Dataset</span>[<span class="type">User</span>] = df.as[<span class="type">User</span>]</span><br><span class="line"><span class="keyword">val</span> fd1:<span class="type">DataFrame</span> = ds.toDF()</span><br></pre></td></tr></table></figure>

<h5 id="RDD-lt-gt-DataSet"><a href="#RDD-lt-gt-DataSet" class="headerlink" title="RDD &lt;=&gt; DataSet"></a>RDD &lt;=&gt; DataSet</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ds1 = value.map(r =&gt; <span class="type">User</span>(r._1, r._2)).toDS()</span><br><span class="line"><span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">User</span>] = ds1.rdd</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">username:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>

<p>首先DataFrame是特定泛型的Dataset。</p>
<p>让RDD转DataFrame时，只需要知道元数据字段是什么即可，而转DataSet时,需要一个强类型，只需要构建成Bean即可转换。</p>
<p>DataFrame转换DataSet时，因为缺少强类型，所以需要通过as来转换成具体的Bean泛型。反之，向弱转换直接toDF即可。</p>
<p>当向RDD转换时，直接rdd即可。</p>
<h3 id="三者之间的关系"><a href="#三者之间的关系" class="headerlink" title="三者之间的关系"></a>三者之间的关系</h3><p>​        在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是 DataFrame 和 DataSet。他们和 RDD 有什么区别呢？首先从版本的产生上来看。</p>
<ul>
<li>Spark1.0 =&gt; RDD</li>
<li>Spark1.3 =&gt; DataFrame</li>
<li>Spark1.6 =&gt; Dataset</li>
</ul>
<h4 id="三者共性"><a href="#三者共性" class="headerlink" title="三者共性"></a>三者共性</h4><ul>
<li><p>RDD、DataFrame、DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数据提供便利; </p>
</li>
<li><p>三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到Action 如 foreach 时，三者才会开始遍历运算; </p>
</li>
<li><p>三者有许多共同的函数，如 filter，排序等; </p>
</li>
<li><p>在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包:import spark.implicits._（在创建好 SparkSession 对象后尽量直接导入）</p>
</li>
<li><p>三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</p>
</li>
<li><p>三者都有 partition 的概念</p>
</li>
<li><p>DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型</p>
</li>
</ul>
<h4 id="三者区别"><a href="#三者区别" class="headerlink" title="三者区别"></a>三者区别</h4><p><strong>RDD</strong></p>
<ul>
<li>RDD 一般和 spark mllib 同时使用</li>
<li>RDD 不支持 sparksql 操作</li>
</ul>
<p><strong>DataFrame</strong></p>
<ul>
<li>与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值</li>
<li>DataFrame 与 DataSet 一般不与 spark mllib 同时使用</li>
<li>DataFrame 与 DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能注册临时表/视窗，进行 sql 语句操作</li>
<li>DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然</li>
</ul>
<h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><ul>
<li>Dataset 和 DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同。DataFrame 其实就是 DataSet 的一个特例 type DataFrame = Dataset[Row]</li>
<li>DataFrame 也可以叫 Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性中的第七条提到的模式匹配拿出特定字段。而 Dataset 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息</li>
</ul>
<p><img src="https://fsats-blog.oss-cn-beijing.aliyuncs.com/blog/image-20220505002901546.png" alt="image-20220505002901546"></p>
<h2 id="UDF、UDAF-用户自定义函数"><a href="#UDF、UDAF-用户自定义函数" class="headerlink" title="UDF、UDAF 用户自定义函数"></a>UDF、UDAF 用户自定义函数</h2><p>​        用户定义函数 (UDF) 是 Spark SQL 的一项功能，当系统的内置函数不足以执行所需的任务时，它允许用户定义自己的函数。在 Spark SQL 中使用 UDF，首先需要定义函数，然后向 Spark 注册函数，最后调用注册的函数。用户定义函数可以作用于单行或一次作用于多行。Spark SQL 还支持 UDF、UDAF 和 UDTF 的现有 Hive 实现的集成。</p>
<p>用户可以通过 spark.udf 功能添加自定义函数，实现自定义功能。</p>
<h3 id="UDF-用户定义函数"><a href="#UDF-用户定义函数" class="headerlink" title="UDF 用户定义函数"></a>UDF 用户定义函数</h3><p>用户定义函数 (UDF) 是作用于一行的用户可编程例程。</p>
<p>要定义用户定义函数的属性，用户可以使用该类中定义的一些方法。</p>
<ul>
<li><p>asNonNullable(): UserDefinedFunction</p>
<p>将 UserDefinedFunction 更新为不可为空。即UDF的返回值不能为空</p>
</li>
<li><p>asNondeterministic(): UserDefinedFunction</p>
<p>将 UserDefinedFunction 更新为不确定。即每次入参返回的结果都可能不一样</p>
</li>
<li><p>withName(name: String): UserDefinedFunction</p>
<p>使用给定名称更新 UserDefinedFunction。</p>
</li>
</ul>
<p>以下代码中的udf函数构建的对象是UserDefinedFunction抽象类的子类SparkUserDefinedFunction</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.udf</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> random = udf(() =&gt; <span class="type">Math</span>.random())</span><br><span class="line">spark.udf.register(<span class="string">&quot;random&quot;</span>,random.asNondeterministic())</span><br><span class="line"><span class="comment">// 或者不需要显示构建udf</span></span><br><span class="line"><span class="comment">//spark.udf.register(&quot;random&quot;,()=&gt;Math.random())</span></span><br><span class="line">spark.sql(<span class="string">&quot;select random()&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+------------------+</span><br><span class="line">|             <span class="type">UDF</span>()|</span><br><span class="line">+------------------+</span><br><span class="line">|<span class="number">0.5220181936811188</span>|</span><br><span class="line">+------------------+</span><br></pre></td></tr></table></figure>

<p><strong>定义两个参数的udf函数</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spark.udf.register(<span class="string">&quot;strLenScala&quot;</span>, (_: <span class="type">String</span>).length + (_: <span class="type">Int</span>))</span><br><span class="line"><span class="comment">// 两种表达式定义的一样</span></span><br><span class="line"><span class="comment">// spark.udf.register(&quot;strLenScala&quot;, (s: String, i: Int) =&gt; s.length + i)</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT strLenScala(&#x27;test&#x27;, 1)&quot;</span>).show()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+--------------------+</span><br><span class="line">|strLenScala(test, <span class="number">1</span>)|</span><br><span class="line">+--------------------+</span><br><span class="line">|                   <span class="number">5</span>|</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>

<p><strong>定义作为where条件的UDF</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">spark.udf.register(<span class="string">&quot;oneArgFilter&quot;</span>, (n: <span class="type">Int</span>) =&gt; &#123; n &gt; <span class="number">5</span> &#125;)</span><br><span class="line">spark.range(<span class="number">1</span>, <span class="number">10</span>).createOrReplaceTempView(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM test WHERE oneArgFilter(id)&quot;</span>).show()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">6</span>|</span><br><span class="line">|  <span class="number">7</span>|</span><br><span class="line">|  <span class="number">8</span>|</span><br><span class="line">|  <span class="number">9</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>

<h3 id="UDAF弱类型实现-新版3-0-0以后已标记为弃用-实现UserDefinedAggregateFunction类"><a href="#UDAF弱类型实现-新版3-0-0以后已标记为弃用-实现UserDefinedAggregateFunction类" class="headerlink" title="UDAF弱类型实现(新版3.0.0以后已标记为弃用) 实现UserDefinedAggregateFunction类"></a>UDAF弱类型实现(新版3.0.0以后已标记为弃用) 实现UserDefinedAggregateFunction类</h3><p>用户定义的聚合函数 (UDAF) 是用户可编程的例程，它们一次作用于多行并返回单个聚合值作为结果。</p>
<p>需要实现<code>org.apache.spark.sql.expressions.UserDefinedAggregateFunction</code>类</p>
<p>以模拟avg平均值计算为例</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">DoubleType</span>, <span class="type">LongType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 定义了求平均数的UDF</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 此聚合函数输入参数的数据类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>,<span class="type">LongType</span>)::<span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 聚合缓冲区中值的数据类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;sum&quot;</span>, <span class="type">LongType</span>), <span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 返回值的数据类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这个函数是否总是在相同的输入上返回相同的输出</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 初始化给定的聚合缓冲区。缓冲区本身是一个“行” 继承自Row，</span></span><br><span class="line">  <span class="comment">// 除了标准的方法(如在索引中检索值(如get()， getBoolean()))之外，</span></span><br><span class="line">  <span class="comment">// 还提供了更新其值的机会。注意，缓冲区内的数组和映射仍然是不可变的</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//buffer(0) = 0</span></span><br><span class="line">    <span class="comment">//buffer(1) = 0</span></span><br><span class="line">    buffer.update(<span class="number">0</span>,<span class="number">0</span>L)</span><br><span class="line">    buffer.update(<span class="number">1</span>,<span class="number">0</span>L)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 使用来自&#x27; input &#x27;的新输入数据更新给定的聚合缓冲区&#x27; buffer &#x27;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer.update(<span class="number">0</span>, buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>)) <span class="comment">// 增加值</span></span><br><span class="line">      buffer.update(<span class="number">1</span>, buffer.getLong(<span class="number">1</span>) + <span class="number">1</span>L) <span class="comment">// 增加数量</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并两个聚合缓冲区，并将更新的缓冲区值存储回&#x27; buffer1 &#x27;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1.update(<span class="number">0</span>, buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>))</span><br><span class="line">    buffer1.update(<span class="number">1</span>, buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算最终结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">spark.udf.register(<span class="string">&quot;myAverage&quot;</span>,<span class="type">MyAverage</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;json/user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">&quot;select myAverage(age) as avg_age from user&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+-------+</span><br><span class="line">|avg_age|</span><br><span class="line">+-------+</span><br><span class="line">|   <span class="number">16.0</span>|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure>



<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.8/api/scala/index.html#org.apache.spark.sql.functions$">内置的 DataFrames 函数</a>提供了常见的聚合，例如<code>count()</code>, <code>countDistinct()</code>, <code>avg()</code>, <code>max()</code>,<code>min()</code>等。虽然这些函数是为 DataFrames 设计的，但 Spark SQL 还为其中一些在 <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.8/api/scala/index.html#org.apache.spark.sql.expressions.scalalang.typed$">Scala</a>和 <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.8/api/java/org/apache/spark/sql/expressions/javalang/typed.html">Java</a>中提供了类型安全版本，以处理强类型数据集</p>
<h3 id="UDAF强类型实现-实现-Aggregator类"><a href="#UDAF强类型实现-实现-Aggregator类" class="headerlink" title="UDAF强类型实现 实现 Aggregator类"></a>UDAF强类型实现 实现 Aggregator类</h3><p>强类型数据集的用户定义聚合围绕<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.2.1/api/scala/org/apache/spark/sql/expressions/Aggregator.html">Aggregator</a>抽象类</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoder</span>, <span class="type">Encoders</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">username: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Average</span>(<span class="params">var sum: <span class="type">Long</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">/**</span></span></span><br><span class="line"><span class="class"><span class="comment"> * 定义平均值,继承Aggregator，采用强类型</span></span></span><br><span class="line"><span class="class"><span class="comment"> */</span></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">MyAverage2</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">User</span>, <span class="type">Average</span>, <span class="type">Double</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 零值、初始值。是否满足任意b + 0 = b的性质</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Average</span> = <span class="type">Average</span>(<span class="number">0</span>L, <span class="number">0</span>L)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并两个值以产生一个新值。出于性能考虑，函数可以修改“buffer”并返回它，而不是构造一个新对象</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">Average</span>, a: <span class="type">User</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">    b.sum += a.age</span><br><span class="line">    b.count += <span class="number">1</span>L</span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并两个聚合缓冲区值</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Average</span>, b2: <span class="type">Average</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">    b1.sum += b2.sum</span><br><span class="line">    b1.count += b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 结果输出函数,计算最终结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Average</span>): <span class="type">Double</span> = reduction.sum.doubleValue() / reduction.count</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 为聚合缓冲区指定编码器</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Average</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 为最终输出值类型指定编码器</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">spark.udf.register(<span class="string">&quot;myAverage&quot;</span>,<span class="type">MyAverage</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds = spark.read.json(<span class="string">&quot;json/user.json&quot;</span>).as[<span class="type">User</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> average = <span class="type">MyAverage2</span>.toColumn.name(<span class="string">&quot;avg_age&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> result = ds.select(average)</span><br><span class="line">result.show()</span><br><span class="line">spark.stop()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+-------+</span><br><span class="line">|avg_age|</span><br><span class="line">+-------+</span><br><span class="line">|   <span class="number">16.0</span>|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure>



<h2 id="数据读取与保存"><a href="#数据读取与保存" class="headerlink" title="数据读取与保存"></a>数据读取与保存</h2> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Spark/Spark核心"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/05/02/Spark/Spark%E6%A0%B8%E5%BF%83/"
    >Spark核心</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/05/02/Spark/Spark%E6%A0%B8%E5%BF%83/" class="article-date">
  <time datetime="2022-05-01T16:00:00.000Z" itemprop="datePublished">2022-05-02</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><h4 id="闭包检查"><a href="#闭包检查" class="headerlink" title="闭包检查"></a>闭包检查</h4><p>​        从计算的角度, <strong>算子以外的代码都是在 Driver 端执行, 算子里面的代码都是在 Executor端执行</strong>。那么在 scala 的函数式编程中，就会导致<strong>算子内</strong>经常会用到<strong>算子外</strong>的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给 Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为<strong>闭包检测</strong>。Scala2.12 版本后闭包编译方式发生了改变。</p>
<p>​        因为算子内使用了算子外的数据，这样的话executor端就会使用Driver端的代码数据。而如果Driver端有自定义对象供executor端使用时，就会需要这个自定义对象，那么传输它就需要将其进行序列化后才能进行传输。</p>
<h4 id="序列化方法和属性"><a href="#序列化方法和属性" class="headerlink" title="序列化方法和属性"></a>序列化方法和属性</h4><p>​        从计算的角度, 算子以外的代码都是在 Driver 端执行, 算子里面的代码都是在 Executor端执行。</p>
<p>因此需要将自定义对象混入特质Serializable。该特质是来自于java io包的Serializable接口类</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Search</span>(<span class="params">query: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span></span>&#123;&#125;</span><br></pre></td></tr></table></figure>

<p>或者使用case关键字</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Search</span>(<span class="params">query: <span class="type">String</span></span>) </span>&#123;&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Serial</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.创建 SparkConf 并设置 App 名称</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;SparkCoreTest&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="comment">//2.创建 SparkContext，该对象是提交 Spark App 的入口</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//3.创建一个 RDD</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello spark&quot;</span>, <span class="string">&quot;hive&quot;</span>, <span class="string">&quot;atguigu&quot;</span>))</span><br><span class="line">    <span class="comment">//3.1 创建一个 Search 对象</span></span><br><span class="line">    <span class="keyword">val</span> search = <span class="keyword">new</span> <span class="type">Search</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">    <span class="comment">//3.2 函数传递，打印：ERROR Task not serializable</span></span><br><span class="line">    search.getMatch1(rdd).collect().foreach(println)</span><br><span class="line">    <span class="comment">//3.3 属性传递，打印：ERROR Task not serializable</span></span><br><span class="line">    <span class="comment">//search.getMatch2(rdd).collect().foreach(println)</span></span><br><span class="line">    <span class="comment">//4.关闭连接</span></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Search</span>(<span class="params">query: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    s.contains(query)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 函数序列化案例</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatch1</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    rdd.filter(isMatch)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 属性序列化案例</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatch2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    rdd.filter(x =&gt; x.contains(query))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="Kryo序列化框架"><a href="#Kryo序列化框架" class="headerlink" title="Kryo序列化框架"></a>Kryo序列化框架</h4><p>​        Java 的序列化能够序列化任何的类。但是比较重（字节多），序列化后，对象的提交也比较大。Spark 出于性能的考虑，Spark2.0 开始支持另外一种 Kryo 序列化机制。Kryo 速度是 Serializable 的 10 倍。当 RDD 在 Shuffle 数据的时候，简单数据类型、数组和字符串类型已经在 Spark 内部使用 Kryo 来序列化。</p>
<p>​        如果自定义对象需要使用Kryo序列化，则需要在SparkConf创建时指定spark的序列化器，并指定那些类使用该序列化。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;SparkCoreTest&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line"><span class="comment">// 替换默认的序列化机制</span></span><br><span class="line">.set(<span class="string">&quot;spark.serializer&quot;</span>,<span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line"><span class="comment">// 注册需要使用 kryo 序列化的自定义类</span></span><br><span class="line">.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">Search</span>]))</span><br></pre></td></tr></table></figure>

<p>注意：即使使用 Kryo 序列化，也要继承 Serializable 接口</p>
<h2 id="依赖关系"><a href="#依赖关系" class="headerlink" title="依赖关系"></a>依赖关系</h2><h4 id="血缘关系"><a href="#血缘关系" class="headerlink" title="血缘关系"></a>血缘关系</h4><p>​        RDD 只支持粗粒度转换，即在大量记录上执行的单个操作。将创建 RDD 的一系列 Lineage（血统）记录下来，以便恢复丢失的分区。RDD 的 Lineage 会记录 RDD 的元数据信息和转换行为，当该 RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;datas/1.txt&quot;</span>)</span><br><span class="line">println(fileRDD.toDebugString)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(wordRDD.toDebugString)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map((_,<span class="number">1</span>))</span><br><span class="line">println(mapRDD.toDebugString)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line">println(resultRDD.toDebugString)</span><br><span class="line">resultRDD.collect()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">(<span class="number">2</span>) datas/<span class="number">1.</span>txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at <span class="type">Dependen</span>.scala:<span class="number">15</span> []</span><br><span class="line"> |  datas/<span class="number">1.</span>txt <span class="type">HadoopRDD</span>[<span class="number">0</span>] at textFile at <span class="type">Dependen</span>.scala:<span class="number">15</span> []</span><br><span class="line">----------------------</span><br><span class="line">(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at <span class="type">Dependen</span>.scala:<span class="number">18</span> []</span><br><span class="line"> |  datas/<span class="number">1.</span>txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at <span class="type">Dependen</span>.scala:<span class="number">15</span> []</span><br><span class="line"> |  datas/<span class="number">1.</span>txt <span class="type">HadoopRDD</span>[<span class="number">0</span>] at textFile at <span class="type">Dependen</span>.scala:<span class="number">15</span> []</span><br><span class="line">----------------------</span><br><span class="line">(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at <span class="type">Dependen</span>.scala:<span class="number">21</span> []</span><br><span class="line"> |  <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at <span class="type">Dependen</span>.scala:<span class="number">18</span> []</span><br><span class="line"> |  datas/<span class="number">1.</span>txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at <span class="type">Dependen</span>.scala:<span class="number">15</span> []</span><br><span class="line"> |  datas/<span class="number">1.</span>txt <span class="type">HadoopRDD</span>[<span class="number">0</span>] at textFile at <span class="type">Dependen</span>.scala:<span class="number">15</span> []</span><br><span class="line">----------------------</span><br><span class="line">(<span class="number">2</span>) <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at reduceByKey at <span class="type">Dependen</span>.scala:<span class="number">24</span> []</span><br><span class="line"> +-(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at <span class="type">Dependen</span>.scala:<span class="number">21</span> []</span><br><span class="line">    |  <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at <span class="type">Dependen</span>.scala:<span class="number">18</span> []</span><br><span class="line">    |  datas/<span class="number">1.</span>txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at <span class="type">Dependen</span>.scala:<span class="number">15</span> []</span><br><span class="line">    |  datas/<span class="number">1.</span>txt <span class="type">HadoopRDD</span>[<span class="number">0</span>] at textFile at <span class="type">Dependen</span>.scala:<span class="number">15</span> []</span><br></pre></td></tr></table></figure>

<p>通过创建RDD，在算子计算后生成一个新的RDD。血缘关系其实是多个RDD质检的依赖关系，两个相邻的RDD是依赖关系，间接依赖的两个RDD是血缘关系。</p>
<p>该debugger记录就是每个RDD质检的依赖关系，记录了RDD的元数据信息和转换行为，包括每一步执行的<code>at textFile</code>,<code>at flatMap</code> ,<code>at map</code>,<code>at reduceByKey</code>。</p>
<h4 id="依赖关系-1"><a href="#依赖关系-1" class="headerlink" title="依赖关系"></a>依赖关系</h4><p>其实就是两个相邻 RDD 之间的关系</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;datas/1.txt&quot;</span>)</span><br><span class="line">println(fileRDD.dependencies)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(wordRDD.dependencies)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map((_,<span class="number">1</span>))</span><br><span class="line">println(mapRDD.dependencies)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line">println(resultRDD.dependencies)</span><br><span class="line">resultRDD.collect()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line"><span class="type">List</span>(org.apache.spark.<span class="type">OneToOneDependency</span>@<span class="number">32e54</span>a9d)</span><br><span class="line">----------------------</span><br><span class="line"><span class="type">List</span>(org.apache.spark.<span class="type">OneToOneDependency</span>@<span class="number">56</span>da7487)</span><br><span class="line">----------------------</span><br><span class="line"><span class="type">List</span>(org.apache.spark.<span class="type">OneToOneDependency</span>@<span class="number">59</span>cda16e)</span><br><span class="line">----------------------</span><br><span class="line"><span class="type">List</span>(org.apache.spark.<span class="type">ShuffleDependency</span>@<span class="number">34332</span>b8d)</span><br></pre></td></tr></table></figure>

<p>在RDD中dependencies存储了依赖关系数据。</p>
<h4 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h4><p>OneToOneDependency类继承自NarrowDependency，也被称为窄依赖，即一对一的依赖类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneToOneDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>)</span></span><br></pre></td></tr></table></figure>

<p>窄依赖表示每一个父(上游)RDD 的 Partition <strong>最多</strong>被子（下游）RDD 的<strong>一个</strong> Partition 使用，窄依赖我们形象的比喻为独生子女。</p>
<p>即从textFile到flatMap到map这三个算子，均是一对一的窄依赖，且不存在Shuffle操作，也就是说数据不会打乱重组。各分区内数据在执行完一个算子后，执行下一个算子即可。</p>
<p><img src="https://fsats-blog.oss-cn-beijing.aliyuncs.com/blog/image-20220503003616912.png" alt="image-20220503003616912"></p>
<h4 id="宽依赖-与之对应的"><a href="#宽依赖-与之对应的" class="headerlink" title="宽依赖(与之对应的)"></a>宽依赖(与之对应的)</h4><p>​        宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle，总结：宽依赖我们形象的比喻为多生。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffleDependency</span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>, <span class="type">C</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    @transient private val _rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></span><br><span class="line"><span class="class"><span class="params">    val partitioner: <span class="type">Partitioner</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val serializer: <span class="type">Serializer</span> = <span class="type">SparkEnv</span>.get.serializer,</span></span></span><br><span class="line"><span class="class"><span class="params">    val keyOrdering: <span class="type">Option</span>[<span class="type">Ordering</span>[<span class="type">K</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val aggregator: <span class="type">Option</span>[<span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val mapSideCombine: <span class="type">Boolean</span> = false,</span></span></span><br><span class="line"><span class="class"><span class="params">    val shuffleWriterProcessor: <span class="type">ShuffleWriteProcessor</span> = new <span class="type">ShuffleWriteProcessor</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Dependency</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] </span></span><br></pre></td></tr></table></figure>

<p>​        在reduceByKey时进行了Shuffle操作，任何触发Shuffle操作都会引起ShuffleDependency宽依赖。</p>
<p>​        即在reduceByKey操作时，引发了Shuffle操作，Shuffle会将数据打乱，重新分区数据。这种操作中引发Shuffle的算子和前一个算子之间，在不同分区是处于多对多的一张关系，而不是各分区各自处理，会有一种交叉形式。因此会说被多个子（下游）RDD 的 Partition 依赖。</p>
<p><img src="https://fsats-blog.oss-cn-beijing.aliyuncs.com/blog/image-20220503003548569.png" alt="image-20220503003548569"></p>
<h4 id="RDD阶段划分"><a href="#RDD阶段划分" class="headerlink" title="RDD阶段划分"></a>RDD阶段划分</h4><p>​        DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。例如，DAG 记录了 RDD 的转换过程和任务的阶段。</p>
<p><img src="https://fsats-blog.oss-cn-beijing.aliyuncs.com/blog/image-20220503003800662.png" alt="image-20220503003800662">)<img src="https://fsats-blog.oss-cn-beijing.aliyuncs.com/blog/image-20220503003806231.png" alt="image-20220503003806231">)<img src="https://fsats-blog.oss-cn-beijing.aliyuncs.com/blog/image-20220503003821940.png" alt="image-20220503003821940"></p>
<p>​        在RDD执行任务中，首先会创建ResultStage阶段，当出现Shuffle操作时，即如果出现ShuffleDependency，会生成新的阶段。</p>
<h4 id="RDD任务划分"><a href="#RDD任务划分" class="headerlink" title="RDD任务划分"></a>RDD任务划分</h4><p>RDD 任务切分中间分为：Application、Job、Stage 和 Task</p>
<ul>
<li><p>Application：初始化一个 SparkContext 即生成一个 Application； </p>
</li>
<li><p>Job：一个 Action 算子就会生成一个 Job； </p>
</li>
<li><p>Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1； </p>
</li>
<li><p>Task：一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数。</p>
</li>
</ul>
<p>注意：Application-&gt;Job-&gt;Stage-&gt;Task 每一层都是 1 对 n 的关系。</p>
<h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><h4 id="RDD-cache持久化"><a href="#RDD-cache持久化" class="headerlink" title="RDD cache持久化"></a>RDD cache持久化</h4><p>​        <strong>RDD 通过 Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据以缓存在 JVM 的堆内存中</strong>。<strong>但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中，并供后面重用</strong>。</p>
<h5 id="触发后面的-action-算子时，该-RDD-将会被缓存在计算节点的内存中？"><a href="#触发后面的-action-算子时，该-RDD-将会被缓存在计算节点的内存中？" class="headerlink" title="触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中？"></a>触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中？</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello spark&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> value = rdd.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="keyword">val</span> value1 = value.map(s =&gt; &#123;</span><br><span class="line">    println(<span class="string">&quot;@@@@@@@@@@@&quot;</span>)</span><br><span class="line">    (s, <span class="number">1</span>)</span><br><span class="line">&#125;)</span><br><span class="line">println(value1.toDebugString)</span><br><span class="line">value1.cache()</span><br><span class="line">println(value1.toDebugString)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> oneResult = value1.reduceByKey(_ + _)</span><br><span class="line">oneResult.collect().foreach(println)</span><br><span class="line">sc.getPersistentRDDs.foreach(println)</span><br><span class="line">println(<span class="string">&quot;*********************&quot;</span>)</span><br><span class="line">println(value1.toDebugString)</span><br><span class="line"><span class="keyword">val</span> twoResult = value1.groupByKey()</span><br><span class="line">twoResult.collect().foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">(<span class="number">16</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at <span class="type">RDDpersistence</span>.scala:<span class="number">22</span> []</span><br><span class="line"> |   <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at flatMap at <span class="type">RDDpersistence</span>.scala:<span class="number">20</span> []</span><br><span class="line"> |   <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at <span class="type">RDDpersistence</span>.scala:<span class="number">18</span> []</span><br><span class="line">(<span class="number">16</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at <span class="type">RDDpersistence</span>.scala:<span class="number">22</span> [<span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line"> |   <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at flatMap at <span class="type">RDDpersistence</span>.scala:<span class="number">20</span> [<span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line"> |   <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at <span class="type">RDDpersistence</span>.scala:<span class="number">18</span> [<span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line">@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@</span><br><span class="line">(hello,<span class="number">2</span>)</span><br><span class="line">(world,<span class="number">1</span>)</span><br><span class="line">(spark,<span class="number">1</span>)</span><br><span class="line">(<span class="number">2</span>,<span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at <span class="type">RDDpersistence</span>.scala:<span class="number">22</span>)</span><br><span class="line">*********************</span><br><span class="line">(<span class="number">16</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at <span class="type">RDDpersistence</span>.scala:<span class="number">22</span> [<span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line"> |        <span class="type">CachedPartitions</span>: <span class="number">16</span>; <span class="type">MemorySize</span>: <span class="number">624.0</span> <span class="type">B</span>; <span class="type">ExternalBlockStoreSize</span>: <span class="number">0.0</span> <span class="type">B</span>; <span class="type">DiskSize</span>: <span class="number">0.0</span> <span class="type">B</span></span><br><span class="line"> |   <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at flatMap at <span class="type">RDDpersistence</span>.scala:<span class="number">20</span> [<span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line"> |   <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at <span class="type">RDDpersistence</span>.scala:<span class="number">18</span> [<span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line">(hello,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">(world,<span class="type">CompactBuffer</span>(<span class="number">1</span>))</span><br><span class="line">(spark,<span class="type">CompactBuffer</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>



<p>​        在当前案例中，变量value1是mapRDD算子之后的结果，oneResult和twoResult均使用该变量进行进一步处理。同时在value1后增加缓存处理。</p>
<p>​        分析打印结果，在缓存前和缓存后打印的debugger信息可以看到，执行后加上了Memory Deserialized 1x Replicated的信息，包括map中的<code>@@</code>的输出，然后使用collect()收集处理，之后打印使用持久化的RDD。</p>
<p>​        收集后再打印debugger，发现出现了CachedPartitions，可以看到使用了缓存，同时不再在<code>**</code>后打印<code>@@</code>，而是直接出结果。</p>
<p>​        如果将oneResult的结果不进行收集action处理</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">16</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at <span class="type">RDDpersistence</span>.scala:<span class="number">20</span> []</span><br><span class="line"> |   <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at flatMap at <span class="type">RDDpersistence</span>.scala:<span class="number">19</span> []</span><br><span class="line"> |   <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at <span class="type">RDDpersistence</span>.scala:<span class="number">18</span> []</span><br><span class="line">(<span class="number">16</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at <span class="type">RDDpersistence</span>.scala:<span class="number">20</span> [<span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line"> |   <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at flatMap at <span class="type">RDDpersistence</span>.scala:<span class="number">19</span> [<span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line"> |   <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at <span class="type">RDDpersistence</span>.scala:<span class="number">18</span> [<span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line">(<span class="number">2</span>,<span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at <span class="type">RDDpersistence</span>.scala:<span class="number">20</span>)</span><br><span class="line">*********************</span><br><span class="line">(<span class="number">16</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at <span class="type">RDDpersistence</span>.scala:<span class="number">20</span> [<span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line"> |   <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at flatMap at <span class="type">RDDpersistence</span>.scala:<span class="number">19</span> [<span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line"> |   <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at <span class="type">RDDpersistence</span>.scala:<span class="number">18</span> [<span class="type">Memory</span> <span class="type">Deserialized</span> <span class="number">1</span>x <span class="type">Replicated</span>]</span><br><span class="line">@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@</span><br><span class="line">(hello,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">(world,<span class="type">CompactBuffer</span>(<span class="number">1</span>))</span><br><span class="line">(spark,<span class="type">CompactBuffer</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>​        通过结果中可以看到，在血缘关系打印中，并没有出现CachedPartitions。并且map中的<code>@@</code>打印在<code>**</code>之后。说明是在<code>**</code>后重新计算了一遍。也就是说这个缓存的处理必须执行了action处理才会持久化的。</p>
<h4 id="存储级别"><a href="#存储级别" class="headerlink" title="存储级别"></a>存储级别</h4><p>默认cache方法是存储在内存中，在打印中也可以看到。其内部使用的是persist()方法，值是<code>persist(StorageLevel.MEMORY_ONLY)</code>。</p>
<p>可以使用persist函数直接设置</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myRDD.persist(<span class="type">StorageLevel</span>.<span class="type">DISK_ONLY</span>)</span><br></pre></td></tr></table></figure>

<p>在StorageLevel类中包含多种级别</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>级别</th>
<th>使用的空间</th>
<th>CPU时间</th>
<th>是否在内存中</th>
<th>是否在磁盘上</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>MEMORY_ONLY</td>
<td>高</td>
<td>第</td>
<td>是</td>
<td>否</td>
<td></td>
</tr>
<tr>
<td>MEMORY_ONLY_SER</td>
<td>低</td>
<td>高</td>
<td>是</td>
<td>否</td>
<td></td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>高</td>
<td>中等</td>
<td>部分</td>
<td>部分</td>
<td>如果数据在内存中存不下，则溢写到磁盘</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER</td>
<td>低</td>
<td>高</td>
<td>部分</td>
<td>部分</td>
<td>如果数据在内存中存不下，则溢写到磁盘。在内存中存放序列后的数据</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>低</td>
<td>高</td>
<td>否</td>
<td>是</td>
<td></td>
</tr>
</tbody></table>
<p>​        缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD 的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于 RDD 的一系列转换，丢失的数据会被重算，由于 RDD 的各个 Partition 是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部 Partition</p>
<p>​        Spark 会自动对一些 Shuffle 操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点 Shuffle 失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用 persist 或 cache。</p>
<h4 id="RDD-CheckPoint检查点"><a href="#RDD-CheckPoint检查点" class="headerlink" title="RDD CheckPoint检查点"></a>RDD CheckPoint检查点</h4><p>​        所谓的检查点其实就是<strong>通过将 RDD 中间结果写入磁盘，由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错</strong>，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发。</p>
<p>必需要设置检查点目录位置，一般存储在HDFS上。</p>
<p>做一个CheckPoint是需要重新重头跑一个job做。因此通常需要在之前添加cache操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sc.setCheckpointDir(<span class="string">&quot;cp&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello spark&quot;</span>),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value = rdd.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="keyword">val</span> value1 = value.map(s =&gt; &#123;</span><br><span class="line">    println(<span class="string">&quot;@@@@@@@@@@@&quot;</span>)</span><br><span class="line">    (s, <span class="number">1</span>)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">//value1.cache()</span></span><br><span class="line">println(value1.toDebugString)</span><br><span class="line">value1.checkpoint()</span><br><span class="line">println(value1.toDebugString)</span><br><span class="line"><span class="keyword">val</span> oneResult = value1.reduceByKey(_ + _)</span><br><span class="line">oneResult.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>此时不加入cache时。可以看到打印的<code>@@</code>是两遍的，加上cache之后，就只执行了一遍。</p>
<h4 id="缓存和检查点区别"><a href="#缓存和检查点区别" class="headerlink" title="缓存和检查点区别"></a>缓存和检查点区别</h4><ol>
<li><p>Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖。</p>
</li>
<li><p>Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高。</p>
</li>
<li><p>建议对 checkpoint()的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次 RDD。</p>
</li>
</ol>
<h2 id="分区器"><a href="#分区器" class="headerlink" title="分区器"></a>分区器</h2><p>Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。Hash 分区为当前的默认分区。分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区，进而决定了 Reduce 的个数。</p>
<ul>
<li>只有 Key-Value 类型的 RDD 才有分区器，非 Key-Value 类型的 RDD 分区的值是 None</li>
<li>每个 RDD 的分区 ID 范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的</li>
</ul>
<h4 id="Hash分区"><a href="#Hash分区" class="headerlink" title="Hash分区"></a>Hash分区</h4><p>对于给定的 key，计算其 hashCode,并除以分区个数取余</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>(<span class="params">partitions: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  require(partitions &gt;= <span class="number">0</span>, <span class="string">s&quot;Number of partitions (<span class="subst">$partitions</span>) cannot be negative.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = partitions</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = key <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="literal">null</span> =&gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="type">Utils</span>.nonNegativeMod(key.hashCode, numPartitions)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(other: <span class="type">Any</span>): <span class="type">Boolean</span> = other <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> h: <span class="type">HashPartitioner</span> =&gt;</span><br><span class="line">      h.numPartitions == numPartitions</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hashCode</span></span>: <span class="type">Int</span> = numPartitions</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Range分区"><a href="#Range分区" class="headerlink" title="Range分区"></a>Range分区</h4><p>将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</p>
<h4 id="自定义分区器"><a href="#自定义分区器" class="headerlink" title="自定义分区器"></a>自定义分区器</h4><p>继承<code>org.apache.spark.Partitioner</code>类并实现方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;篮球&quot;</span>, <span class="string">&quot;张三&quot;</span>), (<span class="string">&quot;足球&quot;</span>, <span class="string">&quot;李四&quot;</span>), (<span class="string">&quot;排球&quot;</span>, <span class="string">&quot;小红&quot;</span>), (<span class="string">&quot;篮球&quot;</span>, <span class="string">&quot;小刚&quot;</span>)))</span><br><span class="line"><span class="keyword">val</span> value = rdd.partitionBy(<span class="keyword">new</span> <span class="type">BallPartitioner</span>)</span><br><span class="line">value.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BallPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    key <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;篮球&quot;</span> =&gt; <span class="number">0</span></span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;足球&quot;</span> =&gt; <span class="number">1</span></span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用partitionBy重新分区，将分区数据输出到文件中。可以看到不同种类的球分到指定的分区号中。</p>
<h2 id="文件读写保存"><a href="#文件读写保存" class="headerlink" title="文件读写保存"></a>文件读写保存</h2><p>Spark 的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。文件格式分为：text 文件、csv 文件、sequence 文件以及 Object 文件；文件系统分为：本地文件系统、HDFS、HBASE 以及数据库。</p>
<h4 id="text文件"><a href="#text文件" class="headerlink" title="text文件"></a>text文件</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;篮球&quot;</span>, <span class="string">&quot;张三&quot;</span>), (<span class="string">&quot;足球&quot;</span>, <span class="string">&quot;李四&quot;</span>), (<span class="string">&quot;排球&quot;</span>, <span class="string">&quot;小红&quot;</span>), (<span class="string">&quot;篮球&quot;</span>, <span class="string">&quot;小刚&quot;</span>)),<span class="number">1</span>)</span><br><span class="line">rdd.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line">sc.textFile(<span class="string">&quot;output&quot;</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<h4 id="sequence文件"><a href="#sequence文件" class="headerlink" title="sequence文件"></a>sequence文件</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;篮球&quot;</span>, <span class="string">&quot;张三&quot;</span>), (<span class="string">&quot;足球&quot;</span>, <span class="string">&quot;李四&quot;</span>), (<span class="string">&quot;排球&quot;</span>, <span class="string">&quot;小红&quot;</span>), (<span class="string">&quot;篮球&quot;</span>, <span class="string">&quot;小刚&quot;</span>)),<span class="number">1</span>)</span><br><span class="line">rdd.saveAsSequenceFile(<span class="string">&quot;output1&quot;</span>)</span><br><span class="line">sc.sequenceFile[<span class="type">String</span>,<span class="type">String</span>](<span class="string">&quot;output1&quot;</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<h4 id="object对象文件"><a href="#object对象文件" class="headerlink" title="object对象文件"></a>object对象文件</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;篮球&quot;</span>, <span class="string">&quot;张三&quot;</span>), (<span class="string">&quot;足球&quot;</span>, <span class="string">&quot;李四&quot;</span>), (<span class="string">&quot;排球&quot;</span>, <span class="string">&quot;小红&quot;</span>), (<span class="string">&quot;篮球&quot;</span>, <span class="string">&quot;小刚&quot;</span>)),<span class="number">1</span>)</span><br><span class="line">rdd.saveAsObjectFile(<span class="string">&quot;output2&quot;</span>)</span><br><span class="line">sc.objectFile(<span class="string">&quot;output2&quot;</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p><strong>累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge</strong>。</p>
<p><strong>累加器的使用也是需要再行动算子中进行添加，否则不起效果。</strong></p>
<p>通常我们在操作简单的数据处理时，会直接通过Driver端的数据与makeRDD中的数据进行交互处理。但此时因为是分布式且多分区的。因此会将driver端的变量数据传输到executor中，而且每一个executor节点都会有自己的一份。那么在每个executor都结束之后，是没办法将这些副本变量的修改，转发给driver端的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="keyword">var</span> acc = <span class="number">0</span>;</span><br><span class="line">value.foreach(i =&gt; acc += i)</span><br><span class="line">println(acc) <span class="comment">// 0</span></span><br></pre></td></tr></table></figure>

<p>以上代码展示了对数据的求和问题。但最后打印的就是0。因为计算是在每个Executor节点的task中。没办法最终的数据回传给driver端的这个变量。</p>
<h4 id="系统累加器"><a href="#系统累加器" class="headerlink" title="系统累加器"></a>系统累加器</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="keyword">val</span> acc = sc.longAccumulator(<span class="string">&quot;acc&quot;</span>)</span><br><span class="line">value.foreach(i =&gt; acc.add(i))</span><br><span class="line">println(acc.value) <span class="comment">// 15</span></span><br></pre></td></tr></table></figure>

<p>以上代码中使用了Long数据类型的累加器，最终的结果是15。</p>
<p>系统的累加器包括了LongAccumulator、DoubleAccumulator、CollectionAccumulator。</p>
<p>还可以自定义自己的累加器</p>
<h4 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h4><p>自定义累加器需要继承AccumulatorV2类并实现其方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AccumulatorV2</span>[<span class="type">IN</span>, <span class="type">OUT</span>] <span class="keyword">extends</span> <span class="title">Serializable</span></span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = ???</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">Nothing</span>, <span class="type">Nothing</span>] = ???</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = ???</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">Nothing</span>): <span class="type">Unit</span> = ???</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">Nothing</span>, <span class="type">Nothing</span>]): <span class="type">Unit</span> = ???</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: <span class="type">Nothing</span> = ???</span><br></pre></td></tr></table></figure>

<p><strong>wordCount计算累加器</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 实现WordCount自定义累加器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCountAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]] </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 定义map类型值存储</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">var</span> map: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = mutable.<span class="type">Map</span>()</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 返回该累加器是否为零值。例如，对于计数器累加器，0就是零值;对于一个列表累加器，Nil是零值</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = map.isEmpty</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 创建此累加器的新副本</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]] = <span class="keyword">new</span> <span class="type">WordCountAccumulator</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 重置这个累加器，它是零值。即调用&#x27; isZero &#x27;必须返回true</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = map.clear()</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 接受输入并进行累积</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param v</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    map.put(v, map.getOrElse(v, <span class="number">0</span>L) + <span class="number">1</span>L)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 多个累加器合并操作</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param other</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    other.value.foreach(item =&gt; &#123;</span><br><span class="line">      map.put(item._1, map.getOrElse(item._1, <span class="number">0</span>L) + item._2)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 返回当前累加器的值</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = map</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="string">&quot;hello spark&quot;</span>, <span class="string">&quot;hello hadoop&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> accumulator = <span class="keyword">new</span> <span class="type">WordCountAccumulator</span></span><br><span class="line"><span class="comment">// 注册累加器</span></span><br><span class="line">sc.register(accumulator, <span class="string">&quot;WordCountAccumulator&quot;</span>)</span><br><span class="line">value.flatMap(i =&gt; i.split(<span class="string">&quot; &quot;</span>)).foreach(s =&gt; accumulator.add(s))</span><br><span class="line">println(accumulator.value)</span><br></pre></td></tr></table></figure>

<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p><strong>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用</strong>。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">4</span>)), <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">7</span>))</span><br><span class="line"><span class="comment">// 声明广播变量</span></span><br><span class="line"><span class="keyword">val</span> broadcast: <span class="type">Broadcast</span>[<span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)]] = sc.broadcast(list)</span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = rdd1.map &#123;</span><br><span class="line">    <span class="keyword">case</span> (key, num) =&gt; &#123;</span><br><span class="line">        <span class="keyword">var</span> num2 = <span class="number">0</span></span><br><span class="line">        <span class="comment">// 使用广播变量</span></span><br><span class="line">        <span class="keyword">for</span> ((k, v) &lt;- broadcast.value) &#123;</span><br><span class="line">            <span class="keyword">if</span> (k == key) &#123;</span><br><span class="line">                num2 = v</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        (key, (num, num2))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">resultRDD.collect().foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">(a,(<span class="number">1</span>,<span class="number">4</span>))</span><br><span class="line">(b,(<span class="number">2</span>,<span class="number">5</span>))</span><br><span class="line">(c,(<span class="number">3</span>,<span class="number">6</span>))</span><br><span class="line">(d,(<span class="number">4</span>,<span class="number">7</span>))</span><br></pre></td></tr></table></figure>

<p>以上代码，通过Broadcast模拟了join算子，将同key的value数据组成元组。</p>
<p><img src="https://fsats-blog.oss-cn-beijing.aliyuncs.com/blog/image-20220504192053432.png" alt="image-20220504192053432"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Linux/Linux系统Openvpn客户端连接配置"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/04/29/Linux/Linux%E7%B3%BB%E7%BB%9FOpenvpn%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9E%E6%8E%A5%E9%85%8D%E7%BD%AE/"
    >Linux系统OpenVPN客户端连接配置</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/04/29/Linux/Linux%E7%B3%BB%E7%BB%9FOpenvpn%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9E%E6%8E%A5%E9%85%8D%E7%BD%AE/" class="article-date">
  <time datetime="2022-04-29T04:00:00.000Z" itemprop="datePublished">2022-04-29</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Linux/">Linux</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>Linux系统使用的是Debain的发行衍生版。Centos等安装的包都一样</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install openvpn easy-rsa</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://juejin.cn/post/6844903647004852237">参考配置</a>，<a target="_blank" rel="noopener" href="https://developer.toradex.com/knowledge-base/openvpn">OpenVPN官网</a></p>
<p>客户端配置时，此时已经有了相关证书，包含后缀为ovpn的配置文件。</p>
<p>安装完openvpn后，配置位置在/etc/openvpn目录下。将所有的证书都放到这个目录下即可。</p>
<p>文件包含 ca.crt、ta.key、client.crt、client.key、xxha.ovpn这五个文件，ovpn就是conf配置文件，只不过后缀为ovpn，在windows上的openvpn客户端可以看到指定的后缀。</p>
<p>使用以下命令开启</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo openvpn --daemon --cd /etc/openvpn --askpass passwd --config client.ovpn --log-append /var/log/openvpn.log</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">--daemon           # 后台运行</span><br><span class="line">--askpass		   # 需要回答的连接密码，是个文件,只有一行密码</span><br><span class="line">--cd               # 配置文件目录路径</span><br><span class="line">--config           # 配置文件名称</span><br><span class="line">--log-append       # 日志文件</span><br></pre></td></tr></table></figure>

<p>指定了–cd之后，所有指定的文件都设置为当前目录的相对路径</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Spark/计算函数算子"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/04/23/Spark/%E8%AE%A1%E7%AE%97%E5%87%BD%E6%95%B0%E7%AE%97%E5%AD%90/"
    >计算函数算子</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/04/23/Spark/%E8%AE%A1%E7%AE%97%E5%87%BD%E6%95%B0%E7%AE%97%E5%AD%90/" class="article-date">
  <time datetime="2022-04-22T16:00:00.000Z" itemprop="datePublished">2022-04-23</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>分区分配算法，分区减小，增大算法</p>
<h1 id="转换算子"><a href="#转换算子" class="headerlink" title="转换算子"></a>转换算子</h1><h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><h5 id="函数签名"><a href="#函数签名" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明"><a href="#函数说明" class="headerlink" title="函数说明"></a>函数说明</h5><p>将处理的数据逐条进行映射转换。对集合的<strong>每一个项</strong>进行单独处理，可以返回任意格式。与Java中的Stream的map一样。</p>
<h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 组装成list形式,每一项类型为List</span></span><br><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> value1 = value.map(<span class="type">List</span>(_))</span><br><span class="line">value1.collect().foreach(println)</span><br><span class="line"><span class="comment">// 格式为原格式，仍为Integer类型</span></span><br><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> value1 = value.map(_*<span class="number">2</span>)</span><br><span class="line">value1.collect().foreach(println)</span><br><span class="line"><span class="comment">// 从文件中构建RDD，对每一行数据(每一条数据)进行分割取索引为6的数据</span></span><br><span class="line"><span class="keyword">val</span> value = sc.textFile(<span class="string">&quot;datas/apache.log&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.map(i =&gt; i.split(<span class="string">&quot; &quot;</span>)(<span class="number">6</span>))</span><br><span class="line">value1.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<h4 id="并行计算说明"><a href="#并行计算说明" class="headerlink" title="并行计算说明"></a>并行计算说明</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>), <span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.map(i =&gt; &#123;</span><br><span class="line">    println(<span class="string">&quot;&gt;&gt;&gt;&gt;&gt;&gt;&quot;</span> + i)</span><br><span class="line">    i</span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">val</span> value2 = value1.map(i =&gt; &#123;</span><br><span class="line">    println(<span class="string">&quot;#####&quot;</span> + i)</span><br><span class="line">    i</span><br><span class="line">&#125;)</span><br><span class="line">value2.collect()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;<span class="number">1</span></span><br><span class="line">#####<span class="number">1</span></span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;<span class="number">2</span></span><br><span class="line">#####<span class="number">2</span></span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;<span class="number">3</span></span><br><span class="line">#####<span class="number">3</span></span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;<span class="number">4</span></span><br><span class="line">#####<span class="number">4</span></span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;<span class="number">5</span></span><br><span class="line">#####<span class="number">5</span></span><br></pre></td></tr></table></figure>

<p>在rdd的计算中，一个分区内，只有前面的该项数据计算逻辑执行完成后才能执行执行后面的逻辑，但不需要一个分区内所有数据全部执行完才执行后面的逻辑。并且分区之间是独立的。</p>
<p>而mapPartitions函数是以分区为单位而不是以某个分区内元素为单位。是类似分区一批次一批次的处理。</p>
<h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><h5 id="函数签名-1"><a href="#函数签名-1" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">      preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line">): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-1"><a href="#函数说明-1" class="headerlink" title="函数说明"></a>函数说明</h5><p>将待处理的数据<strong>以分区为单位</strong>发送到计算节点进行处理。要处理的数据是一个分区的数据，也就是这个迭代器数据，可以对其做任何操作，只需要返回的数据仍然是迭代器类型。</p>
<p>它将一次性获取分区的全部数据。因此处理数据较快，但是占用内存会更大且时间会更久。</p>
<h5 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定分区数量2，则该函数只会处理两次</span></span><br><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.mapPartitions(i =&gt; &#123;</span><br><span class="line">    println(<span class="string">&quot;#######&quot;</span>)</span><br><span class="line">    i</span><br><span class="line">&#125;)</span><br><span class="line">value1.collect()</span><br><span class="line"><span class="comment">// 对每个分区的数据进行过滤除偶数数据</span></span><br><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.mapPartitions(i =&gt; &#123;</span><br><span class="line">    i.filter(_%<span class="number">2</span>==<span class="number">0</span>)</span><br><span class="line">&#125;)</span><br><span class="line">value1.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>以分区进行处理的，但最后收集的结果foreach之后依然是每一项元素，而不是每一个分区数据</p>
<h4 id="疑问？"><a href="#疑问？" class="headerlink" title="疑问？"></a>疑问？</h4><p>在mapPartitions函数中操作时，只要对操作数据进行了处理，最终返回的结果就是第一次处理迭代数据之后的结果，最终的返回无效。例如在println中打印每个分区的最大值，然后返回原来的分区数据，但最终收集的只有两个分区的最大值。</p>
<p>感觉数据已经被处理后就不会被下一步逻辑处理了。</p>
<h4 id="map和mapPartitions的区别"><a href="#map和mapPartitions的区别" class="headerlink" title="map和mapPartitions的区别"></a>map和mapPartitions的区别</h4><table>
<thead>
<tr>
<th></th>
<th>map</th>
<th>mapPartitions</th>
</tr>
</thead>
<tbody><tr>
<td>数据处理角度</td>
<td>分区内一个数据一个数据的执行</td>
<td>以分区为单位进行执行</td>
</tr>
<tr>
<td>功能角度</td>
<td>将数据源中的数据进行转换和改变。但是不会减少或增多数据</td>
<td>要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据</td>
</tr>
<tr>
<td>性能角度</td>
<td>为类似于串行操作，所以性能比较低</td>
<td>类似于批处理，所以性能较高。但是 mapPartitions 算子会长时间占用内存</td>
</tr>
</tbody></table>
<h3 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h3><h5 id="函数签名-2"><a href="#函数签名-2" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">      preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line">): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-2"><a href="#函数说明-2" class="headerlink" title="函数说明"></a>函数说明</h5><p>将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。</p>
<h5 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 对每一个分区进行处理，对此分区的集合执行map操作，改变其类型</span></span><br><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.mapPartitionsWithIndex((index, i) =&gt; &#123;</span><br><span class="line">    i.map(m =&gt; (index, m))</span><br><span class="line">&#125;)</span><br><span class="line">value1.collect().foreach(println)</span><br><span class="line"><span class="comment">//out</span></span><br><span class="line">(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="number">0</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">(<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">(<span class="number">1</span>,<span class="number">6</span>)</span><br></pre></td></tr></table></figure>

<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><h5 id="函数签名-3"><a href="#函数签名-3" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-3"><a href="#函数说明-3" class="headerlink" title="函数说明"></a>函数说明</h5><p>将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射。其内部使用的是Iterator的flatMap函数。函数中参数t表示的是每一个元素。函数最终应该返回一个集合类型或者是一个可以迭代的类型。</p>
<h5 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将List[Int] 类型的RDD进行分解</span></span><br><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>),<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>)), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.flatMap(i =&gt; i)</span><br><span class="line">value1.collect().foreach(println)</span><br><span class="line"><span class="comment">// 将每个元素以空格分开</span></span><br><span class="line"><span class="keyword">val</span> value2 = sc.makeRDD(<span class="type">List</span>(<span class="string">&quot;Hello Scala&quot;</span>, <span class="string">&quot;Hello Spark&quot;</span>),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> value3 = value2.flatMap(i =&gt; &#123;</span><br><span class="line">    i.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">&#125;)</span><br><span class="line">value3.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<h3 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h3><h5 id="函数签名-4"><a href="#函数签名-4" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glom</span></span>(): <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-4"><a href="#函数说明-4" class="headerlink" title="函数说明"></a>函数说明</h5><p>将同一个分区的数据直接转换为相同类型的的数组进行处理，分区不变。</p>
<h5 id="示例-4"><a href="#示例-4" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> glom = value.glom()</span><br><span class="line">glom.collect().foreach(d =&gt; println(d.mkString(<span class="string">&quot;,&quot;</span>)))</span><br><span class="line"><span class="comment">//out</span></span><br><span class="line"><span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span></span><br><span class="line"><span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//计算所有分区最大值求和（分区内取最大值，分区间最大值求和）</span></span><br><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">2</span>)</span><br><span class="line"><span class="comment">// 使用glom函数处理，将每个分区的数据形成Array[Int]类型的数据</span></span><br><span class="line"><span class="keyword">val</span> glom: <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = value.glom()</span><br><span class="line"><span class="comment">// 通过map的方式重新组装成每个Array的最大值的数据</span></span><br><span class="line"><span class="keyword">val</span> value1 = glom.map(a =&gt; a.max)</span><br><span class="line"><span class="comment">// 收集后使用sum函数求和即可</span></span><br><span class="line">println(value1.collect().sum)</span><br></pre></td></tr></table></figure>

<h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><h5 id="函数签名-5"><a href="#函数签名-5" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupBy[<span class="type">K</span>](f: <span class="type">T</span> =&gt; <span class="type">K</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-5"><a href="#函数说明-5" class="headerlink" title="函数说明"></a>函数说明</h5><p>根据数据指定的规则进行分组，分区默认不变，但是数据会被<strong>打乱重组</strong>，我们将这样的操作称之为<strong>shuffle</strong>。极限情况下，数据可能被分在同一个分区中</p>
<p><strong>一个组中的数据在一个分区中，但并不是说一个分区中只有一个组</strong></p>
<p>分组的使用方式与Java Stream类似</p>
<h5 id="示例-5"><a href="#示例-5" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.groupBy(i =&gt; i % <span class="number">2</span>==<span class="number">0</span>)</span><br><span class="line">value1.collect().foreach(println(_))</span><br><span class="line"><span class="comment">//out</span></span><br><span class="line">(<span class="literal">false</span>,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line">(<span class="literal">true</span>,<span class="type">CompactBuffer</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 按照首字母进行分组</span></span><br><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="string">&quot;Hello&quot;</span>,<span class="string">&quot;hive&quot;</span>,<span class="string">&quot;hbash&quot;</span>,<span class="string">&quot;Hadoop&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> value1 = value.groupBy(a =&gt; a.charAt(<span class="number">0</span>))</span><br><span class="line">value1.collect().foreach(println(_))</span><br><span class="line"><span class="comment">//out</span></span><br><span class="line">(h,<span class="type">CompactBuffer</span>(hive, hbash))</span><br><span class="line">(<span class="type">H</span>,<span class="type">CompactBuffer</span>(<span class="type">Hello</span>, <span class="type">Hadoop</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从服务器日志数据 apache.log 中获取每个时间段访问量</span></span><br><span class="line"><span class="keyword">val</span> value = sc.textFile(<span class="string">&quot;datas/apache.log&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.map(d =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> time = d.split(<span class="string">&quot; &quot;</span>)(<span class="number">3</span>).split(<span class="string">&quot;:&quot;</span>)(<span class="number">1</span>)</span><br><span class="line">    (time, <span class="number">1</span>)</span><br><span class="line">&#125;).groupBy(d =&gt; d._1).map(s=&gt;(s._1,s._2.size)).sortByKey()</span><br><span class="line">value1.collect().foreach(println(_))</span><br><span class="line"><span class="comment">// map方法同时可以使用map case的方式</span></span><br><span class="line"><span class="keyword">val</span> value = sc.textFile(<span class="string">&quot;datas/apache.log&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.map(d =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> time = d.split(<span class="string">&quot; &quot;</span>)(<span class="number">3</span>).split(<span class="string">&quot;:&quot;</span>)(<span class="number">1</span>)</span><br><span class="line">    (time, <span class="number">1</span>)</span><br><span class="line">&#125;).groupBy(d =&gt; d._1).map &#123;</span><br><span class="line">    <span class="keyword">case</span> (h,i)=&gt;(h,i.size)</span><br><span class="line">&#125;.sortByKey()</span><br><span class="line">value1.collect().foreach(println(_))</span><br></pre></td></tr></table></figure>

<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><h5 id="函数签名-6"><a href="#函数签名-6" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-6"><a href="#函数说明-6" class="headerlink" title="函数说明"></a>函数说明</h5><p>将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。</p>
<p>当数据进行筛选过滤后分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现<strong>数据倾斜</strong></p>
<h5 id="示例-6"><a href="#示例-6" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">2</span>)</span><br><span class="line">value.filter(i =&gt; i % <span class="number">2</span> == <span class="number">0</span>).collect().foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//从服务器日志数据 apache.log 中获取 2015 年 5 月 17 日的请求路径</span></span><br><span class="line"><span class="keyword">val</span> value = sc.textFile(<span class="string">&quot;datas/apache.log&quot;</span>)</span><br><span class="line">value.filter(i =&gt; i.split(<span class="string">&quot; &quot;</span>)(<span class="number">3</span>).startsWith(<span class="string">&quot;17/05/2015&quot;</span>)).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<h3 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h3><h5 id="函数签名-7"><a href="#函数签名-7" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sample(</span><br><span class="line">      withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">      fraction: <span class="type">Double</span>,</span><br><span class="line">      seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-7"><a href="#函数说明-7" class="headerlink" title="函数说明"></a>函数说明</h5><p>根据指定的规则从数据集中抽取数据.</p>
<p>withReplacement：元素是否可多次采样。true表示可多次采样，数据放回，false表示数据不放回</p>
<ol>
<li><p>当抽取数据不放回（伯努利算法） false</p>
<p>伯努利算法：又叫 0、1 分布。例如扔硬币，要么正面，要么反面</p>
<p>具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不</p>
<p>要</p>
<p>第二个参数：<strong>抽取的几率</strong>，范围在[0, 1]之间。0全不取，1全取</p>
<p>第三个参数：随机数种子</p>
</li>
<li><p>当抽取数据放回（泊松算法） true</p>
<p>第二个参数：<strong>重复数据的几率</strong>，范围大于等于0，表示每个元素期望抽取到的次数</p>
<p>第三个参数：随机数种子</p>
</li>
</ol>
<h5 id="示例-7"><a href="#示例-7" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>), <span class="number">2</span>)</span><br><span class="line"><span class="comment">//当数据不放回时，抽取几率为1</span></span><br><span class="line"><span class="keyword">val</span> value1 = value.sample(<span class="literal">false</span>, <span class="number">1</span>) <span class="comment">// 51678234</span></span><br><span class="line"><span class="comment">//当数据不放回时，抽取几率为0</span></span><br><span class="line"><span class="keyword">val</span> value1 = value.sample(<span class="literal">false</span>, <span class="number">0</span>) <span class="comment">// 没有数据</span></span><br><span class="line"><span class="comment">// 当数据不放回时，抽取几率为大于零小于1</span></span><br><span class="line"><span class="keyword">val</span> value1 = value.sample(<span class="literal">false</span>, <span class="number">0.4</span>) <span class="comment">// 任意数量的不重复数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 当数据放回时，抽取几率为1</span></span><br><span class="line"><span class="keyword">val</span> value1 = value.sample(<span class="literal">true</span>, <span class="number">1</span>) <span class="comment">// 任意数量可重复的数据</span></span><br><span class="line"><span class="comment">// 当数据放回时，抽取几率为0</span></span><br><span class="line"><span class="keyword">val</span> value1 = value.sample(<span class="literal">true</span>, <span class="number">1</span>) <span class="comment">// 没有数据</span></span><br><span class="line"><span class="comment">// 当数据放回时，抽取几率为大于零小于1</span></span><br><span class="line"><span class="keyword">val</span> value1 = value.sample(<span class="literal">true</span>, <span class="number">0.4</span>) <span class="comment">// 任意数量的重复数</span></span><br></pre></td></tr></table></figure>

<h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><h5 id="函数签名-8"><a href="#函数签名-8" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">distinct(): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">distinct(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-8"><a href="#函数说明-8" class="headerlink" title="函数说明"></a>函数说明</h5><p>将数据集中重复的数据去重</p>
<h5 id="示例-8"><a href="#示例-8" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">8</span>), <span class="number">2</span>)</span><br><span class="line">value.distinct().foreach(print)</span><br></pre></td></tr></table></figure>

<h3 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h3><h5 id="函数签名-9"><a href="#函数签名-9" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">coalesce(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">               partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">              (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">      : <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-9"><a href="#函数说明-9" class="headerlink" title="函数说明"></a>函数说明</h5><p>根据数据量<strong>缩减分区</strong>，用于大数据集过滤后，提高小数据集的执行效率</p>
<p>当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少分区的个数，减小任务调度成本。</p>
<p>在没有设置shuffle，缩减分区时。例如，如果你从1000个分区到100个分区，不会有一个转移，而是每100个新分区将占用10个当前分区。（此时会出现示例2的情况）</p>
<p>该函数在不设置shuffle为true的情况下，增大分区不会改变结果。因为数据不会shuffle处理，不会重新对数据进行分区。扩大分区需要设置shuffle为true，使用repartition函数</p>
<h5 id="示例-9"><a href="#示例-9" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 此时将会缩减为2个分区</span></span><br><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>,<span class="number">24</span>,<span class="number">5</span>), <span class="number">7</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.coalesce(<span class="number">2</span>)</span><br><span class="line">value1.saveAsTextFile(<span class="string">&quot;output&quot;</span>) </span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>), <span class="number">3</span>)</span><br><span class="line">value.saveAsTextFile(<span class="string">&quot;output&quot;</span>) <span class="comment">// 123,456,78910</span></span><br><span class="line"><span class="keyword">val</span> value1 = value.coalesce(<span class="number">2</span>)</span><br><span class="line">value1.saveAsTextFile(<span class="string">&quot;output1&quot;</span>) <span class="comment">// 123,45678910</span></span><br><span class="line"><span class="comment">//该情况就是说明中的每100个新分区将占用10个当前分区为例。</span></span><br><span class="line"><span class="comment">// 数据不会shuffle，便不会被打乱重组，因此以类似就近原则，多个分区就近占用当前的少量分区。</span></span><br><span class="line"><span class="comment">// 例如，原来6个，现在2个，则前三个占用第一个分区，后三个占用第二个分区</span></span><br><span class="line"><span class="comment">// 例如，原来7个，现在3个，则每两个一个分区，最后三个占用最后一个分区</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h3><h5 id="函数签名-10"><a href="#函数签名-10" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">repartition(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-10"><a href="#函数说明-10" class="headerlink" title="函数说明"></a>函数说明</h5><p>该操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true。变大或缩小分区数都会经过shuffle操作。</p>
<h5 id="示例-10"><a href="#示例-10" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.repartition(<span class="number">6</span>)</span><br></pre></td></tr></table></figure>

<h3 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h3><h5 id="函数签名-11"><a href="#函数签名-11" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sortBy[<span class="type">K</span>](</span><br><span class="line">      f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">      ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">      numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br><span class="line">      (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">K</span>], ctag: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-11"><a href="#函数说明-11" class="headerlink" title="函数说明"></a>函数说明</h5><p>第二个参数为asc升序</p>
<p>第三个参数为分区数</p>
<p>该操作用于排序数据。在排序之前，可以将数据通过 f 函数进行处理，之后按照 f 函数处理的结果进行排序，默认为升序排列。默认情况下，排序后新产生的 RDD 的分区数与原 RDD 的分区数一致。中间存在 shuffle 的过程</p>
<h5 id="示例-11"><a href="#示例-11" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">10</span>, <span class="number">324</span>, <span class="number">4</span>, <span class="number">5462</span>, <span class="number">42</span>, <span class="number">74</span>, <span class="number">43</span>), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.sortBy(i =&gt; i)</span><br><span class="line">println(value1.collect().mkString(<span class="string">&quot;,&quot;</span>))</span><br></pre></td></tr></table></figure>

<h3 id="intersection、union、subtract"><a href="#intersection、union、subtract" class="headerlink" title="intersection、union、subtract"></a>intersection、union、subtract</h3><h5 id="函数签名-12"><a href="#函数签名-12" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">intersection(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">union(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">subtract(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-12"><a href="#函数说明-12" class="headerlink" title="函数说明"></a>函数说明</h5><p>对源RDD和参数RDD分别求交集、并集、差集后返回一个新的RDD。</p>
<p>两个RDD进行计算，需要保证两个RDD的数据类型是一样的。</p>
<h5 id="示例-12"><a href="#示例-12" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = sc.makeRDD(<span class="type">List</span>(<span class="number">7</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">11</span>, <span class="number">12</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> value2 = value.intersection(value1) <span class="comment">// 6,2</span></span><br><span class="line">println(value2.collect().mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> value2 = value.union(value1) <span class="comment">// 1,2,3,4,5,6,7,2,9,6,11,12</span></span><br><span class="line"><span class="keyword">val</span> value2 = value.subtract(value1) <span class="comment">// 4,1,3,5</span></span><br><span class="line"><span class="keyword">val</span> value2 = value1.subtract(value) <span class="comment">// 12,7,9,11</span></span><br></pre></td></tr></table></figure>

<h3 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h3><h5 id="函数签名-13"><a href="#函数签名-13" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zip[<span class="type">U</span>: <span class="type">ClassTag</span>](other: <span class="type">RDD</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-13"><a href="#函数说明-13" class="headerlink" title="函数说明"></a>函数说明</h5><p>将两个 RDD 中的元素，以键值对的形式进行合并。其中，键值对中的 Key 为第 1 个 RDD中的元素，Value 为第 2 个 RDD 中的相同位置的元素。</p>
<p>需要保证两个RDD的数据类型一致，分区数量一致，分区数据数据量一致。</p>
<h5 id="示例-13"><a href="#示例-13" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = sc.makeRDD(<span class="type">List</span>(<span class="number">7</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">11</span>, <span class="number">12</span>), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value2 = value.zip(value1)</span><br><span class="line">println(value2.collect().mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">(<span class="number">1</span>,<span class="number">7</span>),(<span class="number">2</span>,<span class="number">2</span>),(<span class="number">3</span>,<span class="number">9</span>),(<span class="number">4</span>,<span class="number">6</span>),(<span class="number">5</span>,<span class="number">11</span>),(<span class="number">6</span>,<span class="number">12</span>)</span><br></pre></td></tr></table></figure>

<h3 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h3><h5 id="函数签名-14"><a href="#函数签名-14" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">partitionBy(partitioner: Partitioner): RDD[(K, V)]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-14"><a href="#函数说明-14" class="headerlink" title="函数说明"></a>函数说明</h5><p>将数据按照指定 Partitioner 重新进行分区。Spark 默认的分区器是 HashPartitioner</p>
<p>需要满足数据类型是key-value类型的。</p>
<p>如果使用的分区器与默认分区器相同，且分区数量也相同则直接返回调用者，不进行处理。</p>
<h5 id="示例-14"><a href="#示例-14" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;aaa&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;bbb&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;ccc&quot;</span>)),<span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>)).saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h3><h5 id="函数签名-15"><a href="#函数签名-15" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reduceByKey(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line">reduceByKey(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>, numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line">reduceByKey(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-15"><a href="#函数说明-15" class="headerlink" title="函数说明"></a>函数说明</h5><p>可以将数据按照相同的 Key 对 Value 进行聚合。</p>
<p>类似MapReduce中的“combiner”</p>
<h5 id="示例-15"><a href="#示例-15" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">1</span>)), <span class="number">3</span>)</span><br><span class="line">value.reduceByKey((i, j) =&gt; i + j).collect().foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">(c,<span class="number">1</span>)</span><br><span class="line">(a,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><h5 id="函数签名-16"><a href="#函数签名-16" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">groupByKey(): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line">groupByKey(numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line">groupByKey(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-16"><a href="#函数说明-16" class="headerlink" title="函数说明"></a>函数说明</h5><p>将数据源的数据根据 key 对 value 进行分组。分组后v值是Iterable类型</p>
<h5 id="示例-16"><a href="#示例-16" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">1</span>)), <span class="number">3</span>)</span><br><span class="line">value.groupByKey().map(s=&gt;(s._1,s._2.size)).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<h4 id="groupByKey和reduceByKey的区别"><a href="#groupByKey和reduceByKey的区别" class="headerlink" title="groupByKey和reduceByKey的区别"></a>groupByKey和reduceByKey的区别</h4><h5 id="从shuffle的角度"><a href="#从shuffle的角度" class="headerlink" title="从shuffle的角度"></a>从shuffle的角度</h5><p>reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高</p>
<h5 id="从功能的角度"><a href="#从功能的角度" class="headerlink" title="从功能的角度"></a>从功能的角度</h5><p>reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey</p>
<table>
<thead>
<tr>
<th></th>
<th>groupByKey</th>
<th>reduceByKey</th>
</tr>
</thead>
<tbody><tr>
<td>shuffle角度</td>
<td>不会在shuffle前预聚合,只进行分组</td>
<td>会在shuffle前预聚合，减少落盘数据量</td>
</tr>
<tr>
<td>功能角度</td>
<td>只能分组不能聚合</td>
<td>包含分组和聚合功能</td>
</tr>
</tbody></table>
<h3 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h3><h5 id="函数签名-17"><a href="#函数签名-17" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">aggregateByKey[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>,</span><br><span class="line">      combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-17"><a href="#函数说明-17" class="headerlink" title="函数说明"></a>函数说明</h5><p>将数据根据不同的规则进行分区内计算和分区间计算。</p>
<p>这个函数可以返回不同的结果类型U，而不是RDD中值的类型V。我们需要一个操作来合并一个V到U，一个操作来合并两个U。<strong>前一种操作用于合并分区内的值，后一种操作用于合并分区之间的值</strong>。为了避免内存分配，这两个函数都允许修改并返回它们的第一个参数，而不是创建一个新的U。</p>
<p>该函数使用柯里化的方式。</p>
<ul>
<li>第一个参数列表中的参数表示初始值。该值用于在分区间比较时，第一个key值与该值计算。</li>
<li>第二个参数列表中含有两个参数。第一个参数表示分区内的计算规则，第二个参数标识分区间的计算规则</li>
</ul>
<p>可以看到函数签名中结果类型U。第一个参数列表需要该类型、第二个参数列表的第一个参数和第二个参数都有，包括返回值都是该结果类型。</p>
<h5 id="示例-17"><a href="#示例-17" class="headerlink" title="示例"></a>示例</h5><h6 id="计算每个key在分区间的最大值的和。"><a href="#计算每个key在分区间的最大值的和。" class="headerlink" title="计算每个key在分区间的最大值的和。"></a>计算每个key在分区间的最大值的和。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">6</span>)),<span class="number">2</span>)</span><br><span class="line">value.aggregateByKey(<span class="number">0</span>)((x, y) =&gt; <span class="type">Math</span>.max(x, y), (x, y) =&gt; x + y).collect().foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">(a,<span class="number">9</span>)</span><br><span class="line">(b,<span class="number">12</span>)</span><br></pre></td></tr></table></figure>

<p>该伪代码中，设置了结果类型U为Int类型，因此最终返回的格式为Int类型。<br>初始值为0，分区内的计算时求最大值，分区间的计算是相加。最终求得不同key的分区间最大值的和。</p>
<h6 id="计算每个key的平均值"><a href="#计算每个key的平均值" class="headerlink" title="计算每个key的平均值"></a>计算每个key的平均值</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value1 = value.aggregateByKey((<span class="number">0</span>, <span class="number">0</span>))(</span><br><span class="line">    (u, v) =&gt; (u._1 + v, u._2 + <span class="number">1</span>),</span><br><span class="line">    (u1, u2) =&gt; (u1._1 + u2._1, u1._2 + u2._2)</span><br><span class="line">)</span><br><span class="line">value1.mapValues(i =&gt; i._1 / i._2).collect().foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">(b,<span class="number">4</span>)</span><br><span class="line">(a,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<p>该伪代码中，将一个含两个元素的元组作为结果类型U，第一个元素为每个key的值的总数，第二个为值的个数。</p>
<p>在分区内处理的逻辑是，为第一个元素累加值，第二个元素加1。</p>
<p>分区间的处理是将这些值进行合并，总值和总值相加，总个数和总个数相加。最终aggregateByKey返回的结果是</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(b,(<span class="number">12</span>,<span class="number">3</span>))</span><br><span class="line">(a,(<span class="number">9</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<p>将结果使用mapValues函数处理，该函数只处理value，不处理key。因此将value的类型从元组转为Int即计算平均值。</p>
<h6 id="设置初始值"><a href="#设置初始值" class="headerlink" title="设置初始值"></a>设置初始值</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">6</span>)), <span class="number">2</span>)</span><br><span class="line">value.aggregateByKey(<span class="number">10</span>)((x, y) =&gt; <span class="type">Math</span>.max(x, y), (x, y) =&gt; x + y).collect().foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">(b,<span class="number">20</span>)</span><br><span class="line">(a,<span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<p>该初始值用于和第一个key元素进行逻辑计算。即当从第一个元素开始时我该与谁计算，这个zeroValue就是跟第一个计算的。当此时零值为10的时候，分区内进行最大值比较，最终的值就是10，一共两个分区，分区间进行处理相加就是20。</p>
<h3 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h3><h5 id="函数签名-18"><a href="#函数签名-18" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">foldByKey(zeroValue: <span class="type">V</span>)(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-18"><a href="#函数说明-18" class="headerlink" title="函数说明"></a>函数说明</h5><p>当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey</p>
<h5 id="示例-18"><a href="#示例-18" class="headerlink" title="示例"></a>示例</h5><h3 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h3><h5 id="函数签名-19"><a href="#函数签名-19" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">combineByKey[<span class="type">C</span>](</span><br><span class="line">      createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">      mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">      mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-19"><a href="#函数说明-19" class="headerlink" title="函数说明"></a>函数说明</h5><p>使用一组自定义聚合函数组合每个键的元素。对于“组合类型”C，将RDD[(K, V)]转换为类型RDD[(K, C)]的结果。</p>
<ul>
<li>createCombiner：它将V转换为C，例如，创建一个单元素列表，或一个自定义的元组数据(V, 1)</li>
<li>mergeValue：将一个V合并成一个C(例如，将它添加到列表的末尾)</li>
<li>mergeCombiners：将两个C合并成一个C</li>
</ul>
<p>与aggregateByKey的区别在于，它可以自定义组合类型，并返回这个类型，允许许用户返回值的类型与输入不一致</p>
<h5 id="示例-19"><a href="#示例-19" class="headerlink" title="示例"></a>示例</h5><h5 id="求每个key的平均值"><a href="#求每个key的平均值" class="headerlink" title="求每个key的平均值"></a>求每个key的平均值</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">6</span>)), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = value.combineByKey(</span><br><span class="line">    v=&gt;(v,<span class="number">1</span>),</span><br><span class="line">    (u:(<span class="type">Int</span>,<span class="type">Int</span>), v) =&gt; (u._1 + v, u._2 + <span class="number">1</span>),</span><br><span class="line">    (u1:(<span class="type">Int</span>,<span class="type">Int</span>), u2:(<span class="type">Int</span>,<span class="type">Int</span>)) =&gt; (u1._1 + u2._1, u1._2 + u2._2)</span><br><span class="line">)</span><br><span class="line">value1.mapValues(i =&gt; i._1 / i._2).collect().foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">(b,<span class="number">4</span>)</span><br><span class="line">(a,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<p>该函数是动态编译，因此需要显示指定参数类型。</p>
<p>此处RDD的值类型是Int类型，在createCombiner第一个参数中，将值转为元素类型。最终返回的结果也是RDD[(K, C)]类型，K为String，C为Tuple</p>
<h3 id="reduceByKey、foldByKey、aggregateByKey、combineByKey-的区别"><a href="#reduceByKey、foldByKey、aggregateByKey、combineByKey-的区别" class="headerlink" title="reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别"></a>reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别</h3><ul>
<li><p>reduceByKey：相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>) 分区内与分区间计算相同</span><br></pre></td></tr></table></figure>
</li>
<li><p>foldByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(zeroValue: <span class="type">V</span>)(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>aggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>combineByKey ：当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">combineByKey[<span class="type">C</span>](</span><br><span class="line">      createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">      mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">      mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>)</span><br></pre></td></tr></table></figure>



</li>
</ul>
<p>以上函数的最终的调用都是combineByKeyWithClassTag函数</p>
<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><h5 id="函数签名-20"><a href="#函数签名-20" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">join[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)], partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-20"><a href="#函数说明-20" class="headerlink" title="函数说明"></a>函数说明</h5><p>返回一个包含this和other中键匹配的所有元素对的RDD。每对元素将<strong>以(k， (v1, v2))元组的形式返回</strong>，其中(k, v1)在这个元组中，(k, v2)在另一个元组中。使用给定的Partitioner对输出RDD进行分区。</p>
<p>类似sql中的inner join</p>
<h5 id="示例-20"><a href="#示例-20" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>)), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">6</span>),(<span class="string">&quot;d&quot;</span>,(<span class="string">&quot;d&quot;</span>,<span class="number">2</span>))), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value2 = value.join(value1).collect().foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">(b,(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">(b,(<span class="number">2</span>,<span class="number">6</span>))</span><br><span class="line">(a,(<span class="number">1</span>,<span class="number">5</span>))</span><br><span class="line">(a,(<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<h3 id="leftOuterJoin、rightOuterJoin"><a href="#leftOuterJoin、rightOuterJoin" class="headerlink" title="leftOuterJoin、rightOuterJoin"></a>leftOuterJoin、rightOuterJoin</h3><h5 id="函数签名-21"><a href="#函数签名-21" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">leftOuterJoin[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">Option</span>[<span class="type">W</span>]))]</span><br><span class="line">rightOuterJoin[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Option</span>[<span class="type">V</span>], <span class="type">W</span>))]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-21"><a href="#函数说明-21" class="headerlink" title="函数说明"></a>函数说明</h5><p>执行this和other的左外连接。对于this中的每个元素(k, v)，产生的RDD<strong>要么包含other中的w的所有对(k， (v, Some(w))</strong>，要么<strong>包含other中没有键值为k的元素的对(k， (v, None))</strong></p>
<p>类似sql的左外连接和右外连接</p>
<h5 id="示例-21"><a href="#示例-21" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>)), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;b&quot;</span>, (<span class="number">6</span>,<span class="number">6</span>)),(<span class="string">&quot;d&quot;</span>,(<span class="string">&quot;d&quot;</span>,<span class="number">2</span>))), <span class="number">2</span>)</span><br><span class="line">value.leftOuterJoin(value1).collect().foreach(println)</span><br><span class="line"><span class="comment">// out 左外连接，没有other中d键的存在 (Some中只会存在一个元素)</span></span><br><span class="line">(b,(<span class="number">2</span>,<span class="type">Some</span>(<span class="number">4</span>)))</span><br><span class="line">(b,(<span class="number">2</span>,<span class="type">Some</span>((<span class="number">6</span>,<span class="number">6</span>))))</span><br><span class="line">(f,(<span class="number">9</span>,<span class="type">None</span>)) <span class="comment">// 包含other中没有键值为k的元素对</span></span><br><span class="line">(a,(<span class="number">1</span>,<span class="type">Some</span>(<span class="number">5</span>)))</span><br><span class="line">(a,(<span class="number">1</span>,<span class="type">Some</span>(<span class="number">6</span>)))</span><br><span class="line">(a,(<span class="number">3</span>,<span class="type">Some</span>(<span class="number">5</span>)))</span><br><span class="line">(a,(<span class="number">3</span>,<span class="type">Some</span>(<span class="number">6</span>)))</span><br></pre></td></tr></table></figure>

<h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3><h5 id="函数签名-22"><a href="#函数签名-22" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cogroup[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)], partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-22"><a href="#函数说明-22" class="headerlink" title="函数说明"></a>函数说明</h5><p>对于this或other中的每个键k，返回一个结果RDD，该RDD包含一个元组，其中包含该键以及其他键的值列表。</p>
<p>类似全连接</p>
<h5 id="示例-22"><a href="#示例-22" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>),(<span class="string">&quot;f&quot;</span>,<span class="number">9</span>)), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> value1 = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;b&quot;</span>, (<span class="number">6</span>,<span class="number">6</span>)),(<span class="string">&quot;d&quot;</span>,(<span class="string">&quot;d&quot;</span>,<span class="number">2</span>))), <span class="number">2</span>)</span><br><span class="line">value.cogroup(value1).collect().foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">(d,(<span class="type">CompactBuffer</span>(),<span class="type">CompactBuffer</span>((d,<span class="number">2</span>))))</span><br><span class="line">(b,(<span class="type">CompactBuffer</span>(<span class="number">2</span>),<span class="type">CompactBuffer</span>(<span class="number">4</span>, (<span class="number">6</span>,<span class="number">6</span>))))</span><br><span class="line">(f,(<span class="type">CompactBuffer</span>(<span class="number">9</span>),<span class="type">CompactBuffer</span>()))</span><br><span class="line">(a,(<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">3</span>),<span class="type">CompactBuffer</span>(<span class="number">5</span>, <span class="number">6</span>)))</span><br></pre></td></tr></table></figure>



<h3 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h3><p>agent.log：时间戳，省份，城市，用户，广告，中间字段使用空格分隔</p>
<p>统计出每一个省份<strong>每个广告被点击数量</strong>排行的 Top3</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.textFile(<span class="string">&quot;datas/agent.log&quot;</span>)</span><br><span class="line"><span class="comment">// 组装数据，某省某广告单条数据为1，格式((省份,广告id),1)</span></span><br><span class="line"><span class="keyword">val</span> mapcount = value.map(s =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> data = s.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">    ((data(<span class="number">1</span>), data(<span class="number">4</span>)), <span class="number">1</span>)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">// (某省某广告)的总数量，格式((省份,广告),sum)</span></span><br><span class="line"><span class="keyword">val</span> sumMap = mapcount.reduceByKey(_ + _)</span><br><span class="line"><span class="comment">// 转换为某省(某个广告的总数量) 格式(省份,(广告),sum) =&gt;就是为了分组省份以及整合values</span></span><br><span class="line"><span class="keyword">val</span> prov = sumMap.map(s =&gt; (s._1._1, (s._1._2, s._2)))</span><br><span class="line"><span class="comment">// 按省份分组，获取省份数据的集合</span></span><br><span class="line"><span class="keyword">val</span> groupBy = prov.groupByKey()</span><br><span class="line"><span class="comment">// 对各省份的集合数据进行处理，按数量倒序排序并裁剪前三条数据</span></span><br><span class="line"><span class="keyword">val</span> sortTake = groupBy.mapValues(it =&gt; it.toList.sortBy(f =&gt; f._2)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse).take(<span class="number">3</span>))</span><br><span class="line">sortTake.collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">(<span class="number">4</span>,<span class="type">List</span>((<span class="number">12</span>,<span class="number">25</span>), (<span class="number">2</span>,<span class="number">22</span>), (<span class="number">16</span>,<span class="number">22</span>)))</span><br><span class="line">(<span class="number">8</span>,<span class="type">List</span>((<span class="number">2</span>,<span class="number">27</span>), (<span class="number">20</span>,<span class="number">23</span>), (<span class="number">11</span>,<span class="number">22</span>)))</span><br><span class="line">(<span class="number">6</span>,<span class="type">List</span>((<span class="number">16</span>,<span class="number">23</span>), (<span class="number">24</span>,<span class="number">21</span>), (<span class="number">22</span>,<span class="number">20</span>)))</span><br><span class="line">(<span class="number">0</span>,<span class="type">List</span>((<span class="number">2</span>,<span class="number">29</span>), (<span class="number">24</span>,<span class="number">25</span>), (<span class="number">26</span>,<span class="number">24</span>)))</span><br><span class="line">(<span class="number">2</span>,<span class="type">List</span>((<span class="number">6</span>,<span class="number">24</span>), (<span class="number">21</span>,<span class="number">23</span>), (<span class="number">29</span>,<span class="number">20</span>)))</span><br><span class="line">(<span class="number">7</span>,<span class="type">List</span>((<span class="number">16</span>,<span class="number">26</span>), (<span class="number">26</span>,<span class="number">25</span>), (<span class="number">1</span>,<span class="number">23</span>)))</span><br><span class="line">(<span class="number">5</span>,<span class="type">List</span>((<span class="number">14</span>,<span class="number">26</span>), (<span class="number">21</span>,<span class="number">21</span>), (<span class="number">12</span>,<span class="number">21</span>)))</span><br><span class="line">(<span class="number">9</span>,<span class="type">List</span>((<span class="number">1</span>,<span class="number">31</span>), (<span class="number">28</span>,<span class="number">21</span>), (<span class="number">0</span>,<span class="number">20</span>)))</span><br><span class="line">(<span class="number">3</span>,<span class="type">List</span>((<span class="number">14</span>,<span class="number">28</span>), (<span class="number">28</span>,<span class="number">27</span>), (<span class="number">22</span>,<span class="number">25</span>)))</span><br><span class="line">(<span class="number">1</span>,<span class="type">List</span>((<span class="number">3</span>,<span class="number">25</span>), (<span class="number">6</span>,<span class="number">23</span>), (<span class="number">5</span>,<span class="number">22</span>)))</span><br></pre></td></tr></table></figure>



<h1 id="行动算子"><a href="#行动算子" class="headerlink" title="行动算子"></a>行动算子</h1><h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><h5 id="函数签名-23"><a href="#函数签名-23" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reduce(f: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span></span><br></pre></td></tr></table></figure>

<h5 id="函数说明-23"><a href="#函数说明-23" class="headerlink" title="函数说明"></a>函数说明</h5><p>聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据</p>
<h5 id="示例-23"><a href="#示例-23" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> i = value.reduce(_ + _)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line"><span class="number">21</span></span><br></pre></td></tr></table></figure>

<h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><h5 id="函数签名-24"><a href="#函数签名-24" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">collect(): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-24"><a href="#函数说明-24" class="headerlink" title="函数说明"></a>函数说明</h5><p>在驱动程序中，以数组 Array 的形式返回数据集的所有元素。</p>
<p>注意：这个方法只应该在结果数组很小的情况下使用，因为所有的数据都被加载到驱动程序Driver端的内存中</p>
<h5 id="示例-24"><a href="#示例-24" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line">value.collect().foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure>

<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><h5 id="函数签名-25"><a href="#函数签名-25" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count(): <span class="type">Long</span></span><br></pre></td></tr></table></figure>

<h5 id="函数说明-25"><a href="#函数说明-25" class="headerlink" title="函数说明"></a>函数说明</h5><p>返回RDD中元素的个数</p>
<h5 id="示例-25"><a href="#示例-25" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line">print(value.count())</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure>

<h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><h5 id="函数签名-26"><a href="#函数签名-26" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">first(): <span class="type">T</span></span><br></pre></td></tr></table></figure>

<h5 id="函数说明-26"><a href="#函数说明-26" class="headerlink" title="函数说明"></a>函数说明</h5><p>返回RDD中的第一个元素。内部调用take方法</p>
<h5 id="示例-26"><a href="#示例-26" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line">println(value.first())</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>

<h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><h5 id="函数签名-27"><a href="#函数签名-27" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">take(num: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-27"><a href="#函数说明-27" class="headerlink" title="函数说明"></a>函数说明</h5><p>返回一个由 RDD 的前 n 个元素组成的数组。它首先扫描一个分区，然后使用该分区的结果来估计满足限制所需的额外分区的数量。</p>
<p>注意:</p>
<p>这个方法只应该在结果数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。</p>
<p>注意:</p>
<p>由于内部实现的复杂性，如果在Nothing或Null的RDD上调用此方法，将引发异常。</p>
<h5 id="示例-27"><a href="#示例-27" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line">value.take(<span class="number">3</span>).foreach(print)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line"><span class="number">123</span></span><br></pre></td></tr></table></figure>

<h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h3><h5 id="函数签名-28"><a href="#函数签名-28" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">takeOrdered(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-28"><a href="#函数说明-28" class="headerlink" title="函数说明"></a>函数说明</h5><p>返回该 RDD 排序后的前 n 个元素组成的数组</p>
<p>注意:</p>
<p>这个方法只应该在结果数组很小的情况下使用，因为所有的数据都被加载到驱动程序的内存中。</p>
<h5 id="示例-28"><a href="#示例-28" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value1 = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>))</span><br><span class="line">value1.takeOrdered(<span class="number">2</span>).foreach(print)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line"><span class="number">12</span></span><br></pre></td></tr></table></figure>

<h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><h5 id="函数签名-29"><a href="#函数签名-29" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aggregate[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">T</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">U</span></span><br></pre></td></tr></table></figure>

<h5 id="函数说明-29"><a href="#函数说明-29" class="headerlink" title="函数说明"></a>函数说明</h5><p>分区的数据通过<strong>初始值</strong>和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合。</p>
<p>聚合每个分区的元素，然后使用给定的组合函数和中立的“零值”聚合所有分区的结果</p>
<h5 id="示例-29"><a href="#示例-29" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>),<span class="number">2</span>)</span><br><span class="line">println(value.aggregate(<span class="number">0</span>)(_ + _, _ + _))</span><br><span class="line"><span class="comment">// out </span></span><br><span class="line"><span class="number">21</span></span><br><span class="line">println(value.aggregate(<span class="number">10</span>)(_ + _, _ + _))</span><br><span class="line"><span class="comment">// out 10+1+2+3 + 10 + 10+4+5+6 + 10</span></span><br><span class="line"><span class="number">51</span></span><br></pre></td></tr></table></figure>

<h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><h5 id="函数签名-30"><a href="#函数签名-30" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fold(zeroValue: <span class="type">T</span>)(op: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span></span><br></pre></td></tr></table></figure>

<h5 id="函数说明-30"><a href="#函数说明-30" class="headerlink" title="函数说明"></a>函数说明</h5><p>折叠操作，aggregate 的简化版操作</p>
<h5 id="示例-30"><a href="#示例-30" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>),<span class="number">2</span>)</span><br><span class="line">println(value.fold(<span class="number">0</span>)(_ + _))</span><br><span class="line"><span class="comment">//out 21</span></span><br><span class="line">println(value.fold(<span class="number">10</span>)(_ + _))</span><br><span class="line"><span class="comment">//out 51</span></span><br></pre></td></tr></table></figure>

<h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h3><h5 id="函数签名-31"><a href="#函数签名-31" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">countByKey(): <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Long</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-31"><a href="#函数说明-31" class="headerlink" title="函数说明"></a>函数说明</h5><p>计算每个键的元素数量，将结果收集到本地Map</p>
<p>注意:</p>
<p>这个方法只应该在生成的map很小的情况下使用，因为整个东西都被加载到驱动程序的内存中。要处理非常大的结果，可以考虑使用rdd。mapValues(_ =&gt;1L). reduceByKey(_ + _) : RDD[T, Long] </p>
<h5 id="示例-31"><a href="#示例-31" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value1 = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">4</span>)))</span><br><span class="line">println(value1.countByKey())</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line"><span class="type">Map</span>(a -&gt; <span class="number">1</span>, b -&gt; <span class="number">1</span>, c -&gt; <span class="number">1</span>, d -&gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="countByValue"><a href="#countByValue" class="headerlink" title="countByValue"></a>countByValue</h3><h5 id="函数签名-32"><a href="#函数签名-32" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">countByValue()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">Map</span>[<span class="type">T</span>, <span class="type">Long</span>]</span><br></pre></td></tr></table></figure>

<h5 id="函数说明-32"><a href="#函数说明-32" class="headerlink" title="函数说明"></a>函数说明</h5><p>返回RDD中每个唯一值的计数，作为(value, count)对的本地映射</p>
<p>注意:</p>
<p>这个方法只应该在生成的map很小的情况下使用，因为整个东西都被加载到驱动程序的内存中。要处理非常大的结果，考虑使用</p>
<h5 id="示例-32"><a href="#示例-32" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>),<span class="number">2</span>)</span><br><span class="line">println(value.countByValue())</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line"><span class="type">Map</span>(<span class="number">5</span> -&gt; <span class="number">1</span>, <span class="number">1</span> -&gt; <span class="number">1</span>, <span class="number">6</span> -&gt; <span class="number">1</span>, <span class="number">2</span> -&gt; <span class="number">1</span>, <span class="number">3</span> -&gt; <span class="number">1</span>, <span class="number">4</span> -&gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="saveAsTextFile、saveAsObjectFile、saveAsSequenceFile"><a href="#saveAsTextFile、saveAsObjectFile、saveAsSequenceFile" class="headerlink" title="saveAsTextFile、saveAsObjectFile、saveAsSequenceFile"></a>saveAsTextFile、saveAsObjectFile、saveAsSequenceFile</h3><h5 id="函数签名-33"><a href="#函数签名-33" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">saveAsTextFile(path: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line">saveAsObjectFile(path: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line">saveAsSequenceFile(</span><br><span class="line">      path: <span class="type">String</span>,</span><br><span class="line">      codec: <span class="type">Option</span>[<span class="type">Class</span>[_ &lt;: <span class="type">CompressionCodec</span>]] = <span class="type">None</span>): <span class="type">Unit</span> </span><br></pre></td></tr></table></figure>

<h5 id="函数说明-33"><a href="#函数说明-33" class="headerlink" title="函数说明"></a>函数说明</h5><p>将数据保存到不同格式的文件中</p>
<h5 id="示例-33"><a href="#示例-33" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 保存成text文件，使用元素的字符串表示</span></span><br><span class="line">value.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"><span class="comment">// 将这个RDD保存为序列化对象的SequenceFile</span></span><br><span class="line">value.saveAsObjectFile(<span class="string">&quot;output1&quot;</span>)</span><br><span class="line"><span class="comment">// 将RDD作为Hadoop SequenceFile输出</span></span><br><span class="line">value.map((_,<span class="number">1</span>)).saveAsSequenceFile(<span class="string">&quot;output2&quot;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><h5 id="函数签名-34"><a href="#函数签名-34" class="headerlink" title="函数签名"></a>函数签名</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">foreach(f: <span class="type">T</span> =&gt; <span class="type">Unit</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure>

<h5 id="函数说明-34"><a href="#函数说明-34" class="headerlink" title="函数说明"></a>函数说明</h5><p>对该RDD的所有元素应用函数f。</p>
<p>该循环打印是无序的，执行操作是在executor端，而不是driver端，未进行收集操作。</p>
<h5 id="示例-34"><a href="#示例-34" class="headerlink" title="示例"></a>示例</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>),<span class="number">2</span>)</span><br><span class="line">value.foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">value.collect().foreach(println)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure>

 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">上一页</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/58/">58</a><a class="extend next" rel="next" href="/page/4/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2022
        <i class="ri-heart-fill heart_icon"></i> 陈光奇
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      
        <li>
          <a href="https://beian.miit.gov.cn/" target="_black" rel="nofollow">京ICP备17065668号-3</a>
        </li>
        
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer.jpg" alt="雪里"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/photos">相册</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2020/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.png">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->

<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>