<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="yanm1ng&#39;s blog">
  <meta name="keyword" content="hexo-theme, vuejs">
  
    <link rel="shortcut icon" href="/css/images/logo.png">
  
  <title>
    
      Spark SQL | 雪里
    
  </title>
  <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/tomorrow.min.css" rel="stylesheet">
  
<link rel="stylesheet" href="/css/style.css">

  
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.js"></script>
  
    
<script src="/js/qrious.js"></script>

  
  
  
  
    <!-- MathJax support START -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- MathJax support END -->
  


  
  
    
<script src="/js/local-search.js"></script>


<meta name="generator" content="Hexo 5.2.0"></head>
<div class="wechat-share">
  <img src="/css/images/logo.png" />
</div>
  <body>
    <header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>雪里</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/series/" class="item-link">Series</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/tags/" class="item-link">Tags</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/project/" class="item-link">Projects</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/about/" class="item-link">About</a>
          
        </li>
      
      
        <li class="menu-item menu-item-search right-list">
    <a role="button" class="popup-trigger">
        <i class="fa fa-search fa-fw"></i>
    </a>
</li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/series/" class="menu-link">Series</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/tags/" class="menu-link">Tags</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/project/" class="menu-link">Projects</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/about/" class="menu-link">About</a>
            
          </li>
        
      </ul>
    </div>
    
      <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
            <span class="search-icon">
                <i class="fa fa-search"></i>
            </span>
            <div class="search-input-container">
                <input autocomplete="off" autocapitalize="off"
                    placeholder="Please enter your keyword(s) to search." spellcheck="false"
                    type="search" class="search-input">
            </div>
            <span class="popup-btn-close">
                <i class="fa fa-times-circle"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>
    
  </div>
</header>

    <div id="article-banner">
  <h2>Spark SQL</h2>
  <p class="post-date">2022-05-04</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body flex-box">
  <!-- Article START -->
  <article class="post-article">
    <section class="markdown-content"><h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><p>​        Spark SQL是一个用于结构化数据处理的Spark模块。与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了更多关于数据结构和正在执行的计算的信息。Spark SQL在内部使用这些额外的信息来执行额外的优化。与Spark SQL交互的方式有多种，包括SQL和Dataset API。当计算结果时，将使用相同的执行引擎，而与您使用的API/语言无关。这种统一意味着开发人员可以轻松地在不同的api之间来回切换，这些api提供了表达给定转换的最自然的方式。</p>
<p>​        SparkSQL 可以简化 RDD 的开发，提高开发效率，且执行效率非常快，所以实际工作中，基本上采用的就是 SparkSQL。Spark SQL 为了简化 RDD 的开发，提高开发效率，提供了 2 个编程抽象，类似 Spark Core 中的 RDD</p>
<ul>
<li>DataFrame</li>
<li>DataSet</li>
</ul>
<h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul>
<li><p>易整合：无缝的整合了 SQL 查询和 Spark 编程</p>
</li>
<li><p>统一的数据访问：使用相同的方式连接不同的数据源</p>
</li>
<li><p>兼容 Hive：在已有的仓库上直接运行 SQL 或者 HiveQL</p>
</li>
<li><p>标准数据连接：通过 JDBC 或者 ODBC 来连接</p>
</li>
</ul>
<h2 id="DataFrame和DataSet"><a href="#DataFrame和DataSet" class="headerlink" title="DataFrame和DataSet"></a>DataFrame和DataSet</h2><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>​        在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame所表示的二维表数据集的每一列都带有名称和类型.</p>
<p>​        同时，与 Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）。从 API </p>
<p>易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要</p>
<p>更加友好，门槛更低。</p>
<p><img src="https://fsats-blog.oss-cn-beijing.aliyuncs.com/blog/image-20220504224019368.png" alt="image-20220504224019368"></p>
<p>​        左侧的 RDD[Person]虽然以 Person 为类型参数，但 Spark 框架本身不了解 Person 类的内部结构。而右侧的 DataFrame 却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。</p>
<p>​        <strong>DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待</strong>。</p>
<p>​        DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计</p>
<p>划通过 Spark catalyst optimiser 进行优化。</p>
<h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>​        DataSet 是分布式数据集合。DataSet 是 Spark 1.6 中添加的一个新抽象，是 DataFrame的一个扩展。它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter等等）。</p>
<p><strong>DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象</strong></p>
<ul>
<li><p><strong>用户友好的 API 风格，既具有类型安全检查也具有 DataFrame 的查询优化特性</strong>；</p>
</li>
<li><p>用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet 中的字段名称；</p>
</li>
<li><p>DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。 </p>
</li>
<li><p>DataFrame 是 DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为 DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。获取数据时需要指定顺序</p>
</li>
</ul>
<p>​        简而言之的说，RDD只是一堆数据，他可以是任意列表或Bean类型的数据，但是它只处理数据，并不操心数据是什么类型。具体如果处理数据由开发者来操作。</p>
<p>​        而DataFrame可以认为是一个含数据类型的RDD，它存储了数据的元数据信息，有其元数据属性和其数据类型，可以简要地认为它类似一张表。可以使用SQL语句的方式操作，以及部分API。</p>
<p>​        DataSet是DataFrame的一个拓展，有更好的API处理风格，可以直接操作API(类似Mysql Plus)来做分析。</p>
<h3 id="核心编程操作"><a href="#核心编程操作" class="headerlink" title="核心编程操作"></a>核心编程操作</h3><p>​        Spark Core 中，如果想要执行应用程序，需要首先构建上下文环境对象 SparkContext，Spark SQL 其实可以理解为对 Spark Core 的一种封装，不仅仅在模型上进行了封装，上下文环境对象也进行了封装。</p>
<p>​        SparkSession 是 Spark 最新的 SQL 查询起始点，实质上是 SQLContext 和 HiveContext的组合，所以在 SQLContex 和 HiveContext 上可用的 API 在 SparkSession 上同样是可以使用的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br></pre></td></tr></table></figure>



<h3 id="RDD、DataFrame、DataSet-操作及互相转换"><a href="#RDD、DataFrame、DataSet-操作及互相转换" class="headerlink" title="RDD、DataFrame、DataSet 操作及互相转换"></a>RDD、DataFrame、DataSet 操作及互相转换</h3><h4 id="DataFrame-1"><a href="#DataFrame-1" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>特定泛型的DataSet <code>type DataFrame = Dataset[Row]</code></p>
<p>使用spark的read方法获取数据，以json为例，此处需要的json是一行经过压缩的数据才行，或者是多行json格式的压缩数据，否则会加载失败。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;<span class="attr">&quot;username&quot;</span>:<span class="string">&quot;zhangsan&quot;</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">12</span>&#125;,&#123;<span class="attr">&quot;username&quot;</span>:<span class="string">&quot;xiaohong&quot;</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">16</span>&#125;,&#123;<span class="attr">&quot;username&quot;</span>:<span class="string">&quot;xiaoming&quot;</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">20</span>&#125;]</span><br></pre></td></tr></table></figure>

<p>使用show方法，可以在控制打印数据。字段名是按照字典类型的顺序</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;json/user.json&quot;</span>)</span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="number">12</span>|zhangsan|</span><br><span class="line">| <span class="number">16</span>|xiaohong|</span><br><span class="line">| <span class="number">20</span>|xiaoming|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure>

<h5 id="SQL风格语法"><a href="#SQL风格语法" class="headerlink" title="SQL风格语法"></a>SQL风格语法</h5><p>使用的是SparkSession的方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个临时视图，仅在当前SparkSession会话有效</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;select * from user&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;select username,age from user&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;select avg(age) as avgAge from user&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h5 id="DSL风格"><a href="#DSL风格" class="headerlink" title="DSL风格"></a>DSL风格</h5><p>使用DSL风格，需要从SparkSession中导入包，直接写在该行代码下即可</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> row = df.filter(<span class="symbol">&#x27;age</span> &gt; <span class="number">15</span>).select($<span class="string">&quot;username&quot;</span>,$<span class="string">&quot;age&quot;</span>+<span class="number">1</span>).orderBy(<span class="symbol">&#x27;age</span>.desc).first()</span><br><span class="line">println(row)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">[xiaoming,<span class="number">21</span>]</span><br></pre></td></tr></table></figure>

<p>以上代码中，必须要引入<code>import spark.implicits._</code>，否则在编辑器中就会出现语法错误。</p>
<p>可以直接使用字段名作为参数，如果为参数进行更改或附加使用，则需要使用<code>$&quot;字段名&quot;</code>或<code>&#39;字段名</code>的方式引用字段，这样才能做操作。另外在这些多参方法中，如果其中一个字段使用了DSL语法，则所有字段都需要设置成DSL语法格式，否则依然会出现语法错误。</p>
<h4 id="DataSet-1"><a href="#DataSet-1" class="headerlink" title="DataSet"></a>DataSet</h4><p>可以通过序列创建DataSet，序列中可以是基本数据类集合类型或Bean类型。但必须要导入implicits转换器</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> seq = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">Int</span>] = seq.toDS()</span><br><span class="line">ds.show()</span><br></pre></td></tr></table></figure>

<p>其相关api方法与DataFrame基本一致</p>
<h4 id="互相转换"><a href="#互相转换" class="headerlink" title="互相转换"></a>互相转换</h4><h5 id="RDD-lt-gt-DataFrame"><a href="#RDD-lt-gt-DataFrame" class="headerlink" title="RDD &lt;=&gt; DataFrame"></a>RDD &lt;=&gt; DataFrame</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;小红&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;小明&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;小刚&quot;</span>, <span class="number">14</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = value.toDF(<span class="string">&quot;username&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Row</span>] = df.rdd</span><br></pre></td></tr></table></figure>

<h5 id="DataFrame-lt-gt-DataSet"><a href="#DataFrame-lt-gt-DataSet" class="headerlink" title="DataFrame &lt;=&gt; DataSet"></a>DataFrame &lt;=&gt; DataSet</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ds:<span class="type">Dataset</span>[<span class="type">User</span>] = df.as[<span class="type">User</span>]</span><br><span class="line"><span class="keyword">val</span> fd1:<span class="type">DataFrame</span> = ds.toDF()</span><br></pre></td></tr></table></figure>

<h5 id="RDD-lt-gt-DataSet"><a href="#RDD-lt-gt-DataSet" class="headerlink" title="RDD &lt;=&gt; DataSet"></a>RDD &lt;=&gt; DataSet</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ds1 = value.map(r =&gt; <span class="type">User</span>(r._1, r._2)).toDS()</span><br><span class="line"><span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">User</span>] = ds1.rdd</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">username:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>

<p>首先DataFrame是特定泛型的Dataset。</p>
<p>让RDD转DataFrame时，只需要知道元数据字段是什么即可，而转DataSet时,需要一个强类型，只需要构建成Bean即可转换。</p>
<p>DataFrame转换DataSet时，因为缺少强类型，所以需要通过as来转换成具体的Bean泛型。反之，向弱转换直接toDF即可。</p>
<p>当向RDD转换时，直接rdd即可。</p>
<h3 id="三者之间的关系"><a href="#三者之间的关系" class="headerlink" title="三者之间的关系"></a>三者之间的关系</h3><p>​        在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是 DataFrame 和 DataSet。他们和 RDD 有什么区别呢？首先从版本的产生上来看。</p>
<ul>
<li>Spark1.0 =&gt; RDD</li>
<li>Spark1.3 =&gt; DataFrame</li>
<li>Spark1.6 =&gt; Dataset</li>
</ul>
<h4 id="三者共性"><a href="#三者共性" class="headerlink" title="三者共性"></a>三者共性</h4><ul>
<li><p>RDD、DataFrame、DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数据提供便利; </p>
</li>
<li><p>三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到Action 如 foreach 时，三者才会开始遍历运算; </p>
</li>
<li><p>三者有许多共同的函数，如 filter，排序等; </p>
</li>
<li><p>在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包:import spark.implicits._（在创建好 SparkSession 对象后尽量直接导入）</p>
</li>
<li><p>三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</p>
</li>
<li><p>三者都有 partition 的概念</p>
</li>
<li><p>DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型</p>
</li>
</ul>
<h4 id="三者区别"><a href="#三者区别" class="headerlink" title="三者区别"></a>三者区别</h4><p><strong>RDD</strong></p>
<ul>
<li>RDD 一般和 spark mllib 同时使用</li>
<li>RDD 不支持 sparksql 操作</li>
</ul>
<p><strong>DataFrame</strong></p>
<ul>
<li>与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值</li>
<li>DataFrame 与 DataSet 一般不与 spark mllib 同时使用</li>
<li>DataFrame 与 DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能注册临时表/视窗，进行 sql 语句操作</li>
<li>DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然</li>
</ul>
<h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><ul>
<li>Dataset 和 DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同。DataFrame 其实就是 DataSet 的一个特例 type DataFrame = Dataset[Row]</li>
<li>DataFrame 也可以叫 Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性中的第七条提到的模式匹配拿出特定字段。而 Dataset 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息</li>
</ul>
<p><img src="https://fsats-blog.oss-cn-beijing.aliyuncs.com/blog/image-20220505002901546.png" alt="image-20220505002901546"></p>
<h2 id="UDF、UDAF-用户自定义函数"><a href="#UDF、UDAF-用户自定义函数" class="headerlink" title="UDF、UDAF 用户自定义函数"></a>UDF、UDAF 用户自定义函数</h2><p>​        用户定义函数 (UDF) 是 Spark SQL 的一项功能，当系统的内置函数不足以执行所需的任务时，它允许用户定义自己的函数。在 Spark SQL 中使用 UDF，首先需要定义函数，然后向 Spark 注册函数，最后调用注册的函数。用户定义函数可以作用于单行或一次作用于多行。Spark SQL 还支持 UDF、UDAF 和 UDTF 的现有 Hive 实现的集成。</p>
<p>用户可以通过 spark.udf 功能添加自定义函数，实现自定义功能。</p>
<h3 id="UDF-用户定义函数"><a href="#UDF-用户定义函数" class="headerlink" title="UDF 用户定义函数"></a>UDF 用户定义函数</h3><p>用户定义函数 (UDF) 是作用于一行的用户可编程例程。</p>
<p>要定义用户定义函数的属性，用户可以使用该类中定义的一些方法。</p>
<ul>
<li><p>asNonNullable(): UserDefinedFunction</p>
<p>将 UserDefinedFunction 更新为不可为空。即UDF的返回值不能为空</p>
</li>
<li><p>asNondeterministic(): UserDefinedFunction</p>
<p>将 UserDefinedFunction 更新为不确定。即每次入参返回的结果都可能不一样</p>
</li>
<li><p>withName(name: String): UserDefinedFunction</p>
<p>使用给定名称更新 UserDefinedFunction。</p>
</li>
</ul>
<p>以下代码中的udf函数构建的对象是UserDefinedFunction抽象类的子类SparkUserDefinedFunction</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.udf</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> random = udf(() =&gt; <span class="type">Math</span>.random())</span><br><span class="line">spark.udf.register(<span class="string">&quot;random&quot;</span>,random.asNondeterministic())</span><br><span class="line"><span class="comment">// 或者不需要显示构建udf</span></span><br><span class="line"><span class="comment">//spark.udf.register(&quot;random&quot;,()=&gt;Math.random())</span></span><br><span class="line">spark.sql(<span class="string">&quot;select random()&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+------------------+</span><br><span class="line">|             <span class="type">UDF</span>()|</span><br><span class="line">+------------------+</span><br><span class="line">|<span class="number">0.5220181936811188</span>|</span><br><span class="line">+------------------+</span><br></pre></td></tr></table></figure>

<p><strong>定义两个参数的udf函数</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spark.udf.register(<span class="string">&quot;strLenScala&quot;</span>, (_: <span class="type">String</span>).length + (_: <span class="type">Int</span>))</span><br><span class="line"><span class="comment">// 两种表达式定义的一样</span></span><br><span class="line"><span class="comment">// spark.udf.register(&quot;strLenScala&quot;, (s: String, i: Int) =&gt; s.length + i)</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT strLenScala(&#x27;test&#x27;, 1)&quot;</span>).show()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+--------------------+</span><br><span class="line">|strLenScala(test, <span class="number">1</span>)|</span><br><span class="line">+--------------------+</span><br><span class="line">|                   <span class="number">5</span>|</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>

<p><strong>定义作为where条件的UDF</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">spark.udf.register(<span class="string">&quot;oneArgFilter&quot;</span>, (n: <span class="type">Int</span>) =&gt; &#123; n &gt; <span class="number">5</span> &#125;)</span><br><span class="line">spark.range(<span class="number">1</span>, <span class="number">10</span>).createOrReplaceTempView(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM test WHERE oneArgFilter(id)&quot;</span>).show()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">6</span>|</span><br><span class="line">|  <span class="number">7</span>|</span><br><span class="line">|  <span class="number">8</span>|</span><br><span class="line">|  <span class="number">9</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>

<h3 id="UDAF弱类型实现-新版3-0-0以后已标记为弃用-实现UserDefinedAggregateFunction类"><a href="#UDAF弱类型实现-新版3-0-0以后已标记为弃用-实现UserDefinedAggregateFunction类" class="headerlink" title="UDAF弱类型实现(新版3.0.0以后已标记为弃用) 实现UserDefinedAggregateFunction类"></a>UDAF弱类型实现(新版3.0.0以后已标记为弃用) 实现UserDefinedAggregateFunction类</h3><p>用户定义的聚合函数 (UDAF) 是用户可编程的例程，它们一次作用于多行并返回单个聚合值作为结果。</p>
<p>需要实现<code>org.apache.spark.sql.expressions.UserDefinedAggregateFunction</code>类</p>
<p>以模拟avg平均值计算为例</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">DoubleType</span>, <span class="type">LongType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 定义了求平均数的UDF</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 此聚合函数输入参数的数据类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>,<span class="type">LongType</span>)::<span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 聚合缓冲区中值的数据类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;sum&quot;</span>, <span class="type">LongType</span>), <span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 返回值的数据类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这个函数是否总是在相同的输入上返回相同的输出</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 初始化给定的聚合缓冲区。缓冲区本身是一个“行” 继承自Row，</span></span><br><span class="line">  <span class="comment">// 除了标准的方法(如在索引中检索值(如get()， getBoolean()))之外，</span></span><br><span class="line">  <span class="comment">// 还提供了更新其值的机会。注意，缓冲区内的数组和映射仍然是不可变的</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//buffer(0) = 0</span></span><br><span class="line">    <span class="comment">//buffer(1) = 0</span></span><br><span class="line">    buffer.update(<span class="number">0</span>,<span class="number">0</span>L)</span><br><span class="line">    buffer.update(<span class="number">1</span>,<span class="number">0</span>L)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 使用来自&#x27; input &#x27;的新输入数据更新给定的聚合缓冲区&#x27; buffer &#x27;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer.update(<span class="number">0</span>, buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>)) <span class="comment">// 增加值</span></span><br><span class="line">      buffer.update(<span class="number">1</span>, buffer.getLong(<span class="number">1</span>) + <span class="number">1</span>L) <span class="comment">// 增加数量</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并两个聚合缓冲区，并将更新的缓冲区值存储回&#x27; buffer1 &#x27;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1.update(<span class="number">0</span>, buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>))</span><br><span class="line">    buffer1.update(<span class="number">1</span>, buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算最终结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">spark.udf.register(<span class="string">&quot;myAverage&quot;</span>,<span class="type">MyAverage</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;json/user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">&quot;select myAverage(age) as avg_age from user&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+-------+</span><br><span class="line">|avg_age|</span><br><span class="line">+-------+</span><br><span class="line">|   <span class="number">16.0</span>|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure>



<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.8/api/scala/index.html#org.apache.spark.sql.functions$">内置的 DataFrames 函数</a>提供了常见的聚合，例如<code>count()</code>, <code>countDistinct()</code>, <code>avg()</code>, <code>max()</code>,<code>min()</code>等。虽然这些函数是为 DataFrames 设计的，但 Spark SQL 还为其中一些在 <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.8/api/scala/index.html#org.apache.spark.sql.expressions.scalalang.typed$">Scala</a>和 <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.8/api/java/org/apache/spark/sql/expressions/javalang/typed.html">Java</a>中提供了类型安全版本，以处理强类型数据集</p>
<h3 id="UDAF强类型实现-实现-Aggregator类"><a href="#UDAF强类型实现-实现-Aggregator类" class="headerlink" title="UDAF强类型实现 实现 Aggregator类"></a>UDAF强类型实现 实现 Aggregator类</h3><p>强类型数据集的用户定义聚合围绕<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.2.1/api/scala/org/apache/spark/sql/expressions/Aggregator.html">Aggregator</a>抽象类</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoder</span>, <span class="type">Encoders</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">username: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Average</span>(<span class="params">var sum: <span class="type">Long</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">/**</span></span></span><br><span class="line"><span class="class"><span class="comment"> * 定义平均值,继承Aggregator，采用强类型</span></span></span><br><span class="line"><span class="class"><span class="comment"> */</span></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">MyAverage2</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">User</span>, <span class="type">Average</span>, <span class="type">Double</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 零值、初始值。是否满足任意b + 0 = b的性质</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Average</span> = <span class="type">Average</span>(<span class="number">0</span>L, <span class="number">0</span>L)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并两个值以产生一个新值。出于性能考虑，函数可以修改“buffer”并返回它，而不是构造一个新对象</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">Average</span>, a: <span class="type">User</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">    b.sum += a.age</span><br><span class="line">    b.count += <span class="number">1</span>L</span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并两个聚合缓冲区值</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Average</span>, b2: <span class="type">Average</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">    b1.sum += b2.sum</span><br><span class="line">    b1.count += b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 结果输出函数,计算最终结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Average</span>): <span class="type">Double</span> = reduction.sum.doubleValue() / reduction.count</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 为聚合缓冲区指定编码器</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Average</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 为最终输出值类型指定编码器</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">spark.udf.register(<span class="string">&quot;myAverage&quot;</span>,<span class="type">MyAverage</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds = spark.read.json(<span class="string">&quot;json/user.json&quot;</span>).as[<span class="type">User</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> average = <span class="type">MyAverage2</span>.toColumn.name(<span class="string">&quot;avg_age&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> result = ds.select(average)</span><br><span class="line">result.show()</span><br><span class="line">spark.stop()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+-------+</span><br><span class="line">|avg_age|</span><br><span class="line">+-------+</span><br><span class="line">|   <span class="number">16.0</span>|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure>



<h2 id="数据读取与保存"><a href="#数据读取与保存" class="headerlink" title="数据读取与保存"></a>数据读取与保存</h2></section>
    <!-- Tags START -->
    
    <!-- Tags END -->
    <!-- NAV START -->
    
  <div class="nav-container">
    <!-- reverse left and right to put prev and next in a more logic postition -->
    
      <a class="nav-left" href="/2022/05/02/Spark/Spark%E6%A0%B8%E5%BF%83/">
        <span class="nav-arrow">← </span>
        
          Spark核心
        
      </a>
    
    
      <a class="nav-right" href="/2022/05/04/Scala/Scala%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/">
        
          Scala数据类型
        
        <span class="nav-arrow"> →</span>
      </a>
    
  </div>

    <!-- NAV END -->
    <!-- 打赏 START -->
    
      <div class="money-like">
        <div class="reward-btn">
          赏
          <span class="money-code">
            <span class="alipay-code">
              <div class="code-image"></div>
              <b>使用支付宝打赏</b>
            </span>
            <span class="wechat-code">
              <div class="code-image"></div>
              <b>使用微信打赏</b>
            </span>
          </span>
        </div>
        <p class="notice">若你觉得我的文章对你有帮助，欢迎点击上方按钮对我打赏</p>
      </div>
    
    <!-- 打赏 END -->
    <!-- 二维码 START -->
    
      <div class="qrcode">
        <canvas id="share-qrcode"></canvas>
        <p class="notice">扫描二维码，分享此文章</p>
      </div>
    
    <!-- 二维码 END -->
    
      <!-- Utterances START -->
      <div id="utterances"></div>
      <script src="https://utteranc.es/client.js"
        repo=""
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async></script>    
      <!-- Utterances END -->
    
  </article>
  <!-- Article END -->
  <!-- Catalog START -->
  
    <aside class="catalog-container">
  <div class="toc-main">
    <strong class="toc-title">Catalog</strong>
    
      <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#Spark-SQL"><span class="toc-nav-text">Spark SQL</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E4%BC%98%E5%8A%BF"><span class="toc-nav-text">优势</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#DataFrame%E5%92%8CDataSet"><span class="toc-nav-text">DataFrame和DataSet</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#DataFrame"><span class="toc-nav-text">DataFrame</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#DataSet"><span class="toc-nav-text">DataSet</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B%E6%93%8D%E4%BD%9C"><span class="toc-nav-text">核心编程操作</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#RDD%E3%80%81DataFrame%E3%80%81DataSet-%E6%93%8D%E4%BD%9C%E5%8F%8A%E4%BA%92%E7%9B%B8%E8%BD%AC%E6%8D%A2"><span class="toc-nav-text">RDD、DataFrame、DataSet 操作及互相转换</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#DataFrame-1"><span class="toc-nav-text">DataFrame</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#SQL%E9%A3%8E%E6%A0%BC%E8%AF%AD%E6%B3%95"><span class="toc-nav-text">SQL风格语法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#DSL%E9%A3%8E%E6%A0%BC"><span class="toc-nav-text">DSL风格</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#DataSet-1"><span class="toc-nav-text">DataSet</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E4%BA%92%E7%9B%B8%E8%BD%AC%E6%8D%A2"><span class="toc-nav-text">互相转换</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#RDD-lt-gt-DataFrame"><span class="toc-nav-text">RDD &lt;&#x3D;&gt; DataFrame</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#DataFrame-lt-gt-DataSet"><span class="toc-nav-text">DataFrame &lt;&#x3D;&gt; DataSet</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#RDD-lt-gt-DataSet"><span class="toc-nav-text">RDD &lt;&#x3D;&gt; DataSet</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-nav-text">三者之间的关系</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E4%B8%89%E8%80%85%E5%85%B1%E6%80%A7"><span class="toc-nav-text">三者共性</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E4%B8%89%E8%80%85%E5%8C%BA%E5%88%AB"><span class="toc-nav-text">三者区别</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Dataset"><span class="toc-nav-text">Dataset</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#UDF%E3%80%81UDAF-%E7%94%A8%E6%88%B7%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="toc-nav-text">UDF、UDAF 用户自定义函数</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#UDF-%E7%94%A8%E6%88%B7%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="toc-nav-text">UDF 用户定义函数</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#UDAF%E5%BC%B1%E7%B1%BB%E5%9E%8B%E5%AE%9E%E7%8E%B0-%E6%96%B0%E7%89%883-0-0%E4%BB%A5%E5%90%8E%E5%B7%B2%E6%A0%87%E8%AE%B0%E4%B8%BA%E5%BC%83%E7%94%A8-%E5%AE%9E%E7%8E%B0UserDefinedAggregateFunction%E7%B1%BB"><span class="toc-nav-text">UDAF弱类型实现(新版3.0.0以后已标记为弃用) 实现UserDefinedAggregateFunction类</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#UDAF%E5%BC%BA%E7%B1%BB%E5%9E%8B%E5%AE%9E%E7%8E%B0-%E5%AE%9E%E7%8E%B0-Aggregator%E7%B1%BB"><span class="toc-nav-text">UDAF强类型实现 实现 Aggregator类</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E4%BF%9D%E5%AD%98"><span class="toc-nav-text">数据读取与保存</span></a></li></ol></li></ol>
    
  </div>
</aside>
  
  <!-- Catalog END -->
</main>

<script>
  (function () {
    var url = 'https://www.chenguangqi.com/2022/05/04/Spark/Spark-SQL/';
    var banner = ''
    if (banner !== '' && banner !== 'undefined' && banner !== 'null') {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

    // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', '/css/images/error_icon.png')
      $(this).css({
        'cursor': 'default'
      })
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      if (src !== '/css/images/error_icon.png') {
        var imageW = $(this).width()
        var imageH = $(this).height()

        var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
        zoom = zoom < 1 ? 1 : zoom
        zoom = zoom > 2 ? 2 : zoom
        var transY = (($(window).height() - imageH) / 2).toFixed(2)

        $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
        $('.image-view-wrap').addClass('wrap-active')
        $('.image-view-wrap img').css({
          'width': `${imageW}`,
          'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
        })
        $('html').css('overflow', 'hidden')

        $('.image-view-wrap').on('click', function() {
          $(this).remove()
          $('html').attr('style', '')
        })
      }
    })
  })();
</script>


  <script>
    var qr = new QRious({
      element: document.getElementById('share-qrcode'),
      value: document.location.href
    });
  </script>






    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    &copy; 2022 | Proudly powered by <a href="https://hexo.io" target="_blank">Hexo</a>
    <br>
    Theme by <a target="_blank" rel="noopener" href="https://github.com/yanm1ng">yanm1ng</a>
    
  </p>
</footer>

<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine === 'false') {
        figure.find('.gutter').hide();
      }
      hljs.configure({useBR: true});
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->


<script src="/js/script.js"></script>


  </body>
</html>