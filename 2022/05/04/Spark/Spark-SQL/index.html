<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>Spark SQL |  雪里</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-Spark/Spark-SQL"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Spark SQL
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/05/04/Spark/Spark-SQL/" class="article-date">
  <time datetime="2022-05-03T16:00:00.000Z" itemprop="datePublished">2022-05-04</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">3.8k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">15 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><p>​        Spark SQL是一个用于结构化数据处理的Spark模块。与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了更多关于数据结构和正在执行的计算的信息。Spark SQL在内部使用这些额外的信息来执行额外的优化。与Spark SQL交互的方式有多种，包括SQL和Dataset API。当计算结果时，将使用相同的执行引擎，而与您使用的API/语言无关。这种统一意味着开发人员可以轻松地在不同的api之间来回切换，这些api提供了表达给定转换的最自然的方式。</p>
<p>​        SparkSQL 可以简化 RDD 的开发，提高开发效率，且执行效率非常快，所以实际工作中，基本上采用的就是 SparkSQL。Spark SQL 为了简化 RDD 的开发，提高开发效率，提供了 2 个编程抽象，类似 Spark Core 中的 RDD</p>
<ul>
<li>DataFrame</li>
<li>DataSet</li>
</ul>
<h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul>
<li><p>易整合：无缝的整合了 SQL 查询和 Spark 编程</p>
</li>
<li><p>统一的数据访问：使用相同的方式连接不同的数据源</p>
</li>
<li><p>兼容 Hive：在已有的仓库上直接运行 SQL 或者 HiveQL</p>
</li>
<li><p>标准数据连接：通过 JDBC 或者 ODBC 来连接</p>
</li>
</ul>
<h2 id="DataFrame和DataSet"><a href="#DataFrame和DataSet" class="headerlink" title="DataFrame和DataSet"></a>DataFrame和DataSet</h2><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>​        在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame所表示的二维表数据集的每一列都带有名称和类型.</p>
<p>​        同时，与 Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）。从 API </p>
<p>易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要</p>
<p>更加友好，门槛更低。</p>
<p><img src="https://fsats-blog.oss-cn-beijing.aliyuncs.com/blog/image-20220504224019368.png" alt="image-20220504224019368"></p>
<p>​        左侧的 RDD[Person]虽然以 Person 为类型参数，但 Spark 框架本身不了解 Person 类的内部结构。而右侧的 DataFrame 却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。</p>
<p>​        <strong>DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待</strong>。</p>
<p>​        DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计</p>
<p>划通过 Spark catalyst optimiser 进行优化。</p>
<h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>​        DataSet 是分布式数据集合。DataSet 是 Spark 1.6 中添加的一个新抽象，是 DataFrame的一个扩展。它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter等等）。</p>
<p><strong>DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象</strong></p>
<ul>
<li><p><strong>用户友好的 API 风格，既具有类型安全检查也具有 DataFrame 的查询优化特性</strong>；</p>
</li>
<li><p>用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet 中的字段名称；</p>
</li>
<li><p>DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。 </p>
</li>
<li><p>DataFrame 是 DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将DataFrame 转换为 DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。获取数据时需要指定顺序</p>
</li>
</ul>
<p>​        简而言之的说，RDD只是一堆数据，他可以是任意列表或Bean类型的数据，但是它只处理数据，并不操心数据是什么类型。具体如果处理数据由开发者来操作。</p>
<p>​        而DataFrame可以认为是一个含数据类型的RDD，它存储了数据的元数据信息，有其元数据属性和其数据类型，可以简要地认为它类似一张表。可以使用SQL语句的方式操作，以及部分API。</p>
<p>​        DataSet是DataFrame的一个拓展，有更好的API处理风格，可以直接操作API(类似Mysql Plus)来做分析。</p>
<h3 id="核心编程操作"><a href="#核心编程操作" class="headerlink" title="核心编程操作"></a>核心编程操作</h3><p>​        Spark Core 中，如果想要执行应用程序，需要首先构建上下文环境对象 SparkContext，Spark SQL 其实可以理解为对 Spark Core 的一种封装，不仅仅在模型上进行了封装，上下文环境对象也进行了封装。</p>
<p>​        SparkSession 是 Spark 最新的 SQL 查询起始点，实质上是 SQLContext 和 HiveContext的组合，所以在 SQLContex 和 HiveContext 上可用的 API 在 SparkSession 上同样是可以使用的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br></pre></td></tr></table></figure>



<h3 id="RDD、DataFrame、DataSet-操作及互相转换"><a href="#RDD、DataFrame、DataSet-操作及互相转换" class="headerlink" title="RDD、DataFrame、DataSet 操作及互相转换"></a>RDD、DataFrame、DataSet 操作及互相转换</h3><h4 id="DataFrame-1"><a href="#DataFrame-1" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>特定泛型的DataSet <code>type DataFrame = Dataset[Row]</code></p>
<p>使用spark的read方法获取数据，以json为例，此处需要的json是一行经过压缩的数据才行，或者是多行json格式的压缩数据，否则会加载失败。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;<span class="attr">&quot;username&quot;</span>:<span class="string">&quot;zhangsan&quot;</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">12</span>&#125;,&#123;<span class="attr">&quot;username&quot;</span>:<span class="string">&quot;xiaohong&quot;</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">16</span>&#125;,&#123;<span class="attr">&quot;username&quot;</span>:<span class="string">&quot;xiaoming&quot;</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">20</span>&#125;]</span><br></pre></td></tr></table></figure>

<p>使用show方法，可以在控制打印数据。字段名是按照字典类型的顺序</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;json/user.json&quot;</span>)</span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="number">12</span>|zhangsan|</span><br><span class="line">| <span class="number">16</span>|xiaohong|</span><br><span class="line">| <span class="number">20</span>|xiaoming|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure>

<h5 id="SQL风格语法"><a href="#SQL风格语法" class="headerlink" title="SQL风格语法"></a>SQL风格语法</h5><p>使用的是SparkSession的方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个临时视图，仅在当前SparkSession会话有效</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;select * from user&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;select username,age from user&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;select avg(age) as avgAge from user&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h5 id="DSL风格"><a href="#DSL风格" class="headerlink" title="DSL风格"></a>DSL风格</h5><p>使用DSL风格，需要从SparkSession中导入包，直接写在该行代码下即可</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> row = df.filter(<span class="symbol">&#x27;age</span> &gt; <span class="number">15</span>).select($<span class="string">&quot;username&quot;</span>,$<span class="string">&quot;age&quot;</span>+<span class="number">1</span>).orderBy(<span class="symbol">&#x27;age</span>.desc).first()</span><br><span class="line">println(row)</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">[xiaoming,<span class="number">21</span>]</span><br></pre></td></tr></table></figure>

<p>以上代码中，必须要引入<code>import spark.implicits._</code>，否则在编辑器中就会出现语法错误。</p>
<p>可以直接使用字段名作为参数，如果为参数进行更改或附加使用，则需要使用<code>$&quot;字段名&quot;</code>或<code>&#39;字段名</code>的方式引用字段，这样才能做操作。另外在这些多参方法中，如果其中一个字段使用了DSL语法，则所有字段都需要设置成DSL语法格式，否则依然会出现语法错误。</p>
<h4 id="DataSet-1"><a href="#DataSet-1" class="headerlink" title="DataSet"></a>DataSet</h4><p>可以通过序列创建DataSet，序列中可以是基本数据类集合类型或Bean类型。但必须要导入implicits转换器</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> seq = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">Int</span>] = seq.toDS()</span><br><span class="line">ds.show()</span><br></pre></td></tr></table></figure>

<p>其相关api方法与DataFrame基本一致</p>
<h4 id="互相转换"><a href="#互相转换" class="headerlink" title="互相转换"></a>互相转换</h4><h5 id="RDD-lt-gt-DataFrame"><a href="#RDD-lt-gt-DataFrame" class="headerlink" title="RDD &lt;=&gt; DataFrame"></a>RDD &lt;=&gt; DataFrame</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;小红&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;小明&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;小刚&quot;</span>, <span class="number">14</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = value.toDF(<span class="string">&quot;username&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Row</span>] = df.rdd</span><br></pre></td></tr></table></figure>

<h5 id="DataFrame-lt-gt-DataSet"><a href="#DataFrame-lt-gt-DataSet" class="headerlink" title="DataFrame &lt;=&gt; DataSet"></a>DataFrame &lt;=&gt; DataSet</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ds:<span class="type">Dataset</span>[<span class="type">User</span>] = df.as[<span class="type">User</span>]</span><br><span class="line"><span class="keyword">val</span> fd1:<span class="type">DataFrame</span> = ds.toDF()</span><br></pre></td></tr></table></figure>

<h5 id="RDD-lt-gt-DataSet"><a href="#RDD-lt-gt-DataSet" class="headerlink" title="RDD &lt;=&gt; DataSet"></a>RDD &lt;=&gt; DataSet</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ds1 = value.map(r =&gt; <span class="type">User</span>(r._1, r._2)).toDS()</span><br><span class="line"><span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">User</span>] = ds1.rdd</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">username:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>

<p>首先DataFrame是特定泛型的Dataset。</p>
<p>让RDD转DataFrame时，只需要知道元数据字段是什么即可，而转DataSet时,需要一个强类型，只需要构建成Bean即可转换。</p>
<p>DataFrame转换DataSet时，因为缺少强类型，所以需要通过as来转换成具体的Bean泛型。反之，向弱转换直接toDF即可。</p>
<p>当向RDD转换时，直接rdd即可。</p>
<h3 id="三者之间的关系"><a href="#三者之间的关系" class="headerlink" title="三者之间的关系"></a>三者之间的关系</h3><p>​        在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是 DataFrame 和 DataSet。他们和 RDD 有什么区别呢？首先从版本的产生上来看。</p>
<ul>
<li>Spark1.0 =&gt; RDD</li>
<li>Spark1.3 =&gt; DataFrame</li>
<li>Spark1.6 =&gt; Dataset</li>
</ul>
<h4 id="三者共性"><a href="#三者共性" class="headerlink" title="三者共性"></a>三者共性</h4><ul>
<li><p>RDD、DataFrame、DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数据提供便利; </p>
</li>
<li><p>三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到Action 如 foreach 时，三者才会开始遍历运算; </p>
</li>
<li><p>三者有许多共同的函数，如 filter，排序等; </p>
</li>
<li><p>在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包:import spark.implicits._（在创建好 SparkSession 对象后尽量直接导入）</p>
</li>
<li><p>三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</p>
</li>
<li><p>三者都有 partition 的概念</p>
</li>
<li><p>DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型</p>
</li>
</ul>
<h4 id="三者区别"><a href="#三者区别" class="headerlink" title="三者区别"></a>三者区别</h4><p><strong>RDD</strong></p>
<ul>
<li>RDD 一般和 spark mllib 同时使用</li>
<li>RDD 不支持 sparksql 操作</li>
</ul>
<p><strong>DataFrame</strong></p>
<ul>
<li>与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值</li>
<li>DataFrame 与 DataSet 一般不与 spark mllib 同时使用</li>
<li>DataFrame 与 DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能注册临时表/视窗，进行 sql 语句操作</li>
<li>DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头，这样每一列的字段名一目了然</li>
</ul>
<h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><ul>
<li>Dataset 和 DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同。DataFrame 其实就是 DataSet 的一个特例 type DataFrame = Dataset[Row]</li>
<li>DataFrame 也可以叫 Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性中的第七条提到的模式匹配拿出特定字段。而 Dataset 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息</li>
</ul>
<p><img src="https://fsats-blog.oss-cn-beijing.aliyuncs.com/blog/image-20220505002901546.png" alt="image-20220505002901546"></p>
<h2 id="UDF、UDAF-用户自定义函数"><a href="#UDF、UDAF-用户自定义函数" class="headerlink" title="UDF、UDAF 用户自定义函数"></a>UDF、UDAF 用户自定义函数</h2><p>​        用户定义函数 (UDF) 是 Spark SQL 的一项功能，当系统的内置函数不足以执行所需的任务时，它允许用户定义自己的函数。在 Spark SQL 中使用 UDF，首先需要定义函数，然后向 Spark 注册函数，最后调用注册的函数。用户定义函数可以作用于单行或一次作用于多行。Spark SQL 还支持 UDF、UDAF 和 UDTF 的现有 Hive 实现的集成。</p>
<p>用户可以通过 spark.udf 功能添加自定义函数，实现自定义功能。</p>
<h3 id="UDF-用户定义函数"><a href="#UDF-用户定义函数" class="headerlink" title="UDF 用户定义函数"></a>UDF 用户定义函数</h3><p>用户定义函数 (UDF) 是作用于一行的用户可编程例程。</p>
<p>要定义用户定义函数的属性，用户可以使用该类中定义的一些方法。</p>
<ul>
<li><p>asNonNullable(): UserDefinedFunction</p>
<p>将 UserDefinedFunction 更新为不可为空。即UDF的返回值不能为空</p>
</li>
<li><p>asNondeterministic(): UserDefinedFunction</p>
<p>将 UserDefinedFunction 更新为不确定。即每次入参返回的结果都可能不一样</p>
</li>
<li><p>withName(name: String): UserDefinedFunction</p>
<p>使用给定名称更新 UserDefinedFunction。</p>
</li>
</ul>
<p>以下代码中的udf函数构建的对象是UserDefinedFunction抽象类的子类SparkUserDefinedFunction</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.udf</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> random = udf(() =&gt; <span class="type">Math</span>.random())</span><br><span class="line">spark.udf.register(<span class="string">&quot;random&quot;</span>,random.asNondeterministic())</span><br><span class="line"><span class="comment">// 或者不需要显示构建udf</span></span><br><span class="line"><span class="comment">//spark.udf.register(&quot;random&quot;,()=&gt;Math.random())</span></span><br><span class="line">spark.sql(<span class="string">&quot;select random()&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+------------------+</span><br><span class="line">|             <span class="type">UDF</span>()|</span><br><span class="line">+------------------+</span><br><span class="line">|<span class="number">0.5220181936811188</span>|</span><br><span class="line">+------------------+</span><br></pre></td></tr></table></figure>

<p><strong>定义两个参数的udf函数</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spark.udf.register(<span class="string">&quot;strLenScala&quot;</span>, (_: <span class="type">String</span>).length + (_: <span class="type">Int</span>))</span><br><span class="line"><span class="comment">// 两种表达式定义的一样</span></span><br><span class="line"><span class="comment">// spark.udf.register(&quot;strLenScala&quot;, (s: String, i: Int) =&gt; s.length + i)</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT strLenScala(&#x27;test&#x27;, 1)&quot;</span>).show()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+--------------------+</span><br><span class="line">|strLenScala(test, <span class="number">1</span>)|</span><br><span class="line">+--------------------+</span><br><span class="line">|                   <span class="number">5</span>|</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>

<p><strong>定义作为where条件的UDF</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">spark.udf.register(<span class="string">&quot;oneArgFilter&quot;</span>, (n: <span class="type">Int</span>) =&gt; &#123; n &gt; <span class="number">5</span> &#125;)</span><br><span class="line">spark.range(<span class="number">1</span>, <span class="number">10</span>).createOrReplaceTempView(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM test WHERE oneArgFilter(id)&quot;</span>).show()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">6</span>|</span><br><span class="line">|  <span class="number">7</span>|</span><br><span class="line">|  <span class="number">8</span>|</span><br><span class="line">|  <span class="number">9</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>

<h3 id="UDAF弱类型实现-新版3-0-0以后已标记为弃用-实现UserDefinedAggregateFunction类"><a href="#UDAF弱类型实现-新版3-0-0以后已标记为弃用-实现UserDefinedAggregateFunction类" class="headerlink" title="UDAF弱类型实现(新版3.0.0以后已标记为弃用) 实现UserDefinedAggregateFunction类"></a>UDAF弱类型实现(新版3.0.0以后已标记为弃用) 实现UserDefinedAggregateFunction类</h3><p>用户定义的聚合函数 (UDAF) 是用户可编程的例程，它们一次作用于多行并返回单个聚合值作为结果。</p>
<p>需要实现<code>org.apache.spark.sql.expressions.UserDefinedAggregateFunction</code>类</p>
<p>以模拟avg平均值计算为例</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">DoubleType</span>, <span class="type">LongType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 定义了求平均数的UDF</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 此聚合函数输入参数的数据类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>,<span class="type">LongType</span>)::<span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 聚合缓冲区中值的数据类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;sum&quot;</span>, <span class="type">LongType</span>), <span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 返回值的数据类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这个函数是否总是在相同的输入上返回相同的输出</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 初始化给定的聚合缓冲区。缓冲区本身是一个“行” 继承自Row，</span></span><br><span class="line">  <span class="comment">// 除了标准的方法(如在索引中检索值(如get()， getBoolean()))之外，</span></span><br><span class="line">  <span class="comment">// 还提供了更新其值的机会。注意，缓冲区内的数组和映射仍然是不可变的</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//buffer(0) = 0</span></span><br><span class="line">    <span class="comment">//buffer(1) = 0</span></span><br><span class="line">    buffer.update(<span class="number">0</span>,<span class="number">0</span>L)</span><br><span class="line">    buffer.update(<span class="number">1</span>,<span class="number">0</span>L)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 使用来自&#x27; input &#x27;的新输入数据更新给定的聚合缓冲区&#x27; buffer &#x27;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer.update(<span class="number">0</span>, buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>)) <span class="comment">// 增加值</span></span><br><span class="line">      buffer.update(<span class="number">1</span>, buffer.getLong(<span class="number">1</span>) + <span class="number">1</span>L) <span class="comment">// 增加数量</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并两个聚合缓冲区，并将更新的缓冲区值存储回&#x27; buffer1 &#x27;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1.update(<span class="number">0</span>, buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>))</span><br><span class="line">    buffer1.update(<span class="number">1</span>, buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算最终结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">spark.udf.register(<span class="string">&quot;myAverage&quot;</span>,<span class="type">MyAverage</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;json/user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">&quot;select myAverage(age) as avg_age from user&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+-------+</span><br><span class="line">|avg_age|</span><br><span class="line">+-------+</span><br><span class="line">|   <span class="number">16.0</span>|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure>



<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.8/api/scala/index.html#org.apache.spark.sql.functions$">内置的 DataFrames 函数</a>提供了常见的聚合，例如<code>count()</code>, <code>countDistinct()</code>, <code>avg()</code>, <code>max()</code>,<code>min()</code>等。虽然这些函数是为 DataFrames 设计的，但 Spark SQL 还为其中一些在 <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.8/api/scala/index.html#org.apache.spark.sql.expressions.scalalang.typed$">Scala</a>和 <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.8/api/java/org/apache/spark/sql/expressions/javalang/typed.html">Java</a>中提供了类型安全版本，以处理强类型数据集</p>
<h3 id="UDAF强类型实现-实现-Aggregator类"><a href="#UDAF强类型实现-实现-Aggregator类" class="headerlink" title="UDAF强类型实现 实现 Aggregator类"></a>UDAF强类型实现 实现 Aggregator类</h3><p>强类型数据集的用户定义聚合围绕<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.2.1/api/scala/org/apache/spark/sql/expressions/Aggregator.html">Aggregator</a>抽象类</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoder</span>, <span class="type">Encoders</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">username: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Average</span>(<span class="params">var sum: <span class="type">Long</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">/**</span></span></span><br><span class="line"><span class="class"><span class="comment"> * 定义平均值,继承Aggregator，采用强类型</span></span></span><br><span class="line"><span class="class"><span class="comment"> */</span></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">MyAverage2</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">User</span>, <span class="type">Average</span>, <span class="type">Double</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 零值、初始值。是否满足任意b + 0 = b的性质</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Average</span> = <span class="type">Average</span>(<span class="number">0</span>L, <span class="number">0</span>L)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并两个值以产生一个新值。出于性能考虑，函数可以修改“buffer”并返回它，而不是构造一个新对象</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">Average</span>, a: <span class="type">User</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">    b.sum += a.age</span><br><span class="line">    b.count += <span class="number">1</span>L</span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并两个聚合缓冲区值</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Average</span>, b2: <span class="type">Average</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">    b1.sum += b2.sum</span><br><span class="line">    b1.count += b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 结果输出函数,计算最终结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Average</span>): <span class="type">Double</span> = reduction.sum.doubleValue() / reduction.count</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 为聚合缓冲区指定编码器</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Average</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 为最终输出值类型指定编码器</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">spark.udf.register(<span class="string">&quot;myAverage&quot;</span>,<span class="type">MyAverage</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds = spark.read.json(<span class="string">&quot;json/user.json&quot;</span>).as[<span class="type">User</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> average = <span class="type">MyAverage2</span>.toColumn.name(<span class="string">&quot;avg_age&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> result = ds.select(average)</span><br><span class="line">result.show()</span><br><span class="line">spark.stop()</span><br><span class="line"><span class="comment">// out</span></span><br><span class="line">+-------+</span><br><span class="line">|avg_age|</span><br><span class="line">+-------+</span><br><span class="line">|   <span class="number">16.0</span>|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure>



<h2 id="数据读取与保存"><a href="#数据读取与保存" class="headerlink" title="数据读取与保存"></a>数据读取与保存</h2> 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          Donate
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://www.chenguangqi.com/2022/05/04/Spark/Spark-SQL/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2022/05/04/Scala/Scala%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Scala数据类型
          
        </div>
      </a>
    
    
      <a href="/2022/05/02/Spark/Spark%E6%A0%B8%E5%BF%83/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Spark核心</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.staticfile.org/valine/1.4.16/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "6mEPMQ2xurhSekkGkJI5wUzA-gzGzoHsz",
    app_key: "iliPU54Izlh7k5143khLQ93u",
    path: window.location.pathname,
    avatar: "mp",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
    
    <script src="https://cdn.staticfile.org/twikoo/1.4.18/twikoo.all.min.js"></script>
    <div id="twikoo" class="twikoo"></div>
    <script>
        twikoo.init({
            envId: ""
        })
    </script>
 
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2022
        <i class="ri-heart-fill heart_icon"></i> 陈光奇
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      
        <li>
          <a href="https://beian.miit.gov.cn/" target="_black" rel="nofollow">京ICP备17065668号-3</a>
        </li>
        
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer.jpg" alt="雪里"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/photos">相册</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2020/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.png">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->

<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>